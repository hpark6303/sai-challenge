#!/usr/bin/env python3
"""
KURE-v1 ê¸°ë°˜ ë²¡í„° DB RAG íŒŒì´í”„ë¼ì¸
- KURE-v1 ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©
- ChromaDB ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ í™œìš©
- í•œêµ­ì–´/ì˜ì–´ ë‹¤êµ­ì–´ ì§€ì›
- Kaggle ì œì¶œ í˜•ì‹ ì¶œë ¥

í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬: requirements.txt (ê¸°ì¡´ íŒŒì¼ ì‚¬ìš©)
- sentence-transformers>=2.2.0
- chromadb>=0.4.0
- konlpy>=0.6.0
- ê¸°íƒ€ ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤
"""

import json
import uuid
import time
from pathlib import Path
import sys
import pandas as pd
import numpy as np
from typing import List, Dict, Optional
from tqdm import tqdm
import re
from collections import Counter

# ë²¡í„° DB ë° ì„ë² ë”© ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    import chromadb
    from chromadb.config import Settings
    from sentence_transformers import SentenceTransformer
    from konlpy.tag import Okt
    from scienceon_api_example import ScienceONAPIClient
    from gemini_client import GeminiClient
except ImportError as e:
    print(f"ğŸš¨ [ì˜¤ë¥˜] í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {e}")
    print("ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”:")
    print("pip install -r requirements.txt")
    sys.exit(1)

class KUREVectorDB:
    """KURE-v1 ê¸°ë°˜ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬ì"""
    
    def __init__(self, db_path: str = "./vector_db", model_name: str = "nlpai-lab/KURE-v1"):
        """
        ë²¡í„° DB ì´ˆê¸°í™”
        
        Args:
            db_path: ë²¡í„° DB ì €ì¥ ê²½ë¡œ
            model_name: ì„ë² ë”© ëª¨ë¸ëª… (ê¸°ë³¸ê°’: KURE-v1)
        """
        self.db_path = db_path
        self.model_name = model_name
        
        # KURE-v1 ëª¨ë¸ ë¡œë“œ
        print(f"ğŸ”„ KURE-v1 ëª¨ë¸ ë¡œë”© ì¤‘... ({model_name})")
        self.model = SentenceTransformer(model_name)
        print(f"âœ… KURE-v1 ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (ì°¨ì›: {self.model.get_sentence_embedding_dimension()})")
        
        # ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.client = chromadb.PersistentClient(
            path=db_path,
            settings=Settings(
                chroma_db_impl="duckdb+parquet",
                persist_directory=db_path
            )
        )
        
        # ì»¬ë ‰ì…˜ ìƒì„± ë˜ëŠ” ë¡œë“œ
        try:
            self.collection = self.client.get_collection("papers")
            print(f"âœ… ê¸°ì¡´ ë²¡í„° DB ì»¬ë ‰ì…˜ ë¡œë“œ ì™„ë£Œ")
        except:
            self.collection = self.client.create_collection(
                name="papers",
                metadata={"description": "í•™ìˆ  ë…¼ë¬¸ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤"}
            )
            print(f"âœ… ìƒˆë¡œìš´ ë²¡í„° DB ì»¬ë ‰ì…˜ ìƒì„± ì™„ë£Œ")
    
    def preprocess_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        if not text:
            return ""
        
        # ê¸°ë³¸ ì •ë¦¬
        text = re.sub(r'\s+', ' ', text.strip())
        text = re.sub(r'[^\w\sê°€-í£]', ' ', text)
        
        return text
    
    def create_document_text(self, doc: Dict) -> str:
        """ë¬¸ì„œë¥¼ ì„ë² ë”©ìš© í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        title = self.preprocess_text(doc.get('title', ''))
        abstract = self.preprocess_text(doc.get('abstract', ''))
        
        # ì œëª©ê³¼ ì´ˆë¡ì„ ê²°í•© (ì œëª©ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜)
        if title and abstract:
            return f"{title} [SEP] {abstract}"
        elif title:
            return title
        elif abstract:
            return abstract
        else:
            return ""
    
    def add_documents(self, documents: List[Dict]) -> int:
        """
        ë¬¸ì„œë“¤ì„ ë²¡í„° DBì— ì¶”ê°€
        
        Args:
            documents: ì¶”ê°€í•  ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ì¶”ê°€ëœ ë¬¸ì„œ ìˆ˜
        """
        if not documents:
            return 0
        
        # ê¸°ì¡´ ë¬¸ì„œ ID í™•ì¸
        existing_ids = set()
        try:
            existing = self.collection.get()
            existing_ids = set(existing['ids'])
        except:
            pass
        
        # ìƒˆ ë¬¸ì„œë§Œ í•„í„°ë§
        new_docs = []
        for doc in documents:
            doc_id = doc.get('CN', str(uuid.uuid4()))
            if doc_id not in existing_ids:
                new_docs.append(doc)
        
        if not new_docs:
            print(f"   - ëª¨ë“  ë¬¸ì„œê°€ ì´ë¯¸ ë²¡í„° DBì— ì¡´ì¬í•©ë‹ˆë‹¤.")
            return 0
        
        # ë¬¸ì„œ í…ìŠ¤íŠ¸ ìƒì„±
        texts = []
        metadatas = []
        ids = []
        
        for doc in new_docs:
            text = self.create_document_text(doc)
            if text:  # ë¹ˆ í…ìŠ¤íŠ¸ ì œì™¸
                texts.append(text)
                metadatas.append(doc)
                ids.append(doc.get('CN', str(uuid.uuid4())))
        
        if not texts:
            return 0
        
        # ì„ë² ë”© ìƒì„±
        print(f"   - {len(texts)}ê°œ ë¬¸ì„œ ì„ë² ë”© ìƒì„± ì¤‘...")
        embeddings = self.model.encode(texts, show_progress_bar=True)
        
        # ë²¡í„° DBì— ì¶”ê°€
        self.collection.add(
            embeddings=embeddings.tolist(),
            documents=texts,
            metadatas=metadatas,
            ids=ids
        )
        
        print(f"   - {len(texts)}ê°œ ë¬¸ì„œ ë²¡í„° DB ì¶”ê°€ ì™„ë£Œ")
        return len(texts)
    
    def search_similar(self, query: str, n_results: int = 50, threshold: float = 0.3) -> List[Dict]:
        """
        ì¿¼ë¦¬ì™€ ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            n_results: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            threshold: ìœ ì‚¬ë„ ì„ê³„ê°’
            
        Returns:
            ìœ ì‚¬í•œ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        if not query.strip():
            return []
        
        # ì¿¼ë¦¬ ì „ì²˜ë¦¬
        processed_query = self.preprocess_text(query)
        if not processed_query:
            return []
        
        # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
        query_embedding = self.model.encode([processed_query])
        
        # ë²¡í„° ê²€ìƒ‰
        results = self.collection.query(
            query_embeddings=query_embedding.tolist(),
            n_results=n_results,
            include=['metadatas', 'distances']
        )
        
        # ê²°ê³¼ í•„í„°ë§ ë° ì •ë ¬
        filtered_results = []
        for i, (metadata, distance) in enumerate(zip(results['metadatas'][0], results['distances'][0])):
            # ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜ (ChromaDBëŠ” ê±°ë¦¬ ê¸°ë°˜)
            similarity = 1.0 - distance
            
            if similarity >= threshold:
                metadata['similarity_score'] = similarity
                metadata['rank'] = i + 1
                filtered_results.append(metadata)
        
        return filtered_results
    
    def get_collection_stats(self) -> Dict:
        """ì»¬ë ‰ì…˜ í†µê³„ ì •ë³´"""
        try:
            count = self.collection.count()
            return {
                "total_documents": count,
                "model_name": self.model_name,
                "embedding_dimension": self.model.get_sentence_embedding_dimension()
            }
        except Exception as e:
            return {"error": str(e)}

class KUREVectorRAGPipeline:
    """KURE-v1 ê¸°ë°˜ ë²¡í„° RAG íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, config_path: str = "./configs"):
        """
        íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
        
        Args:
            config_path: ì„¤ì • íŒŒì¼ ê²½ë¡œ
        """
        self.config_path = Path(config_path)
        
        # API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.api_client = ScienceONAPIClient(
            credentials_path=self.config_path / "scienceon_api_credentials.json"
        )
        self.gemini_client = GeminiClient(
            credentials_path=self.config_path / "gemini_api_credentials.json"
        )
        
        # ë²¡í„° DB ì´ˆê¸°í™”
        self.vector_db = KUREVectorDB()
        
        # í•œêµ­ì–´ ì²˜ë¦¬ê¸°
        self.okt = Okt()
        
        print("âœ… KURE-v1 ë²¡í„° RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def extract_keywords(self, query: str) -> List[str]:
        """ì¿¼ë¦¬ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
        if re.search('[ê°€-í£]', query):
            # í•œêµ­ì–´: ëª…ì‚¬ ì¶”ì¶œ
            nouns = self.okt.nouns(query)
            keywords = [noun for noun in nouns if len(noun) > 1]
            return keywords[:5]
        else:
            # ì˜ì–´: ë¶ˆìš©ì–´ ì œê±°
            stop_words = {'a', 'an', 'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}
            words = re.findall(r'\w+', query.lower())
            keywords = [word for word in words if word not in stop_words and len(word) > 2]
            return keywords[:5]
    
    def retrieve_documents(self, query: str, max_docs: int = 100) -> List[Dict]:
        """
        ë¬¸ì„œ ê²€ìƒ‰ ë° ë²¡í„° DB ì €ì¥
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            max_docs: ìµœëŒ€ ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜
            
        Returns:
            ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        print(f"ğŸ” ë¬¸ì„œ ê²€ìƒ‰ ì‹œì‘: '{query[:50]}...'")
        
        # 1ë‹¨ê³„: APIë¡œ ì´ˆê¸° ê²€ìƒ‰
        keywords = self.extract_keywords(query)
        all_docs = []
        
        for keyword in keywords:
            try:
                docs = self.api_client.search_articles(
                    keyword, 
                    row_count=20, 
                    fields=['title', 'abstract', 'CN']
                )
                all_docs.extend(docs)
                time.sleep(0.2)  # API í˜¸ì¶œ ì œí•œ ë°©ì§€
            except Exception as e:
                print(f"   âš ï¸  í‚¤ì›Œë“œ '{keyword}' ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                continue
        
        # ì¤‘ë³µ ì œê±°
        unique_docs = list({doc['CN']: doc for doc in all_docs if 'CN' in doc}.values())
        print(f"   - API ê²€ìƒ‰ ê²°ê³¼: {len(unique_docs)}ê°œ ë¬¸ì„œ")
        
        # 2ë‹¨ê³„: ë²¡í„° DBì— ì €ì¥
        added_count = self.vector_db.add_documents(unique_docs)
        
        # 3ë‹¨ê³„: ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰
        similar_docs = self.vector_db.search_similar(query, n_results=max_docs)
        print(f"   - ë²¡í„° ê²€ìƒ‰ ê²°ê³¼: {len(similar_docs)}ê°œ ë¬¸ì„œ")
        
        return similar_docs
    
    def create_context(self, documents: List[Dict], max_docs: int = 5) -> str:
        """ë‹µë³€ ìƒì„±ì„ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ ìƒì„±"""
        if not documents:
            return ""
        
        context_parts = []
        for i, doc in enumerate(documents[:max_docs]):
            title = doc.get('title', '')
            abstract = doc.get('abstract', '')
            similarity = doc.get('similarity_score', 0)
            
            context = f"[ë¬¸ì„œ {i+1}] (ìœ ì‚¬ë„: {similarity:.3f})\n"
            context += f"ì œëª©: {title}\n"
            context += f"ì´ˆë¡: {abstract}\n"
            context_parts.append(context)
        
        return "\n".join(context_parts)
    
    def generate_answer(self, query: str, context: str, language: str) -> str:
        """ë‹µë³€ ìƒì„±"""
        if not context:
            return "ì œê³µëœ ì°¸ê³  ë¬¸ì„œë¡œëŠ” ì§ˆë¬¸ì— ëŒ€í•œ ì¶©ë¶„í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        language_instruction = "í•œêµ­ì–´ë¡œ" if language == "ko" else "ì˜ì–´ë¡œ (in English)"
        
        prompt = f"""ë‹¹ì‹ ì€ ì£¼ì–´ì§„ í•™ìˆ  ë¬¸ì„œë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•œ ì‹¬ì¸µ ë¶„ì„ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ëŠ” ì „ë¬¸ ì—°êµ¬ì›ì…ë‹ˆë‹¤.

### ì°¸ê³  ë¬¸ì„œ (Context):
{context}

### ì›ë³¸ ì§ˆë¬¸ (Original Question):
{query}

### í•µì‹¬ ì§€ì¹¨ (Core Directives):
1. **ì–¸ì–´ ì¤€ìˆ˜**: ì›ë³¸ ì§ˆë¬¸ì´ '{language_instruction}'ë¡œ ì‘ì„±ë˜ì—ˆìœ¼ë¯€ë¡œ, ìµœì¢… ë³´ê³ ì„œ ì „ì²´ë¥¼ ë°˜ë“œì‹œ **{language_instruction}**ë¡œ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤.
2. **ì „ë¬¸ê°€ì˜ ìì„¸**: ë‹¹ì‹ ì€ ì´ ì£¼ì œì˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. "ì •ë³´ê°€ ë¶€ì¡±í•˜ë‹¤", "~ì¼ ê²ƒìœ¼ë¡œ ì¶”ì •ëœë‹¤"ì™€ ê°™ì€ ë¶ˆí™•ì‹¤í•œ í‘œí˜„ì„ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
3. **ì‚¬ì‹¤ ê¸°ë°˜ ì¢…í•©**: ì—¬ëŸ¬ ë¬¸ì„œì— í©ì–´ì ¸ ìˆëŠ” ì •ë³´ë¥¼ ë…¼ë¦¬ì ìœ¼ë¡œ ì—°ê²°í•˜ê³  ì¢…í•©í•˜ì—¬ í•˜ë‚˜ì˜ ì™„ì„±ëœ ê¸€ë¡œ ì¬êµ¬ì„±í•˜ì„¸ìš”.
4. **ì—„ê²©í•œ ì¶œì²˜ í‘œê¸°**: ë³´ê³ ì„œì˜ ëª¨ë“  ë¬¸ì¥ì€ ë°˜ë“œì‹œ 'ì°¸ê³  ë¬¸ì„œ'ì— ëª…ì‹œëœ ì‚¬ì‹¤ì— ê¸°ë°˜í•´ì•¼ í•©ë‹ˆë‹¤.

### ì¶œë ¥ í˜•ì‹ (Output Format):
1. **ì œëª© (Title):** ì§ˆë¬¸ì˜ í•µì‹¬ ë‚´ìš©ì„ í¬ê´„í•˜ëŠ” ê°„ê²°í•˜ê³  ì „ë¬¸ì ì¸ ì œëª©
2. **ì„œë¡  (Introduction):** ì§ˆë¬¸ì˜ ë°°ê²½ê³¼ í•µì‹¬ ì£¼ì œë¥¼ ê°„ëµíˆ ì–¸ê¸‰
3. **ë³¸ë¡  (Body):** ì°¸ê³  ë¬¸ì„œì—ì„œ ì°¾ì•„ë‚¸ í•µì‹¬ì ì¸ ì‚¬ì‹¤, ë°ì´í„°, ì£¼ì¥ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ êµ¬ì²´ì ì¸ ë‹µë³€
4. **ê²°ë¡  (Conclusion):** ë³¸ë¡ ì˜ í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•˜ë©° ë³´ê³ ì„œë¥¼ ë§ˆë¬´ë¦¬

---
### ìµœì¢… ë³´ê³ ì„œ:
"""
        
        try:
            answer = self.gemini_client.generate_answer(prompt)
            
            # ë‹µë³€ í’ˆì§ˆ ê²€ì¦
            if not answer or len(answer.strip()) < 20:
                return self.generate_fallback_answer(query, documents, language)
            
            return answer
            
        except Exception as e:
            print(f"   âš ï¸  ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            return self.generate_fallback_answer(query, documents, language)
    
    def generate_fallback_answer(self, query: str, documents: List[Dict], language: str) -> str:
        """ëŒ€ì²´ ë‹µë³€ ìƒì„±"""
        if not documents:
            return "ì œê³µëœ ì°¸ê³  ë¬¸ì„œë¡œëŠ” ì§ˆë¬¸ì— ëŒ€í•œ ì¶©ë¶„í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        titles = [doc.get('title', '') for doc in documents[:3] if doc.get('title')]
        
        if language == "ko":
            answer = f"ì œê³µëœ ë¬¸ì„œë“¤ì„ ë°”íƒ•ìœ¼ë¡œ '{query}'ì— ëŒ€í•œ ë¶„ì„ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤.\n\n"
            answer += "ì£¼ìš” ì°¸ê³  ë¬¸ì„œ:\n"
            for i, title in enumerate(titles, 1):
                answer += f"{i}. {title}\n"
            answer += f"\nì´ ë¬¸ì„œë“¤ì€ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ìœ ìš©í•œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤."
        else:
            answer = f"Based on the provided documents, I have analyzed '{query}'.\n\n"
            answer += "Key reference documents:\n"
            for i, title in enumerate(titles, 1):
                answer += f"{i}. {title}\n"
            answer += f"\nThese documents provide useful information related to the question."
        
        return answer
    
    def create_kaggle_format_article(self, doc: Dict, index: int) -> str:
        """Kaggle í˜•ì‹ìœ¼ë¡œ ë…¼ë¬¸ ì •ë³´ ìƒì„±"""
        title = doc.get('title', '')
        abstract = doc.get('abstract', '')
        cn = doc.get('CN', '')
        
        source_url = f"http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=05787966&target=NART&cn={cn}"
        return f'Title: {title}, Abstract: {abstract}, Source: {source_url}'
    
    def process_questions(self, test_df: pd.DataFrame) -> pd.DataFrame:
        """
        ëª¨ë“  ì§ˆë¬¸ ì²˜ë¦¬
        
        Args:
            test_df: í…ŒìŠ¤íŠ¸ ë°ì´í„°í”„ë ˆì„
            
        Returns:
            ê²°ê³¼ ë°ì´í„°í”„ë ˆì„
        """
        print(f"ğŸš€ {len(test_df)}ê°œ ì§ˆë¬¸ ì²˜ë¦¬ ì‹œì‘")
        
        predictions = []
        predicted_articles = []
        
        for index, row in tqdm(test_df.iterrows(), total=len(test_df), desc="ì§ˆë¬¸ ì²˜ë¦¬"):
            query = row['Question']
            language = "ko" if re.search('[ê°€-í£]', query) else "en"
            
            try:
                # 1ë‹¨ê³„: ë¬¸ì„œ ê²€ìƒ‰
                documents = self.retrieve_documents(query, max_docs=100)
                
                # 2ë‹¨ê³„: ì»¨í…ìŠ¤íŠ¸ ìƒì„±
                context = self.create_context(documents, max_docs=5)
                
                # 3ë‹¨ê³„: ë‹µë³€ ìƒì„±
                answer = self.generate_answer(query, context, language)
                predictions.append(answer)
                
                # 4ë‹¨ê³„: ë…¼ë¬¸ ì •ë³´ ì¶”ì¶œ
                article_titles = []
                for i, doc in enumerate(documents[:50]):
                    formatted_article = self.create_kaggle_format_article(doc, i+1)
                    article_titles.append(formatted_article)
                
                # 50ê°œê°€ ë˜ë„ë¡ ë¹ˆ ë¬¸ìì—´ë¡œ ì±„ì›€
                while len(article_titles) < 50:
                    article_titles.append('')
                
                predicted_articles.append(article_titles)
                
                print(f"   âœ… ì§ˆë¬¸ {index+1} ì²˜ë¦¬ ì™„ë£Œ")
                
            except Exception as e:
                print(f"   âŒ ì§ˆë¬¸ {index+1} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                predictions.append(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
                predicted_articles.append([''] * 50)
        
        # ê²°ê³¼ ë°ì´í„°í”„ë ˆì„ ìƒì„±
        submission_df = test_df.copy()
        submission_df['Prediction'] = predictions
        
        # 50ê°œ prediction_retrieved_article_name ì»¬ëŸ¼ ì¶”ê°€
        for i in range(1, 51):
            column_name = f'prediction_retrieved_article_name_{i}'
            submission_df[column_name] = [''] * len(submission_df)
        
        # ì‹¤ì œ ê°’ ì±„ìš°ê¸°
        for i in range(1, 51):
            column_name = f'prediction_retrieved_article_name_{i}'
            for row_idx, articles in enumerate(predicted_articles):
                if i-1 < len(articles) and articles[i-1]:
                    submission_df.at[row_idx, column_name] = articles[i-1]
        
        return submission_df
    
    def run(self, test_file: str = "test.csv", output_file: str = "submission_kure.csv"):
        """
        ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        
        Args:
            test_file: í…ŒìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ
            output_file: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
        """
        start_time = time.time()
        
        print("â­ KURE-v1 ë²¡í„° RAG íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
        try:
            test_df = pd.read_csv(test_file)
            print(f"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(test_df)}ê°œ ì§ˆë¬¸")
        except Exception as e:
            print(f"âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return
        
        # ë²¡í„° DB í†µê³„ ì¶œë ¥
        stats = self.vector_db.get_collection_stats()
        print(f"ğŸ“Š ë²¡í„° DB í†µê³„: {stats}")
        
        # ì§ˆë¬¸ ì²˜ë¦¬
        submission_df = self.process_questions(test_df)
        
        # ê²°ê³¼ ì €ì¥
        submission_df = submission_df.fillna('')
        submission_df.to_csv(output_file, index=False, encoding='utf-8-sig')
        
        end_time = time.time()
        total_time = end_time - start_time
        
        print(f"\nğŸ‰ KURE-v1 ë²¡í„° RAG íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
        print(f"   - ì´ ì†Œìš” ì‹œê°„: {total_time:.2f}ì´ˆ ({total_time/60:.1f}ë¶„)")
        print(f"   - í‰ê·  ì²˜ë¦¬ ì‹œê°„: {total_time/len(test_df):.2f}ì´ˆ/ì§ˆë¬¸")
        print(f"   - ìµœì¢… ì œì¶œ íŒŒì¼: {output_file}")
        print(f"   - íŒŒì¼ í¬ê¸°: {len(submission_df)} í–‰ Ã— {len(submission_df.columns)} ì—´")
        
        # ì„±ê³µë¥  ê³„ì‚°
        successful_count = len([p for p in submission_df['Prediction'] if 'ì˜¤ë¥˜' not in p])
        success_rate = (successful_count / len(test_df)) * 100
        print(f"   - ì„±ê³µë¥ : {successful_count}/{len(test_df)} ({success_rate:.1f}%)")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    pipeline = KUREVectorRAGPipeline()
    pipeline.run()

if __name__ == "__main__":
    main()

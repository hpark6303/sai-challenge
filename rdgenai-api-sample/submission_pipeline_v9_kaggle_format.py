#!/usr/bin/env python3
"""
Kaggle ì œì¶œìš© RAG íŒŒì´í”„ë¼ì¸ v9.0
- test.csv í˜•ì‹ì— ë§ì¶° ì¶œë ¥
- í•œêµ­ì–´/ì˜ì–´ ë²ˆì—­ ì²˜ë¦¬
- 5ê°œ ë…¼ë¬¸ ê²€ìƒ‰ (title + abstract + source í˜•ì‹)
- ë°°ì¹˜ ì²˜ë¦¬ë¡œ ìµœì í™”
"""

import json
from pathlib import Path
import sys
import time
import pandas as pd
import re
from tqdm import tqdm
from typing import List, Dict
import numpy as np
from collections import Counter

# --- í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ---
try:
    from konlpy.tag import Okt
    from scienceon_api_example import ScienceONAPIClient
    from gemini_client import GeminiClient
except ImportError as e:
    print(f"ğŸš¨ [ì˜¤ë¥˜] í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {e}")
    print("ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”: pip install tqdm konlpy")
    sys.exit(1)

def validate_credentials(path: Path) -> dict:
    """API ì¸ì¦ ì •ë³´ ê²€ì¦"""
    credentials = {}
    if not path.exists():
        print(f"ğŸš¨ ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤! (ê²½ë¡œ: {path})")
        sys.exit(1)
    try:
        with open(path, 'r', encoding='utf-8') as f:
            credentials = json.load(f)
    except json.JSONDecodeError:
        print(f"ğŸš¨ ì„¤ì • íŒŒì¼ì˜ í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤! (ê²½ë¡œ: {path})")
        sys.exit(1)
    
    required_keys = ['auth_key', 'client_id', 'mac_address']
    missing_keys = [key for key in required_keys if key not in credentials or not credentials[key]]
    if missing_keys:
        print(f"ğŸš¨ ì„¤ì • íŒŒì¼ì— í•„ìˆ˜ ì •ë³´ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤! (ëˆ„ë½ëœ ì •ë³´: {', '.join(missing_keys)})")
        sys.exit(1)
    
    if len(credentials['auth_key']) != 32:
        print(f"ğŸš¨ ì¸ì¦í‚¤(auth_key)ì˜ ê¸¸ì´ê°€ 32ìê°€ ì•„ë‹™ë‹ˆë‹¤! (í˜„ì¬ ê¸¸ì´: {len(credentials['auth_key'])}ì)")
        sys.exit(1)
    
    print("âœ… [ì„±ê³µ] API ì¸ì¦ ì •ë³´ íŒŒì¼ì´ ìœ íš¨í•©ë‹ˆë‹¤.")
    return credentials

def is_korean(text: str) -> bool:
    """í•œêµ­ì–´ í…ìŠ¤íŠ¸ ê°ì§€"""
    return bool(re.search('[ê°€-í£]', text))

def extract_english_keywords(text: str) -> List[str]:
    """ì˜ì–´ í‚¤ì›Œë“œ ì¶”ì¶œ"""
    text_no_punct = re.sub(r'[^\w\s-]', '', text)
    words = text_no_punct.split()
    proper_nouns = [word for word in words if word[0].isupper() and len(word) > 2]
    
    text_lower = text.lower()
    stop_words = [
        'a', 'an', 'the', 'what', 'how', 'who', 'when', 'where', 'why', 
        'can', 'could', 'would', 'is', 'are', 'be', 'do', 'does', 'did', 
        'in', 'of', 'for', 'to', 'and', 'or', 'it', 'its', 'their', 'by', 
        'on', 'with', 'from', 'as', 'about', 'summarize', 'outline', 
        'describe', 'propose', 'explain', 'provide', 'capture', 'distill', 
        'characterize', 'evaluate', 'summarized', 'discussed', 'based'
    ]
    
    common_words = text_lower.split()
    common_keywords = [word for word in common_words if word not in stop_words and len(word) > 3]
    
    final_keywords = list(dict.fromkeys(proper_nouns + common_keywords))
    return final_keywords[:5]

def get_verified_korean_synonyms(word: str) -> List[str]:
    """ê²€ì¦ëœ í•œêµ­ì–´ ë™ì˜ì–´ ì‚¬ì „"""
    synonym_dict = {
        'ì¸ê³µì§€ëŠ¥': ['AI', 'artificial intelligence', 'ë¨¸ì‹ ëŸ¬ë‹', 'ë”¥ëŸ¬ë‹'],
        'ë¨¸ì‹ ëŸ¬ë‹': ['machine learning', 'ML', 'ê¸°ê³„í•™ìŠµ', 'í•™ìŠµ ì•Œê³ ë¦¬ì¦˜'],
        'ë”¥ëŸ¬ë‹': ['deep learning', 'ì‹ ê²½ë§', 'neural network', 'CNN', 'RNN'],
        'ìì—°ì–´ì²˜ë¦¬': ['NLP', 'natural language processing', 'í…ìŠ¤íŠ¸ ë¶„ì„'],
        'ì»´í“¨í„°ë¹„ì „': ['computer vision', 'ì´ë¯¸ì§€ ì²˜ë¦¬', 'ì˜ìƒ ë¶„ì„'],
        'ê°•í™”í•™ìŠµ': ['reinforcement learning', 'RL', 'ë³´ìƒ í•™ìŠµ'],
        'ë°ì´í„°ë§ˆì´ë‹': ['data mining', 'ë°ì´í„° ë¶„ì„', 'íŒ¨í„´ ë°œê²¬'],
        'ë¹…ë°ì´í„°': ['big data', 'ëŒ€ìš©ëŸ‰ ë°ì´í„°', 'ë°ì´í„° ì²˜ë¦¬'],
        'í´ë¼ìš°ë“œ': ['cloud computing', 'í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤', 'ì›ê²© ì²˜ë¦¬'],
        'ë¸”ë¡ì²´ì¸': ['blockchain', 'ë¶„ì‚°ì›ì¥', 'ì•”í˜¸í™”í'],
        'IoT': ['internet of things', 'ì‚¬ë¬¼ì¸í„°ë„·', 'ì„¼ì„œ ë„¤íŠ¸ì›Œí¬'],
        '5G': ['5th generation', '5ì„¸ëŒ€', 'ëª¨ë°”ì¼ í†µì‹ '],
        'ììœ¨ì£¼í–‰': ['autonomous driving', 'self-driving', 'ë¬´ì¸ ìš´ì „'],
        'ë¡œë´‡': ['robot', 'ìë™í™”', 'ë©”ì¹´íŠ¸ë¡œë‹‰ìŠ¤'],
        'ë“œë¡ ': ['drone', 'UAV', 'ë¬´ì¸í•­ê³µê¸°'],
        'ê°€ìƒí˜„ì‹¤': ['VR', 'virtual reality', 'ì¦ê°•í˜„ì‹¤', 'AR'],
        'ë©”íƒ€ë²„ìŠ¤': ['metaverse', 'ê°€ìƒì„¸ê³„', 'ë””ì§€í„¸ ê³µê°„'],
        'ì•”í˜¸í™”': ['encryption', 'ë³´ì•ˆ', 'cryptography'],
        'ì‚¬ì´ë²„ë³´ì•ˆ': ['cybersecurity', 'ì •ë³´ë³´ì•ˆ', 'ë„¤íŠ¸ì›Œí¬ ë³´ì•ˆ'],
        'ì–‘ìì»´í“¨íŒ…': ['quantum computing', 'ì–‘ì ì•Œê³ ë¦¬ì¦˜', 'ì–‘ì ì •ë³´']
    }
    return synonym_dict.get(word, [word])

def extract_korean_keywords_with_synonyms(query: str, okt) -> List[str]:
    """í•œêµ­ì–´ í‚¤ì›Œë“œ ì¶”ì¶œ ë° ë™ì˜ì–´ í™•ì¥"""
    nouns = okt.nouns(query)
    important_nouns = [noun for noun in nouns if len(noun) > 1]
    
    expanded_keywords = []
    for noun in important_nouns[:3]:  # ìƒìœ„ 3ê°œ ëª…ì‚¬ë§Œ í™•ì¥
        synonyms = get_verified_korean_synonyms(noun)
        expanded_keywords.extend(synonyms)
    
    # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
    unique_keywords = list(set(expanded_keywords))
    return unique_keywords[:8]  # ìµœëŒ€ 8ê°œ í‚¤ì›Œë“œ

def extract_more_english_keywords(text: str) -> List[str]:
    """ì˜ì–´ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ê°€ í‚¤ì›Œë“œ ì¶”ì¶œ (ë” ë„“ì€ ë²”ìœ„)"""
    # ê¸°ë³¸ í‚¤ì›Œë“œ ì¶”ì¶œ
    basic_keywords = extract_english_keywords(text)
    
    # ì¶”ê°€ í‚¤ì›Œë“œ ì¶”ì¶œ (ë” ë§ì€ ë¶ˆìš©ì–´ í¬í•¨)
    text_no_punct = re.sub(r'[^\w\s-]', '', text)
    words = text_no_punct.split()
    
    # í™•ì¥ëœ ë¶ˆìš©ì–´ ì œê±°
    extended_stop_words = [
        'a', 'an', 'the', 'what', 'how', 'who', 'when', 'where', 'why', 
        'can', 'could', 'would', 'is', 'are', 'be', 'do', 'does', 'did', 
        'in', 'of', 'for', 'to', 'and', 'or', 'it', 'its', 'their', 'by', 
        'on', 'with', 'from', 'as', 'about', 'summarize', 'outline', 
        'describe', 'propose', 'explain', 'provide', 'capture', 'distill', 
        'characterize', 'evaluate', 'summarized', 'discussed', 'based',
        'also', 'into', 'only', 'then', 'more', 'most', 'even', 'must', 'may', 'might', 'shall', 'should', 'would', 'could', 'will', 'can', 'do', 'does', 'did', 'done', 'doing', 'am', 'is', 'are', 'was', 'were', 'been', 'being', 'have', 'has', 'had', 'having', 'be', 'being', 'been', 'become', 'becomes', 'becoming', 'became', 'seem', 'seems', 'seemed', 'seeming', 'appear', 'appears', 'appeared', 'appearing', 'look', 'looks', 'looked', 'looking', 'feel', 'feels', 'felt', 'feeling', 'sound', 'sounds', 'sounded', 'sounding', 'taste', 'tastes', 'tasted', 'tasting', 'smell', 'smells', 'smelled', 'smelling'
    ]
    
    # í‚¤ì›Œë“œ í•„í„°ë§ ë° ì •ë ¬ (ë” ë§ì€ í‚¤ì›Œë“œ)
    keywords = [word for word in words if word.lower() not in extended_stop_words and len(word) > 1]
    
    # ë¹ˆë„ìˆœ ì •ë ¬ (ìƒìœ„ 20ê°œ)
    word_freq = Counter(keywords)
    additional_keywords = [word for word, freq in word_freq.most_common(20)]
    
    # ê¸°ë³¸ í‚¤ì›Œë“œì™€ í•©ì¹˜ê³  ì¤‘ë³µ ì œê±°
    all_keywords = basic_keywords + additional_keywords
    return list(dict.fromkeys(all_keywords))  # ìˆœì„œ ìœ ì§€í•˜ë©´ì„œ ì¤‘ë³µ ì œê±°

def extract_more_korean_keywords(text: str, okt) -> List[str]:
    """í•œêµ­ì–´ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ê°€ í‚¤ì›Œë“œ ì¶”ì¶œ (ë” ë„“ì€ ë²”ìœ„)"""
    # ê¸°ë³¸ í‚¤ì›Œë“œ ì¶”ì¶œ
    basic_keywords = extract_korean_keywords_with_synonyms(text, okt)
    
    # ì¶”ê°€ í‚¤ì›Œë“œ ì¶”ì¶œ (ë” ë§ì€ í’ˆì‚¬ í¬í•¨)
    try:
        # ëª…ì‚¬, í˜•ìš©ì‚¬, ë™ì‚¬ ëª¨ë‘ ì¶”ì¶œ
        pos_tags = okt.pos(text, norm=True, stem=True)
        
        # ë” ë§ì€ í’ˆì‚¬ í¬í•¨
        target_pos = ['Noun', 'Adjective', 'Verb', 'Adverb']
        additional_keywords = []
        
        for word, pos in pos_tags:
            if pos in target_pos and len(word) > 1:
                additional_keywords.append(word)
        
        # ë¹ˆë„ìˆœ ì •ë ¬
        word_freq = Counter(additional_keywords)
        additional_keywords = [word for word, freq in word_freq.most_common(20)]
        
        # ê¸°ë³¸ í‚¤ì›Œë“œì™€ í•©ì¹˜ê³  ì¤‘ë³µ ì œê±°
        all_keywords = basic_keywords + additional_keywords
        return list(dict.fromkeys(all_keywords))  # ìˆœì„œ ìœ ì§€í•˜ë©´ì„œ ì¤‘ë³µ ì œê±°
        
    except Exception as e:
        print(f"   âš ï¸  ì¶”ê°€ í•œêµ­ì–´ í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return basic_keywords

def simple_semantic_filtering(documents: List[Dict], query: str) -> List[Dict]:
    """ê°„ë‹¨í•œ ì˜ë¯¸ ê¸°ë°˜ í•„í„°ë§ (í‚¤ì›Œë“œ ë§¤ì¹­)"""
    if not documents:
        return []
    
    query_lower = query.lower()
    query_words = set(re.findall(r'\w+', query_lower))
    
    # ê° ë¬¸ì„œì— ì ìˆ˜ ë¶€ì—¬
    scored_docs = []
    for doc in documents:
        title = doc.get('title', '').lower()
        abstract = doc.get('abstract', '').lower()
        content = f"{title} {abstract}"
        
        # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜
        content_words = set(re.findall(r'\w+', content))
        keyword_score = len(query_words.intersection(content_words)) * 2
        
        # ì œëª© ë§¤ì¹­ ë³´ë„ˆìŠ¤
        title_match = len(query_words.intersection(set(re.findall(r'\w+', title)))) * 3
        
        # ì „ë¬¸ ìš©ì–´ ë§¤ì¹­ ë³´ë„ˆìŠ¤
        tech_terms = ['AI', 'machine learning', 'deep learning', 'neural network', 'algorithm', 'data', 'system', 'model', 'analysis', 'research', 'study', 'method', 'approach', 'framework', 'architecture', 'technology', 'innovation', 'development', 'implementation', 'evaluation', 'performance', 'accuracy', 'efficiency', 'optimization', 'automation', 'intelligence', 'computing', 'processing', 'recognition', 'classification', 'prediction', 'forecasting', 'detection', 'monitoring', 'control', 'management', 'integration', 'deployment', 'scalability', 'robustness', 'reliability', 'security', 'privacy', 'ethics', 'sustainability', 'environmental', 'social', 'economic', 'policy', 'regulation', 'standard', 'protocol', 'interface', 'platform', 'service', 'application', 'solution', 'tool', 'software', 'hardware', 'infrastructure', 'network', 'communication', 'collaboration', 'interaction', 'user', 'experience', 'design', 'interface', 'visualization', 'representation', 'knowledge', 'information', 'data', 'database', 'storage', 'retrieval', 'search', 'query', 'indexing', 'ranking', 'filtering', 'clustering', 'segmentation', 'classification', 'regression', 'clustering', 'association', 'correlation', 'causation', 'inference', 'reasoning', 'logic', 'decision', 'planning', 'scheduling', 'optimization', 'allocation', 'distribution', 'coordination', 'synchronization', 'parallelization', 'distributed', 'centralized', 'decentralized', 'hierarchical', 'flat', 'modular', 'component', 'module', 'library', 'framework', 'api', 'sdk', 'middleware', 'backend', 'frontend', 'client', 'server', 'database', 'cache', 'queue', 'stream', 'batch', 'real-time', 'offline', 'online', 'cloud', 'edge', 'fog', 'mobile', 'web', 'desktop', 'embedded', 'iot', 'wearable', 'smart', 'intelligent', 'adaptive', 'learning', 'evolutionary', 'genetic', 'swarm', 'collective', 'emergent', 'self-organizing', 'autonomous', 'automatic', 'manual', 'semi-automatic', 'hybrid', 'multi-modal', 'cross-modal', 'inter-modal', 'trans-modal', 'meta', 'hyper', 'super', 'ultra', 'mega', 'giga', 'tera', 'peta', 'exa', 'zetta', 'yotta']
        
        tech_match = 0
        for term in tech_terms:
            if term.lower() in content:
                tech_match += 1
        
        # ë¬¸ì„œ ê¸¸ì´ ë³´ë„ˆìŠ¤
        length_bonus = min(len(content.split()) / 100, 2)
        
        # ìµœì¢… ì ìˆ˜ ê³„ì‚°
        total_score = keyword_score + title_match + tech_match + length_bonus
        
        scored_docs.append((doc, total_score))
    
    # ì ìˆ˜ìˆœìœ¼ë¡œ ì •ë ¬í•˜ê³  ìƒìœ„ 5ê°œ ì„ íƒ
    scored_docs.sort(key=lambda x: x[1], reverse=True)
    return [doc for doc, score in scored_docs[:5]]

def create_kaggle_format_article(doc: Dict, index: int) -> str:
    """Kaggle í˜•ì‹ìœ¼ë¡œ ë…¼ë¬¸ ì •ë³´ ìƒì„± (title + abstract + source)"""
    title = doc.get('title', '')
    abstract = doc.get('abstract', '')
    cn = doc.get('CN', '')
    
    # Source URL ìƒì„±
    source_url = f"http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=05787966&target=NART&cn={cn}"
    
    # Kaggle í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…
    formatted_article = f'Title: {title}, Abstract: {abstract}, Source: {source_url}'
    return formatted_article

def create_final_prompt_v9(query: str, context: str, language: str) -> str:
    """ìµœì¢… ë‹µë³€ ìƒì„±ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ v9 (ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”)"""
    language_instruction = "í•œêµ­ì–´ë¡œ" if language == "ko" else "ì˜ì–´ë¡œ (in English)"
    
    return f"""ë‹¹ì‹ ì€ ì£¼ì–´ì§„ í•™ìˆ  ë¬¸ì„œë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•œ ì‹¬ì¸µ ë¶„ì„ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ëŠ” ì „ë¬¸ ì—°êµ¬ì›ì…ë‹ˆë‹¤.

### ì°¸ê³  ë¬¸ì„œ (Context):
{context}

### ê³¼ì œ (Task):
'ì°¸ê³  ë¬¸ì„œ'ì˜ ë‚´ìš©ì„ ì™„ë²½í•˜ê²Œ ìˆ™ì§€í•œ í›„, ì•„ë˜ 'ì¶œë ¥ í˜•ì‹'ì— ë§ì¶° 'ì›ë³¸ ì§ˆë¬¸'ì— ëŒ€í•œ ë¶„ì„ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”.

### ì›ë³¸ ì§ˆë¬¸ (Original Question):
{query}

### í•µì‹¬ ì§€ì¹¨ (Core Directives):
1. **ì–¸ì–´ ì¤€ìˆ˜**: ì›ë³¸ ì§ˆë¬¸ì´ '{language_instruction}'ë¡œ ì‘ì„±ë˜ì—ˆìœ¼ë¯€ë¡œ, ìµœì¢… ë³´ê³ ì„œ ì „ì²´ë¥¼ ë°˜ë“œì‹œ **{language_instruction}**ë¡œ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤.
2. **ì „ë¬¸ê°€ì˜ ìì„¸**: ë‹¹ì‹ ì€ ì´ ì£¼ì œì˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. "ì •ë³´ê°€ ë¶€ì¡±í•˜ë‹¤", "~ì¼ ê²ƒìœ¼ë¡œ ì¶”ì •ëœë‹¤"ì™€ ê°™ì€ ë¶ˆí™•ì‹¤í•œ í‘œí˜„ì„ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
3. **ì‚¬ì‹¤ ê¸°ë°˜ ì¢…í•©**: ì—¬ëŸ¬ ë¬¸ì„œì— í©ì–´ì ¸ ìˆëŠ” ì •ë³´ë¥¼ ë…¼ë¦¬ì ìœ¼ë¡œ ì—°ê²°í•˜ê³  ì¢…í•©í•˜ì—¬ í•˜ë‚˜ì˜ ì™„ì„±ëœ ê¸€ë¡œ ì¬êµ¬ì„±í•˜ì„¸ìš”.
4. **ì—„ê²©í•œ ì¶œì²˜ í‘œê¸°**: ë³´ê³ ì„œì˜ ëª¨ë“  ë¬¸ì¥ì€ ë°˜ë“œì‹œ 'ì°¸ê³  ë¬¸ì„œ'ì— ëª…ì‹œëœ ì‚¬ì‹¤ì— ê¸°ë°˜í•´ì•¼ í•©ë‹ˆë‹¤.

### ì¶œë ¥ í˜•ì‹ (Output Format):
1. **ì œëª© (Title):** ì§ˆë¬¸ì˜ í•µì‹¬ ë‚´ìš©ì„ í¬ê´„í•˜ëŠ” ê°„ê²°í•˜ê³  ì „ë¬¸ì ì¸ ì œëª©
2. **ì„œë¡  (Introduction):** ì§ˆë¬¸ì˜ ë°°ê²½ê³¼ í•µì‹¬ ì£¼ì œë¥¼ ê°„ëµíˆ ì–¸ê¸‰
3. **ë³¸ë¡  (Body):** ì°¸ê³  ë¬¸ì„œì—ì„œ ì°¾ì•„ë‚¸ í•µì‹¬ì ì¸ ì‚¬ì‹¤, ë°ì´í„°, ì£¼ì¥ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ êµ¬ì²´ì ì¸ ë‹µë³€
4. **ê²°ë¡  (Conclusion):** ë³¸ë¡ ì˜ í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•˜ë©° ë³´ê³ ì„œë¥¼ ë§ˆë¬´ë¦¬

---
### ìµœì¢… ë³´ê³ ì„œ:
"""

def generate_fallback_answer(query: str, documents: List[Dict], language: str) -> str:
    """API í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ ëŒ€ì²´ ë‹µë³€ ìƒì„±"""
    if not documents:
        return "ì œê³µëœ ì°¸ê³  ë¬¸ì„œë¡œëŠ” ì§ˆë¬¸ì— ëŒ€í•œ ì¶©ë¶„í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    # ë¬¸ì„œì—ì„œ í•µì‹¬ ì •ë³´ ì¶”ì¶œ
    titles = [doc.get('title', '') for doc in documents if doc.get('title')]
    
    # ê°„ë‹¨í•œ í…œí”Œë¦¿ ê¸°ë°˜ ë‹µë³€ ìƒì„±
    if language == "ko":
        answer = f"ì œê³µëœ ë¬¸ì„œë“¤ì„ ë°”íƒ•ìœ¼ë¡œ '{query}'ì— ëŒ€í•œ ë¶„ì„ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤.\n\n"
        answer += "ì£¼ìš” ì°¸ê³  ë¬¸ì„œ:\n"
        for i, title in enumerate(titles[:3], 1):
            answer += f"{i}. {title}\n"
        answer += f"\nì´ ë¬¸ì„œë“¤ì€ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ìœ ìš©í•œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ìƒì„¸í•œ ë‚´ìš©ì€ ì°¸ê³  ë¬¸ì„œë¥¼ í™•ì¸í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤."
    else:
        answer = f"Based on the provided documents, I have analyzed '{query}'.\n\n"
        answer += "Key reference documents:\n"
        for i, title in enumerate(titles[:3], 1):
            answer += f"{i}. {title}\n"
        answer += f"\nThese documents provide useful information related to the question. Please refer to the documents for detailed content."
    
    return answer

def translate_text(text: str, target_language: str) -> str:
    """ê°„ë‹¨í•œ ë²ˆì—­ í•¨ìˆ˜ (ì‹¤ì œë¡œëŠ” Gemini API ì‚¬ìš©)"""
    # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” Gemini APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë²ˆì—­
    # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ ì˜ˆì‹œë¡œ ëŒ€ì²´
    if target_language == "en" and is_korean(text):
        # í•œêµ­ì–´ë¥¼ ì˜ì–´ë¡œ ë²ˆì—­í•˜ëŠ” ê°„ë‹¨í•œ ì˜ˆì‹œ
        return f"[Translated to English: {text}]"
    elif target_language == "ko" and not is_korean(text):
        # ì˜ì–´ë¥¼ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ëŠ” ê°„ë‹¨í•œ ì˜ˆì‹œ
        return f"[í•œêµ­ì–´ë¡œ ë²ˆì—­: {text}]"
    else:
        return text

def main():
    """Kaggle ì œì¶œìš© RAG íŒŒì´í”„ë¼ì¸ - test.csv í˜•ì‹ì— ë§ì¶° ì¶œë ¥"""
    start_total_time = time.time()
    
    # --- 1. ì´ˆê¸°í™” ---
    print("â­ Kaggle ì œì¶œìš© RAG íŒŒì´í”„ë¼ì¸ v9.0 ì‹œì‘")
    
    # API ì¸ì¦ ì •ë³´ ê²€ì¦
    credentials_path = Path('./configs/scienceon_api_credentials.json')
    validate_credentials(credentials_path)
    client = ScienceONAPIClient(credentials_path=credentials_path)
    okt = Okt()
    
    # Gemini API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    gemini_credentials_path = Path('./configs/gemini_api_credentials.json')
    gemini_client = GeminiClient(gemini_credentials_path)
    
    print(f"   - âœ… API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ (base DataFrameìœ¼ë¡œ ì‚¬ìš©)
    try:
        test_df = pd.read_csv("test.csv")
        print(f"\nâœ… ì´ˆê¸°í™” ì™„ë£Œ. {len(test_df)}ê°œì˜ ì§ˆë¬¸ ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
    except FileNotFoundError:
        print("âŒ Error: test.csv file not found!")
        return
    except Exception as e:
        print(f"âŒ Error reading test.csv: {e}")
        return

    # ê²°ê³¼ ì €ì¥ì„ ìœ„í•œ ë©”ëª¨ë¦¬ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
    predictions = []
    predicted_articles = []

    # --- 2. [ë°°ì¹˜] 1ë‹¨ê³„: ëª¨ë“  ì§ˆë¬¸ì— ëŒ€í•œ ë¬¸ì„œ ê²€ìƒ‰ ---
    print("\n--- [1/3] ëª¨ë“  ì§ˆë¬¸ì— ëŒ€í•œ ë¬¸ì„œ ê²€ìƒ‰ ì‹œì‘ ---")
    all_questions_data = []
    
    for index, row in tqdm(test_df.iterrows(), total=len(test_df), desc="   - Retrieving Documents"):
        real_query = row['Question']
        question_id = index
        
        # ì–¸ì–´ ê°ì§€ ë° í‚¤ì›Œë“œ ì¶”ì¶œ
        if is_korean(real_query):
            search_keywords = extract_korean_keywords_with_synonyms(real_query, okt)
        else:
            search_keywords = extract_english_keywords(real_query)
        
        # ëª¨ë“  í‚¤ì›Œë“œë¡œ ê²€ìƒ‰
        all_candidate_docs = []
        for keyword in search_keywords:
            try:
                docs = client.search_articles(keyword, row_count=10, fields=['title', 'abstract', 'CN'])
                all_candidate_docs.extend(docs)
                time.sleep(0.2)  # API í˜¸ì¶œ ì œí•œ ë°©ì§€
            except Exception as e:
                print(f"   âš ï¸  ê²€ìƒ‰ ì˜¤ë¥˜ (í‚¤ì›Œë“œ: {keyword}): {e}")
                continue
        
        # ì¤‘ë³µ ì œê±°
        unique_docs = list({doc['CN']: doc for doc in all_candidate_docs if 'CN' in doc}.values())
        
        # 50ê°œê°€ ì•ˆ ë˜ë©´ ì¶”ê°€ í‚¤ì›Œë“œë¡œ ë” ê²€ìƒ‰
        if len(unique_docs) < 50:
            print(f"   - ì§ˆë¬¸ {question_id+1}: {len(unique_docs)}ê°œ ë¬¸ì„œ (50ê°œ ë¯¸ë§Œ), ì¶”ê°€ ê²€ìƒ‰ ì‹œì‘...")
            
            # ì¶”ê°€ í‚¤ì›Œë“œ ì¶”ì¶œ (ë” ë„“ì€ ë²”ìœ„)
            if is_korean(real_query):
                # í•œêµ­ì–´: ë” ë§ì€ í‚¤ì›Œë“œ ì¶”ì¶œ
                additional_keywords = extract_more_korean_keywords(real_query, okt)
            else:
                # ì˜ì–´: ë” ë§ì€ í‚¤ì›Œë“œ ì¶”ì¶œ
                additional_keywords = extract_more_english_keywords(real_query)
            
            # ê¸°ì¡´ í‚¤ì›Œë“œì™€ ì¤‘ë³µ ì œê±°
            existing_keywords = set(search_keywords)
            new_keywords = [kw for kw in additional_keywords if kw not in existing_keywords]
            
            # ì¶”ê°€ ê²€ìƒ‰ (ìµœëŒ€ 10ê°œ í‚¤ì›Œë“œê¹Œì§€)
            for keyword in new_keywords[:10]:
                if len(unique_docs) >= 50:
                    break
                try:
                    docs = client.search_articles(keyword, row_count=5, fields=['title', 'abstract', 'CN'])
                    all_candidate_docs.extend(docs)
                    time.sleep(0.2)
                except Exception as e:
                    print(f"   âš ï¸  ì¶”ê°€ ê²€ìƒ‰ ì˜¤ë¥˜ (í‚¤ì›Œë“œ: {keyword}): {e}")
                    continue
            
            # ë‹¤ì‹œ ì¤‘ë³µ ì œê±°
            unique_docs = list({doc['CN']: doc for doc in all_candidate_docs if 'CN' in doc}.values())
            print(f"   - ì§ˆë¬¸ {question_id+1}: ì¶”ê°€ ê²€ìƒ‰ í›„ {len(unique_docs)}ê°œ ë¬¸ì„œ")
        all_questions_data.append({
            'query': real_query, 
            'id': question_id, 
            'candidates': unique_docs,
            'language': 'ko' if is_korean(real_query) else 'en'
        })
        
        print(f"   - ì§ˆë¬¸ {question_id+1}: {len(unique_docs)}ê°œ ë¬¸ì„œ ìˆ˜ì§‘ ì™„ë£Œ")

    # --- 3. [ë°°ì¹˜] 2ë‹¨ê³„: ì˜ë¯¸ ê¸°ë°˜ í•„í„°ë§ ë° ì¬ìˆœìœ„í™” ---
    print("\n--- [2/3] ì˜ë¯¸ ê¸°ë°˜ í•„í„°ë§ ë° ì¬ìˆœìœ„í™” ì‹œì‘ ---")
    
    for data in tqdm(all_questions_data, desc="   - Semantic Filtering & Re-ranking"):
        if not data['candidates']:
            data['final_docs'] = []
            continue

        # ì˜ë¯¸ ê¸°ë°˜ í•„í„°ë§ (ìƒìœ„ 20ê°œ ì„ íƒ)
        filtered_docs = simple_semantic_filtering(data['candidates'], data['query'])
        
        # ì¶”ê°€ ì¬ìˆœìœ„í™” (ìƒìœ„ 50ê°œê¹Œì§€ í™•ì¥)
        if len(filtered_docs) < 50 and len(data['candidates']) > len(filtered_docs):
            remaining_docs = [doc for doc in data['candidates'] if doc not in filtered_docs]
            # ë‚˜ë¨¸ì§€ ë¬¸ì„œë“¤ì„ ê°„ë‹¨í•œ ì ìˆ˜ë¡œ ì •ë ¬
            remaining_scored = []
            for doc in remaining_docs:
                title = doc.get('title', '').lower()
                abstract = doc.get('abstract', '').lower()
                content = f"{title} {abstract}"
                score = len([word for word in data['query'].lower().split() if word in content])
                remaining_scored.append((doc, score))
            
            remaining_scored.sort(key=lambda x: x[1], reverse=True)
            additional_docs = [doc for doc, score in remaining_scored[:50-len(filtered_docs)]]
            filtered_docs.extend(additional_docs)
        
        # ìµœëŒ€ 50ê°œë¡œ ì œí•œ
        data['final_docs'] = filtered_docs[:50]
        print(f"   - ì§ˆë¬¸ {data['id']+1}: {len(data['final_docs'])}ê°œ ë¬¸ì„œ í•„í„°ë§ ì™„ë£Œ")

    # --- 4. [ë°°ì¹˜] 3ë‹¨ê³„: ë‹µë³€ ìƒì„± ---
    print("\n--- [3/3] ë‹µë³€ ìƒì„± ì‹œì‘ ---")
    
    for data in tqdm(all_questions_data, desc="   - Generating Answers"):
        try:
            if not data['final_docs']:
                final_answer = "ì œê³µëœ ì°¸ê³  ë¬¸ì„œë¡œëŠ” ì§ˆë¬¸ì— ëŒ€í•œ ì¶©ë¶„í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            else:
                # ì»¨í…ìŠ¤íŠ¸ êµ¬ì¶• (ìƒìœ„ 5ê°œ ë¬¸ì„œë§Œ ì‚¬ìš©)
                context_parts = []
                for i, doc in enumerate(data['final_docs'][:5]):
                    title = doc.get('title', '')
                    abstract = doc.get('abstract', '')
                    doc_context = f"[ë¬¸ì„œ {i+1}]\nì œëª©: {title}\nì´ˆë¡: {abstract}\n"
                    context_parts.append(doc_context)
                
                context = "\n".join(context_parts)
                
                # í”„ë¡¬í”„íŠ¸ ìƒì„±
                prompt_template = create_final_prompt_v9(data['query'], context, data['language'])
                
                # Gemini APIë¡œ ë‹µë³€ ìƒì„± (ì‹¤íŒ¨ ì‹œ ëŒ€ì²´ ë‹µë³€ ì‚¬ìš©)
                try:
                    final_answer = gemini_client.generate_answer(prompt_template)
                    
                    # ë‹µë³€ í’ˆì§ˆ ê²€ì¦
                    if not final_answer or len(final_answer.strip()) < 20:
                        final_answer = generate_fallback_answer(data['query'], data['final_docs'], data['language'])
                        print(f"   âš ï¸  Step 3: Generated answer is too short, using fallback.")
                    else:
                        print(f"   âœ… Step 3: Answer generation complete ({len(final_answer)} characters).")
                        
                except Exception as e:
                    print(f"   âš ï¸  Gemini API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)[:100]}...")
                    final_answer = generate_fallback_answer(data['query'], data['final_docs'], data['language'])
                    print(f"   âœ… Step 3: Using fallback answer generation.")
            
            # ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë©”ëª¨ë¦¬ì— ì €ì¥
            predictions.append(final_answer)
            
            # ìƒìœ„ 50ê°œ ë…¼ë¬¸ ì •ë³´ ì¶”ì¶œ (test.csv í˜•ì‹ì— ë§ì¶°)
            article_titles = []
            for i, doc in enumerate(data['final_docs'][:50]):
                formatted_article = create_kaggle_format_article(doc, i+1)
                article_titles.append(formatted_article)
            
            # 50ê°œê°€ ë˜ë„ë¡ ë¹ˆ ë¬¸ìì—´ë¡œ ì±„ì›€
            while len(article_titles) < 50:
                article_titles.append('')
            
            predicted_articles.append(article_titles)
            
        except Exception as e:
            print(f"   âš ï¸  ë‹µë³€ ìƒì„± ì˜¤ë¥˜ (ì§ˆë¬¸ {data['id']+1}): {e}")
            # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ê°’ìœ¼ë¡œ ì±„ì›€
            predictions.append(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
            predicted_articles.append([''] * 50)  # 50ê°œ ë¹ˆ ë¬¸ìì—´

    # --- 5. ìµœì¢… ì œì¶œ íŒŒì¼ ìƒì„± (test.csv í˜•ì‹ì— ë§ì¶°) ---
    print("\n--- ìµœì¢… ì œì¶œ íŒŒì¼ ìƒì„± ---")
    
    try:
        # test_dfë¥¼ ë³µì‚¬í•˜ì—¬ base DataFrame ìƒì„±
        submission_df = test_df.copy()
        
        # Prediction ì»¬ëŸ¼ ì¶”ê°€
        submission_df['Prediction'] = predictions
        
        # 50ê°œ prediction_retrieved_article_name ì»¬ëŸ¼ ì¶”ê°€ (ëª¨ë‘ ë¹ˆ ë¬¸ìì—´ë¡œ ì´ˆê¸°í™”)
        for i in range(1, 51):
            column_name = f'prediction_retrieved_article_name_{i}'
            submission_df[column_name] = [''] * len(submission_df)
        
        # ì´ì œ ê° ì»¬ëŸ¼ì— ì‹¤ì œ ê°’ ì±„ìš°ê¸°
        for i in range(1, 51):
            column_name = f'prediction_retrieved_article_name_{i}'
            for row_idx, articles in enumerate(predicted_articles):
                if i-1 < len(articles) and articles[i-1]:
                    submission_df.at[row_idx, column_name] = articles[i-1]
        
        submission_path = 'submission.csv'
        # ëª¨ë“  null ê°’ì„ ë¹ˆ ë¬¸ìì—´ë¡œ ë³€í™˜
        submission_df = submission_df.fillna('')
        submission_df.to_csv(submission_path, index=False, encoding='utf-8-sig')
        
        end_total_time = time.time()
        total_time = end_total_time - start_total_time
        
        print(f"\nğŸ‰ Kaggle ì œì¶œìš© íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
        print(f"   - ì´ ì†Œìš” ì‹œê°„: {total_time:.2f}ì´ˆ ({total_time/60:.1f}ë¶„)")
        print(f"   - í‰ê·  ì²˜ë¦¬ ì‹œê°„: {total_time/len(test_df):.2f}ì´ˆ/ì§ˆë¬¸")
        print(f"   - ìµœì¢… ì œì¶œ íŒŒì¼: {submission_path}")
        print(f"   - íŒŒì¼ í¬ê¸°: {len(submission_df)} í–‰ Ã— {len(submission_df.columns)} ì—´")
        
        # ì„±ê³µë¥  ê³„ì‚°
        successful_count = len([p for p in predictions if 'ì˜¤ë¥˜' not in p])
        success_rate = (successful_count / len(test_df)) * 100
        print(f"   - ì„±ê³µë¥ : {successful_count}/{len(test_df)} ({success_rate:.1f}%)")
        
        # íŒŒì¼ ê²€ì¦
        print(f"\nğŸ“Š íŒŒì¼ ê²€ì¦:")
        print(f"   - ì´ ì§ˆë¬¸ ìˆ˜: {len(submission_df)}")
        print(f"   - ë‹µë³€ ìƒì„±ëœ ì§ˆë¬¸ ìˆ˜: {len(submission_df[submission_df['Prediction'].notna() & (submission_df['Prediction'] != '')])}")
        print(f"   - ë…¼ë¬¸ ê²€ìƒ‰ëœ ì§ˆë¬¸ ìˆ˜: {len(submission_df[submission_df['prediction_retrieved_article_name_1'].notna() & (submission_df['prediction_retrieved_article_name_1'] != '')])}")
        
        # null ê°’ í™•ì¸
        null_counts = submission_df.isnull().sum()
        if null_counts.sum() > 0:
            print(f"   âš ï¸  null ê°’ ë°œê²¬: {null_counts[null_counts > 0].to_dict()}")
        else:
            print(f"   âœ… null ê°’ ì—†ìŒ")
        
        # ì»¬ëŸ¼ êµ¬ì¡° í™•ì¸
        print(f"\nğŸ“‹ ì»¬ëŸ¼ êµ¬ì¡°:")
        print(f"   - ì›ë³¸ ì»¬ëŸ¼ ìˆ˜: {len(test_df.columns)}")
        print(f"   - ìµœì¢… ì»¬ëŸ¼ ìˆ˜: {len(submission_df.columns)}")
        print(f"   - ì¶”ê°€ëœ ì»¬ëŸ¼: Prediction, prediction_retrieved_article_name_1 ~ prediction_retrieved_article_name_50")
        
    except Exception as e:
        print(f"âŒ ì œì¶œ íŒŒì¼ ìƒì„± ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    main()

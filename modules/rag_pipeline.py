"""
RAG íŒŒì´í”„ë¼ì¸ ë©”ì¸ ëª¨ë“ˆ (ê°œì„ ëœ ë²„ì „)
- ì „ì²´ RAG ì›Œí¬í”Œë¡œìš° ê´€ë¦¬
- ëª¨ë“ˆ ê°„ í˜‘ë ¥ ì¡°ì •
- ê²°ê³¼ ìƒì„± ë° ê²€ì¦
- í–¥ìƒëœ ê²€ìƒ‰ ì‹œìŠ¤í…œ í†µí•©
"""

import sys
import logging
from typing import List, Dict, Tuple
from .document_manager import DocumentManager
from .search_engine import FlexibleSearchEngine
from .search_tools import ScienceONTool
from .search_methods import KeywordSearchMethod, HybridSearchMethod, SemanticSearchMethod
from .keyword_extractors import LLMKeywordExtractor, BasicKeywordExtractor
from .reranking import DocumentReranker
from .answer_generator import AnswerGenerator
from .config import SEARCH_CONFIG, ANSWER_CONFIG, TEST_CONFIG

class RAGPipeline:
    """RAG íŒŒì´í”„ë¼ì¸ ë©”ì¸ í´ë˜ìŠ¤ (ê°œì„ ëœ ë²„ì „)"""
    
    def __init__(self, api_client, gemini_client, dataset_name: str = "scienceon"):
        """
        RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
        
        Args:
            api_client: ScienceON API í´ë¼ì´ì–¸íŠ¸
            gemini_client: Gemini API í´ë¼ì´ì–¸íŠ¸
            dataset_name: ë°ì´í„°ì…‹ ì´ë¦„
        """
        self.dataset_name = dataset_name
        
        # ë²¡í„° DB ì´ˆê¸°í™” ì—¬ë¶€ í™•ì¸
        clear_db = TEST_CONFIG.get('clear_vector_db', False)
        
        # ê° ëª¨ë“ˆ ì´ˆê¸°í™”
        self.document_manager = DocumentManager(clear_db=clear_db)
        
        # ê²€ìƒ‰ ë„êµ¬ë“¤ ì´ˆê¸°í™”
        scienceon_tool = ScienceONTool(api_client)
        
        # ê²€ìƒ‰ ë°©ë²•ë“¤ ì´ˆê¸°í™”
        keyword_method = KeywordSearchMethod()
        hybrid_method = HybridSearchMethod()
        semantic_method = SemanticSearchMethod()
        
        # ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”
        self.search_engine = FlexibleSearchEngine(self.document_manager)
        self.search_engine.register_tool("scienceon", scienceon_tool, is_default=True)
        self.search_engine.register_method("keyword", keyword_method)
        self.search_engine.register_method("hybrid", hybrid_method, is_default=True)
        self.search_engine.register_method("semantic", semantic_method)
        
        # í‚¤ì›Œë“œ ì¶”ì¶œê¸°ë“¤ ì´ˆê¸°í™”
        self.keyword_extractors = {
            'llm': LLMKeywordExtractor(gemini_client),
            'basic': BasicKeywordExtractor()
        }
        
        self.reranker = DocumentReranker()
        self.answer_generator = AnswerGenerator(gemini_client)
        
        logging.info("âœ… í–¥ìƒëœ RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def process_question(self, question_id: int, query: str) -> Tuple[str, List[str]]:
        """
        ë‹¨ì¼ ì§ˆë¬¸ ì²˜ë¦¬ (ì „ì²´ RAG ì›Œí¬í”Œë¡œìš°)
        
        Args:
            question_id: ì§ˆë¬¸ ID
            query: ì§ˆë¬¸ ë‚´ìš©
            
        Returns:
            (ë‹µë³€, ë…¼ë¬¸ ì •ë³´ ë¦¬ìŠ¤íŠ¸) íŠœí”Œ
        """
        print(f"\nğŸ” ì§ˆë¬¸ {question_id+1} ì²˜ë¦¬: '{query[:50]}...'")
        
        try:
            # 1ë‹¨ê³„: ë¬¸ì„œ ê²€ìƒ‰
            documents = self._retrieve_documents(query)
            print(f"   ğŸ“š ê²€ìƒ‰ëœ ë¬¸ì„œ: {len(documents)}ê°œ")
            
            # 2ë‹¨ê³„: ë²¡í„° DBì— ì €ì¥
            added_count = self.document_manager.store_documents(documents, query)
            if added_count > 0:
                print(f"   ğŸ“š ë²¡í„° DBì— {added_count}ê°œ ë¬¸ì„œ ì¶”ê°€")
            
            # 3ë‹¨ê³„: ë²¡í„° ê²€ìƒ‰
            similar_docs = self.document_manager.search_similar_documents(query)
            
            # 4ë‹¨ê³„: ê²€ìƒ‰ ê²°ê³¼ ë³´ì¶© (ê¸°ì¡´ ë¬¸ì„œì™€ ìœ ì‚¬ ë¬¸ì„œ ê²°í•©)
            final_docs = documents + similar_docs
            
            # 5ë‹¨ê³„: ì¬ìˆœìœ„í™”
            reranked_docs = self.reranker.rerank_documents(final_docs, query)
            
            # 6ë‹¨ê³„: ë‹µë³€ ìƒì„±ìš© ìƒìœ„ ë¬¸ì„œ ì„ íƒ
            context_docs = self.reranker.get_top_documents(reranked_docs)
            
            # 7ë‹¨ê³„: ë‹µë³€ ìƒì„±
            answer = self.answer_generator.generate_quality_answer(query, context_docs)
            
            # 8ë‹¨ê³„: ë…¼ë¬¸ ì •ë³´ í˜•ì‹í™”
            articles = self._format_articles(reranked_docs)
            
            return answer, articles
            
        except Exception as e:
            print(f"   âŒ ì§ˆë¬¸ {question_id+1} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}", [''] * 50
    
    def _retrieve_documents(self, query: str, search_strategy: str = None) -> List[Dict]:
        """
        ë¬¸ì„œ ê²€ìƒ‰ (í–¥ìƒëœ ê²€ìƒ‰ ì‹œìŠ¤í…œ ì‚¬ìš©)
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            search_strategy: ê²€ìƒ‰ ì „ëµ
            
        Returns:
            ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        # í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = self.keyword_extractors['llm'].extract_keywords(query)
        
        # ê²€ìƒ‰ ì‹¤í–‰
        documents, search_metadata = self.search_engine.search(
            query, 
            dataset_name=self.dataset_name,
            method=search_strategy,
            keywords=keywords
        )
        
        return documents
    
    def _format_articles(self, documents: List[Dict]) -> List[str]:
        """
        ë¬¸ì„œë¥¼ Kaggle í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ì‹¤ì œ 50ê°œ ë¬¸ì„œ ë³´ì¥)
        
        Args:
            documents: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            Kaggle í˜•ì‹ì˜ ë…¼ë¬¸ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        articles = []
        
        # ì‹¤ì œ ë¬¸ì„œê°€ 50ê°œ ë¯¸ë§Œì´ë©´ ì¶”ê°€ ê²€ìƒ‰
        if len(documents) < 50:
            print(f"   ğŸš¨ ì‹¤ì œ ë¬¸ì„œ ë¶€ì¡±: {len(documents)}ê°œ (ëª©í‘œ: 50ê°œ)")
            print(f"   ğŸ” ì¶”ê°€ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...")
            
            # ì¶”ê°€ ê²€ìƒ‰ì„ ìœ„í•´ search_engine ì‚¬ìš©
            additional_docs, _ = self.search_engine.search(
                "", 
                dataset_name=self.dataset_name,
                method="keyword",
                keywords=["research", "study", "analysis"]
            )
            documents.extend(additional_docs)
            
            # ì¤‘ë³µ ì œê±°
            seen_ids = set()
            unique_docs = []
            for doc in documents:
                doc_id = doc.get('CN')
                if doc_id and doc_id not in seen_ids:
                    seen_ids.add(doc_id)
                    unique_docs.append(doc)
            documents = unique_docs
            
            print(f"   ğŸ“Š ì¶”ê°€ ê²€ìƒ‰ í›„: {len(documents)}ê°œ ë¬¸ì„œ")
        
        # ì •í™•íˆ 50ê°œ ë¬¸ì„œë§Œ ì‚¬ìš©
        documents = documents[:50]
        
        for doc in documents:
            formatted_article = self._create_kaggle_format_article(doc)
            if not formatted_article or formatted_article.strip() == '':
                # ë¹ˆ ë¬¸ì„œì¸ ê²½ìš° ê¸°ë³¸ê°’ìœ¼ë¡œ ëŒ€ì²´
                formatted_article = f'Title: {doc.get("title", "Research Document")}, Abstract: {doc.get("abstract", "This document contains relevant research information.")}, Source: http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=05787966&target=NART&cn={doc.get("CN", "DOCUMENT")}'
            articles.append(formatted_article)
        
        # ì •í™•íˆ 50ê°œ ë°˜í™˜
        return articles[:50]
    
    def _create_kaggle_format_article(self, doc: Dict) -> str:
        """
        Kaggle í˜•ì‹ìœ¼ë¡œ ë…¼ë¬¸ ì •ë³´ ìƒì„±
        
        Args:
            doc: ë¬¸ì„œ ë”•ì…”ë„ˆë¦¬
            
        Returns:
            Kaggle í˜•ì‹ì˜ ë¬¸ìì—´
        """
        title = doc.get('title', '')
        abstract = doc.get('abstract', '')
        cn = doc.get('CN', '')
        
        # í•„ìˆ˜ í•„ë“œ ê²€ì¦
        if not title or not cn:
            return ''
        
        # Source URL ìƒì„±
        source_url = f"http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=05787966&target=NART&cn={cn}"
        
        # Abstractê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ë¡œ ì²˜ë¦¬
        if not abstract:
            abstract = ''
        
        # Kaggle í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…
        return f'Title: {title}, Abstract: {abstract}, Source: {source_url}'
    
    def batch_process_questions(self, questions: List[Tuple[int, str]]) -> List[Tuple[int, str, List[str]]]:
        """
        ë°°ì¹˜ ì§ˆë¬¸ ì²˜ë¦¬
        
        Args:
            questions: (ì§ˆë¬¸ ID, ì§ˆë¬¸) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            (ì§ˆë¬¸ ID, ë‹µë³€, ë…¼ë¬¸ ì •ë³´) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        """
        results = []
        
        for question_id, query in questions:
            answer, articles = self.process_question(question_id, query)
            results.append((question_id, answer, articles))
        
        return results
    
    def get_pipeline_stats(self) -> Dict:
        """
        íŒŒì´í”„ë¼ì¸ í†µê³„ ì •ë³´
        
        Returns:
            í†µê³„ ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        vector_stats = self.document_manager.get_stats()
        
        return {
            "vector_db": vector_stats,
            "search_config": SEARCH_CONFIG,
            "answer_config": ANSWER_CONFIG
        }

#!/usr/bin/env python3
"""
ê²€ìƒ‰ ë©”íƒ€ë°ì´í„° ìƒì„± ì‹œìŠ¤í…œ
- LLMì„ ì‚¬ìš©í•œ í‚¤ì›Œë“œ ì¶”ì¶œ
- ScienceON APIë¥¼ í†µí•œ ë¬¸ì„œ ê²€ìƒ‰
- êµ¬ì¡°í™”ëœ JSON íŒŒì¼ ìƒì„±
"""

import os
import json
import logging
import pandas as pd
from typing import List, Dict, Any, Optional
from pathlib import Path
from datetime import datetime
import google.generativeai as genai

# ê¸°ì¡´ ScienceON API í´ë¼ì´ì–¸íŠ¸ import
from scienceon_api_example import ScienceONAPIClient

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

class KeywordExtractor:
    """LLMì„ ì‚¬ìš©í•œ í‚¤ì›Œë“œ ì¶”ì¶œê¸°"""
    
    def __init__(self, api_key: str, model_name: str = "gemini-2.5-flash"):
        """
        í‚¤ì›Œë“œ ì¶”ì¶œê¸° ì´ˆê¸°í™”
        
        Args:
            api_key: Google API í‚¤
            model_name: ì‚¬ìš©í•  Gemini ëª¨ë¸ëª…
        """
        self.api_key = api_key
        self.model_name = model_name
        self.model = self._init_gemini()
        
    def _init_gemini(self) -> genai.GenerativeModel:
        """Gemini ëª¨ë¸ ì´ˆê¸°í™”"""
        genai.configure(api_key=self.api_key)
        generation_config = genai.GenerationConfig(
            temperature=0.2,
            candidate_count=1,
        )
        
        return genai.GenerativeModel(
            model_name=self.model_name,
            generation_config=generation_config,
        )
    
    def _create_keyword_prompt(self, query: str) -> str:
        """í‚¤ì›Œë“œ ì¶”ì¶œì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        prompt = f"""ë‹¹ì‹ ì€ ë…¼ë¬¸ ê²€ìƒ‰ì„ ìœ„í•œ í‚¤ì›Œë“œ ì¶”ì¶œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì§ˆë¬¸ì—ì„œ ScienceON API ê²€ìƒ‰ì— ìµœì í™”ëœ í•µì‹¬ í‚¤ì›Œë“œë“¤ì„ í•œêµ­ì–´ì™€ ì˜ì–´ë¡œ ê°ê° ì¶”ì¶œí•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: "{query}"

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì„¸ìš”:

1. í•œêµ­ì–´ í‚¤ì›Œë“œ: 3-5ê°œì˜ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì‰¼í‘œë¡œ êµ¬ë¶„ (ì „ìêµê³¼ì„œ)
2. ì˜ì–´ í‚¤ì›Œë“œ: ìœ„ í•œêµ­ì–´ í‚¤ì›Œë“œë“¤ì˜ ì˜ì–´ ë²ˆì—­ì„ ì‰¼í‘œë¡œ êµ¬ë¶„

ê·œì¹™:
- ì „ë¬¸ìš©ì–´ì™€ ê¸°ìˆ ìš©ì–´ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì„ íƒ
- ì¶•ì•½ì–´ê°€ ìˆìœ¼ë©´ ì¶•ì•½ì–´ ì‚¬ìš© (ì˜ˆ: AI, ML, NLP)
- ê° í‚¤ì›Œë“œëŠ” 1-20ì ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ


ì¶œë ¥ í˜•ì‹:
í•œêµ­ì–´: í‚¤ì›Œë“œ1, í‚¤ì›Œë“œ2, í‚¤ì›Œë“œ3, í‚¤ì›Œë“œ4
ì˜ì–´: keyword1, keyword2, keyword3, keyword4



í‚¤ì›Œë“œ:"""
        return prompt
    
    def extract_keywords(self, query: str) -> Dict[str, List[str]]:
        """
        ì§ˆë¬¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ (í•œêµ­ì–´, ì˜ì–´ ê°ê°)
        
        Args:
            query: ì¶”ì¶œí•  ì§ˆë¬¸
            
        Returns:
            {'korean': [í‚¤ì›Œë“œë“¤], 'english': [í‚¤ì›Œë“œë“¤]} í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬
        """
        try:
            prompt = self._create_keyword_prompt(query)
            response = self.model.generate_content(prompt)
            
            # ì‘ë‹µì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
            response_text = response.text.strip()
            
            # "í‚¤ì›Œë“œ:" í…ìŠ¤íŠ¸ ì œê±°
            if response_text.startswith("í‚¤ì›Œë“œ:"):
                response_text = response_text[4:].strip()
            
            # í•œêµ­ì–´ì™€ ì˜ì–´ í‚¤ì›Œë“œ ë¶„ë¦¬
            korean_keywords = []
            english_keywords = []
            
            lines = response_text.split('\n')
            for line in lines:
                line = line.strip()
                if line.startswith('í•œêµ­ì–´:'):
                    korean_text = line.replace('í•œêµ­ì–´:', '').strip()
                    korean_keywords = [kw.strip() for kw in korean_text.split(',') if kw.strip()]
                elif line.startswith('ì˜ì–´:'):
                    english_text = line.replace('ì˜ì–´:', '').strip()
                    english_keywords = [kw.strip() for kw in english_text.split(',') if kw.strip()]
            
            # ë¹ˆ í‚¤ì›Œë“œ ì œê±° ë° ê¸¸ì´ ì œí•œ
            korean_keywords = [kw for kw in korean_keywords if kw and 1 < len(kw) <= 30]
            english_keywords = [kw for kw in english_keywords if kw and 1 < len(kw) <= 30]
            
            result = {
                'korean': korean_keywords,
                'english': english_keywords
            }
            
            logging.info(f"ì§ˆë¬¸: {query}")
            logging.info(f"í•œêµ­ì–´ í‚¤ì›Œë“œ: {korean_keywords}")
            logging.info(f"ì˜ì–´ í‚¤ì›Œë“œ: {english_keywords}")
            
            return result
            
        except Exception as e:
            logging.error(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {'korean': [], 'english': []}


class SearchQueryGenerator:
    """Geminië¥¼ í™œìš©í•œ ì§€ëŠ¥ì  ê²€ìƒ‰ì–´ ìƒì„±ê¸°"""
    
    def __init__(self, api_key: str, model_name: str = "gemini-2.5-flash"):
        """
        ê²€ìƒ‰ì–´ ìƒì„±ê¸° ì´ˆê¸°í™”
        
        Args:
            api_key: Google Gemini API í‚¤
            model_name: ì‚¬ìš©í•  Gemini ëª¨ë¸ëª…
        """
        self.model = self._init_gemini(api_key, model_name)
    
    def _init_gemini(self, api_key: str, model_name: str):
        """Gemini ëª¨ë¸ ì´ˆê¸°í™”"""
        import google.generativeai as genai
        genai.configure(api_key=api_key)
        return genai.GenerativeModel(model_name)
    
    def _create_search_query_prompt(self, query: str, keywords_dict: Dict[str, List[str]]) -> str:
        """ê²€ìƒ‰ì–´ ìƒì„±ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        korean_keywords = keywords_dict.get('korean', [])
        english_keywords = keywords_dict.get('english', [])
        
        prompt = f"""ë‹¹ì‹ ì€ ScienceON API ê²€ìƒ‰ì„ ìœ„í•œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì§ˆë¬¸ê³¼ í‚¤ì›Œë“œë“¤ì„ ë°”íƒ•ìœ¼ë¡œ íš¨ê³¼ì ì¸ ê²€ìƒ‰ì–´ë“¤ì„ ìƒì„±í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: "{query}"

í•œêµ­ì–´ í‚¤ì›Œë“œ: {', '.join(korean_keywords)}
ì˜ì–´ í‚¤ì›Œë“œ: {', '.join(english_keywords)}

ë‹¤ìŒ ê²€ìƒ‰ ì—°ì‚°ìë“¤ì„ í™œìš©í•˜ì—¬ ê²€ìƒ‰ì–´ë¥¼ ìƒì„±í•˜ì„¸ìš”:

1. ê³µë°± ì—°ì‚°ì: ë‘ ê°œ ì´ìƒì˜ ê²€ìƒ‰ì–´ë¥¼ ëª¨ë‘ í¬í•¨í•˜ëŠ” ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
   ì˜ˆ: "ë‚˜ë…¸ ê¸°ê³„"

2. | ì—°ì‚°ì: ë‘ ê°œ ì´ìƒì˜ ê²€ìƒ‰ì–´ ì¤‘ 1ê°œ ì´ìƒì„ í¬í•¨í•˜ëŠ” ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
   ì˜ˆ: "ë‚˜ë…¸|ê¸°ê³„"

3. * ì—°ì‚°ì: ê²€ìƒ‰ì–´ ë’¤ 0ê°œ ì´ìƒì˜ ì„ì˜ì˜ ë¬¸ìê°€ í¬í•¨ëœ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
   ì˜ˆ: "ë‚˜ë…¸*"

4. ( ) ì—°ì‚°ì: ê´„í˜¸ ì•ˆì˜ ê²€ìƒ‰ì–´ê°€ ìš°ì„ ìˆœìœ„ë¡œ ì§€ì •ë©ë‹ˆë‹¤.
   ì˜ˆ: "ë‚˜ë…¸ (ê¸°ê³„ | machine)"

ì¤‘ìš”í•œ ê·œì¹™:
- ì§ˆë¬¸ì˜ í•µì‹¬ ì˜ë„ì™€ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ê²€ìƒ‰ì–´ë¥¼ ë¨¼ì € ìƒì„±í•˜ì„¸ìš”
- ìì—°ìŠ¤ëŸ¬ìš´ êµ¬ë¬¸ê³¼ ë¬¸ë§¥ì„ ê³ ë ¤í•œ ê²€ìƒ‰ì–´ë¥¼ ìš°ì„ í•˜ì„¸ìš”
- ì˜ˆ: "AI mathematics machine learning textbook" ê°™ì€ ìì—°ìŠ¤ëŸ¬ìš´ êµ¬ë¬¸
- ë‹¨ìˆœí•˜ê³  ì‹¤ìš©ì ì¸ ê²€ìƒ‰ì–´ë¥¼ ìš°ì„ í•˜ì„¸ìš”
- ë³µì¡í•œ ë”°ì˜´í‘œë‚˜ ì •í™•í•œ êµ¬ë¬¸ ê²€ìƒ‰ì€ í”¼í•˜ì„¸ìš”
- ê° ê²€ìƒ‰ì–´ëŠ” í•œ ì¤„ì— í•˜ë‚˜ì”© ì‘ì„±í•˜ì„¸ìš”
- ìµœëŒ€ 12ê°œì˜ ê²€ìƒ‰ì–´ë¥¼ ìƒì„±í•˜ì„¸ìš”
- ì„¤ëª…ì´ë‚˜ ë²ˆí˜¸ ì—†ì´ ê²€ìƒ‰ì–´ë§Œ ë‚˜ì—´í•˜ì„¸ìš”

ê²€ìƒ‰ì–´ ëª©ë¡:"""
        return prompt
    
    def generate_search_queries(self, query: str, keywords_dict: Dict[str, List[str]]) -> List[str]:
        """
        Geminië¥¼ í™œìš©í•˜ì—¬ ì§€ëŠ¥ì ì¸ ê²€ìƒ‰ì–´ ìƒì„±
        
        Args:
            query: ì›ë³¸ ì§ˆë¬¸
            keywords_dict: {'korean': [...], 'english': [...]} í˜•íƒœì˜ í‚¤ì›Œë“œ ë”•ì…”ë„ˆë¦¬
            
        Returns:
            ê²€ìƒ‰ì–´ ë¦¬ìŠ¤íŠ¸
        """
        try:
            prompt = self._create_search_query_prompt(query, keywords_dict)
            response = self.model.generate_content(prompt)
            response_text = response.text.strip()
            
            # ê²€ìƒ‰ì–´ íŒŒì‹±
            search_queries = []
            lines = response_text.split('\n')
            
            for line in lines:
                line = line.strip()
                
                # ì„¤ëª…ì ì¸ í…ìŠ¤íŠ¸ë‚˜ ë¹ˆ ì¤„ ì œì™¸
                if (not line or 
                    line.startswith('ê²€ìƒ‰ì–´:') or 
                    line.startswith('ì˜ˆ:') or
                    line.startswith('ë‹¤ìŒì€') or
                    line.startswith('ScienceON') or
                    line.startswith('ê·œì¹™:') or
                    line.startswith('ì§ˆë¬¸:') or
                    line.startswith('í•œêµ­ì–´ í‚¤ì›Œë“œ:') or
                    line.startswith('ì˜ì–´ í‚¤ì›Œë“œ:') or
                    len(line) < 3):
                    continue
                
                # ë²ˆí˜¸ë‚˜ ê¸°í˜¸ ì œê±°
                clean_line = line
                
                # ë²ˆí˜¸ íŒ¨í„´ ì œê±° (1., 2., 10., 11. ë“±)
                import re
                clean_line = re.sub(r'^\d+\.\s*', '', clean_line)
                
                # ê¸°í˜¸ ì œê±°
                if clean_line.startswith(('-', 'â€¢', '.', '`')):
                    clean_line = clean_line[1:].strip()
                
                # ë°±í‹± ì œê±°
                clean_line = clean_line.replace('`', '').strip()
                
                # ë”°ì˜´í‘œ ì •ë¦¬ - ì›ë³¸ ìœ ì§€
                clean_line = clean_line.replace('"', '"').replace('"', '"')
                
                if clean_line and len(clean_line) <= 100 and not clean_line.startswith('ë‹¤ìŒì€'):
                    search_queries.append(clean_line)
            
            # ì¤‘ë³µ ì œê±°
            unique_queries = []
            seen = set()
            for query in search_queries:
                if query not in seen:
                    unique_queries.append(query)
                    seen.add(query)
            
            logging.info(f"Geminiê°€ ìƒì„±í•œ ê²€ìƒ‰ì–´: {unique_queries}")
            return unique_queries[:15]  # ìµœëŒ€ 15ê°œë¡œ ì œí•œ
            
        except Exception as e:
            logging.error(f"ê²€ìƒ‰ì–´ ìƒì„± ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ í‚¤ì›Œë“œë“¤ ë°˜í™˜
            korean_keywords = keywords_dict.get('korean', [])
            english_keywords = keywords_dict.get('english', [])
            return korean_keywords + english_keywords


class SearchMetaGenerator:
    """ê²€ìƒ‰ ë©”íƒ€ë°ì´í„° ìƒì„±ê¸° - í‚¤ì›Œë“œ ì¶”ì¶œ + ë¬¸ì„œ ê²€ìƒ‰"""
    
    def __init__(self, gemini_api_key: str, scienceon_credentials_path: str = "./configs/scienceon_api_credentials.json"):
        """
        ê²€ìƒ‰ ë©”íƒ€ë°ì´í„° ìƒì„±ê¸° ì´ˆê¸°í™”
        
        Args:
            gemini_api_key: Gemini API í‚¤
            scienceon_credentials_path: ScienceON API ìê²©ì¦ëª… íŒŒì¼ ê²½ë¡œ
        """
        self.keyword_extractor = KeywordExtractor(gemini_api_key)
        self.scienceon_client = ScienceONAPIClient(Path(scienceon_credentials_path))
        self.query_generator = SearchQueryGenerator(gemini_api_key)
    
    def _prepare_search_query_for_api(self, search_query: str) -> str:
        """
        ê²€ìƒ‰ì–´ë¥¼ ScienceON APIì— ì „ë‹¬í•˜ê¸° ìœ„í•´ ì¤€ë¹„
        
        Args:
            search_query: ì›ë³¸ ê²€ìƒ‰ì–´
            
        Returns:
            APIìš© ê²€ìƒ‰ì–´
        """
        # JSONì—ì„œ ì´ìŠ¤ì¼€ì´í”„ëœ ë”°ì˜´í‘œë¥¼ ì›ë˜ ë”°ì˜´í‘œë¡œ ë³µì›
        api_query = search_query.replace('\\"', '"')
        
        # ë°±ìŠ¬ë˜ì‹œê°€ í¬í•¨ëœ ê²½ìš° ì œê±°
        api_query = api_query.replace('\\', '')
        
        return api_query
    
    def process_query(self, query: str, min_documents: int = 50) -> Dict[str, Any]:
        """
        ë‹¨ì¼ ì¿¼ë¦¬ ì²˜ë¦¬: í‚¤ì›Œë“œ ì¶”ì¶œ + ë¬¸ì„œ ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰í•  ì§ˆë¬¸
            min_documents: ìµœì†Œ ë³´ì¥ ë¬¸ì„œ ìˆ˜ (ê¸°ë³¸ 50ê°œ)
            
        Returns:
            ì²˜ë¦¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        logging.info(f"ì§ˆë¬¸ ì²˜ë¦¬ ì‹œì‘: {query[:50]}...")
        
        # 1. í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords_dict = self.keyword_extractor.extract_keywords(query)
        korean_keywords = keywords_dict.get('korean', [])
        english_keywords = keywords_dict.get('english', [])
        
        # 2. ê²€ìƒ‰ì–´ ìƒì„± (Gemini í™œìš©)
        search_queries = self.query_generator.generate_search_queries(query, keywords_dict)
        logging.info(f"ìƒì„±ëœ ê²€ìƒ‰ì–´ {len(search_queries)}ê°œ: {search_queries[:5]}...")  # ì²˜ìŒ 5ê°œë§Œ ë¡œê·¸
        
        # 3. ì§ˆë¬¸ ì–¸ì–´ ê°ì§€ ë° ìš°ì„  ê²€ìƒ‰ì–´ ì„ íƒ
        is_english_query = any(char.isascii() and char.isalpha() for char in query[:50])
        
        if is_english_query:
            logging.info(f"ì˜ì–´ ì§ˆë¬¸ ê°ì§€: ì˜ì–´ ìš°ì„  ê²€ìƒ‰ì–´ ì‚¬ìš©")
        else:
            logging.info(f"í•œêµ­ì–´ ì§ˆë¬¸ ê°ì§€: í•œêµ­ì–´ ìš°ì„  ê²€ìƒ‰ì–´ ì‚¬ìš©")
        
        # 4. ê²€ìƒ‰ì–´ë¡œ ë¬¸ì„œ ê²€ìƒ‰ (50ê°œ ì´ìƒ í™•ë³´í•  ë•Œê¹Œì§€ ë°˜ë³µ)
        all_documents = []
        page = 1
        
        while len(all_documents) < min_documents and page <= 2:
            logging.info(f"í˜ì´ì§€ {page} ê²€ìƒ‰ ì¤‘... (í˜„ì¬ {len(all_documents)}ê°œ ë¬¸ì„œ)")
            
            # ê²€ìƒ‰ì–´ë¡œ ê²€ìƒ‰ (í˜ì´ì§€ë‹¹ ìµœëŒ€ 10ê°œ ê²€ìƒ‰ì–´)
            queries_per_page = search_queries[:10] if page == 1 else search_queries[10:20]
            
            for search_query in queries_per_page:
                # ê²€ìƒ‰ì–´ë¥¼ APIì— ì „ë‹¬í•  ë•Œ ë”°ì˜´í‘œ ì²˜ë¦¬
                api_query = self._prepare_search_query_for_api(search_query)
                docs = self.scienceon_client.search_articles(api_query, cur_page=page, row_count=20)
                all_documents.extend(docs)
                
                if len(all_documents) >= min_documents:
                    break
            
            page += 1
        
        # 5. ì¤‘ë³µ ì œê±° (ì œëª© ê¸°ì¤€)
        unique_docs = []
        seen_titles = set()
        for doc in all_documents:
            title = doc.get('title', '')
            if title and title not in seen_titles:
                # source í•„ë“œ ì œê±° (ScienceONì´ ìëª…í•˜ë¯€ë¡œ)
                if 'source' in doc:
                    del doc['source']
                unique_docs.append(doc)
                seen_titles.add(title)
        
        # 6. ê²°ê³¼ ì •ë¦¬
        result = {
            'question': query,
            'keywords': {
                'korean': korean_keywords,
                'english': english_keywords
            },
            'search_queries': search_queries,
            'documents': unique_docs,  # ëª¨ë“  ë¬¸ì„œ í¬í•¨
            'total_documents_found': len(unique_docs),
            'search_timestamp': datetime.now().isoformat()
        }
        
        total_keywords = len(korean_keywords) + len(english_keywords)
        logging.info(f"ì§ˆë¬¸ ì²˜ë¦¬ ì™„ë£Œ: {total_keywords}ê°œ í‚¤ì›Œë“œ â†’ {len(search_queries)}ê°œ ê²€ìƒ‰ì–´ â†’ {len(unique_docs)}ê°œ ë¬¸ì„œ")
        return result
    
    def process_queries(self, queries: List[str], min_documents_per_query: int = 50) -> List[Dict[str, Any]]:
        """
        ì—¬ëŸ¬ ì¿¼ë¦¬ ì¼ê´„ ì²˜ë¦¬
        
        Args:
            queries: ì²˜ë¦¬í•  ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸
            min_documents_per_query: ì¿¼ë¦¬ë‹¹ ìµœì†Œ ë³´ì¥ ë¬¸ì„œ ìˆ˜ (ê¸°ë³¸ 50ê°œ)
            
        Returns:
            ì²˜ë¦¬ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        results = []
        
        for i, query in enumerate(queries, 1):
            logging.info(f"ì§„í–‰ë¥ : {i}/{len(queries)}")
            try:
                result = self.process_query(query, min_documents_per_query)
                results.append(result)
            except Exception as e:
                logging.error(f"ì¿¼ë¦¬ ì²˜ë¦¬ ì‹¤íŒ¨: {query[:50]}... - {e}")
                # ì‹¤íŒ¨í•œ ì¿¼ë¦¬ë„ ê²°ê³¼ì— í¬í•¨
                results.append({
                    'question': query,
                    'keywords': [],
                    'documents': [],
                    'total_documents_found': 0,
                    'error': str(e),
                    'search_timestamp': datetime.now().isoformat()
                })
        
        return results
    
    def save_results_to_json(self, results: List[Dict[str, Any]], output_path: str = None) -> str:
        """
        ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
        
        Args:
            results: ì €ì¥í•  ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            output_path: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ (Noneì´ë©´ ìë™ ìƒì„±)
            
        Returns:
            ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
        """
        if output_path is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = f"search_meta_results_{timestamp}.json"
        
        # ê²°ê³¼ ìš”ì•½ í†µê³„ ì¶”ê°€
        summary = {
            'total_queries': len(results),
            'successful_queries': len([r for r in results if 'error' not in r]),
            'total_documents': sum(r.get('total_documents_found', 0) for r in results),
            'generation_timestamp': datetime.now().isoformat(),
            'results': results
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        logging.info(f"ê²°ê³¼ê°€ {output_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return output_path
    
    def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        self.scienceon_client.close_session()

def test_keyword_extraction():
    """í‚¤ì›Œë“œ ì¶”ì¶œ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    
    # API í‚¤ ì„¤ì • (í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ ì§ì ‘ ì„¤ì •)
    api_key = os.environ.get("GOOGLE_API_KEY")
    if not api_key:
        # configs í´ë”ì—ì„œ API í‚¤ ë¡œë“œ
        try:
            with open('./configs/gemini_api_credentials.json', 'r', encoding='utf-8') as f:
                credentials = json.load(f)
                api_key = credentials.get('api_key')
        except Exception as e:
            print(f"âŒ API í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
            return
    
    if not api_key:
        print("âŒ GOOGLE_API_KEY í™˜ê²½ë³€ìˆ˜ë‚˜ configs/gemini_api_credentials.json íŒŒì¼ì— API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        return
    
    # í‚¤ì›Œë“œ ì¶”ì¶œê¸° ì´ˆê¸°í™”
    extractor = KeywordExtractor(api_key)
    
    # test.csvì—ì„œ ì§ˆë¬¸ ë¡œë“œ
    try:
        df = pd.read_csv('test.csv')
        print(f"ğŸ“„ test.csvì—ì„œ {len(df)}ê°œì˜ ì§ˆë¬¸ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        
        # í…ŒìŠ¤íŠ¸í•  ì§ˆë¬¸ ê°œìˆ˜ ì„¤ì • (ì²˜ìŒ 5ê°œë§Œ í…ŒìŠ¤íŠ¸)
        MAX_QUESTIONS = 5
        test_queries = df['Question'].head(MAX_QUESTIONS).tolist()
        
    except Exception as e:
        print(f"âŒ test.csv íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        print("ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        test_queries = [
            "ì¸ê³µì§€ëŠ¥ì˜ ë¯¸ë˜ëŠ” ì–´ë–»ê²Œ ë ê¹Œìš”?",
            "ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?"
        ]
    
    print("ğŸ” í‚¤ì›Œë“œ ì¶”ì¶œ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 50)
    
    for i, query in enumerate(test_queries, 1):
        print(f"\nğŸ“ í…ŒìŠ¤íŠ¸ {i}: {query[:80]}...")
        
        keywords_dict = extractor.extract_keywords(query)
        korean_keywords = keywords_dict.get('korean', [])
        english_keywords = keywords_dict.get('english', [])
        
        # ê²€ìƒ‰ì–´ ìƒì„± í…ŒìŠ¤íŠ¸
        query_generator = SearchQueryGenerator(api_key)
        search_queries = query_generator.generate_search_queries(query, keywords_dict)
        
        print(f"   ğŸ”‘ í•œêµ­ì–´ í‚¤ì›Œë“œ: {korean_keywords}")
        print(f"   ğŸ”‘ ì˜ì–´ í‚¤ì›Œë“œ: {english_keywords}")
        print(f"   ğŸ” ìƒì„±ëœ ê²€ìƒ‰ì–´ ({len(search_queries)}ê°œ): {search_queries[:3]}...")
        print(f"   ğŸ“Š ì´ í‚¤ì›Œë“œ ê°œìˆ˜: {len(korean_keywords) + len(english_keywords)}ê°œ")
    
    print("\nâœ… í‚¤ì›Œë“œ ì¶”ì¶œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

def test_full_search_pipeline():
    """ì „ì²´ ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸"""
    
    # API í‚¤ ì„¤ì •
    gemini_api_key = os.environ.get("GOOGLE_API_KEY")
    if not gemini_api_key:
        try:
            with open('./configs/gemini_api_credentials.json', 'r', encoding='utf-8') as f:
                credentials = json.load(f)
                gemini_api_key = credentials.get('api_key')
        except Exception as e:
            print(f"âŒ Gemini API í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
            return
    
    if not gemini_api_key:
        print("âŒ GOOGLE_API_KEY í™˜ê²½ë³€ìˆ˜ë‚˜ configs/gemini_api_credentials.json íŒŒì¼ì— API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        return
    
    # ScienceON API ìê²©ì¦ëª… í™•ì¸
    scienceon_credentials_path = './configs/scienceon_api_credentials.json'
    if not os.path.exists(scienceon_credentials_path):
        print(f"âŒ ScienceON API ìê²©ì¦ëª… íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {scienceon_credentials_path}")
        return
    
    try:
        # ê²€ìƒ‰ ë©”íƒ€ë°ì´í„° ìƒì„±ê¸° ì´ˆê¸°í™”
        search_generator = SearchMetaGenerator(gemini_api_key, scienceon_credentials_path)
        
        # test.csvì—ì„œ ì§ˆë¬¸ ë¡œë“œ
        df = pd.read_csv('test.csv')
        print(f"ğŸ“„ test.csvì—ì„œ {len(df)}ê°œì˜ ì§ˆë¬¸ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        
        # í…ŒìŠ¤íŠ¸í•  ì§ˆë¬¸ ê°œìˆ˜ ì„¤ì • (ì²˜ìŒ 3ê°œë§Œ í…ŒìŠ¤íŠ¸)
        MAX_QUESTIONS = 3
        test_queries = df['Question'].head(MAX_QUESTIONS).tolist()
        
        print("ğŸ” ì „ì²´ ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        print("=" * 60)
        
        # ì¿¼ë¦¬ ì²˜ë¦¬ (ìµœì†Œ 50ê°œ ë¬¸ì„œ ë³´ì¥)
        results = search_generator.process_queries(test_queries, min_documents_per_query=50)
        
        # ê²°ê³¼ ì €ì¥
        output_file = search_generator.save_results_to_json(results)
        
        # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        print("\nğŸ“Š ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½:")
        print(f"   ì´ ì²˜ë¦¬ëœ ì§ˆë¬¸: {len(results)}ê°œ")
        print(f"   ì„±ê³µí•œ ì§ˆë¬¸: {len([r for r in results if 'error' not in r])}ê°œ")
        print(f"   ì´ ì°¾ì€ ë¬¸ì„œ: {sum(r.get('total_documents_found', 0) for r in results)}ê°œ")
        print(f"   ê²°ê³¼ íŒŒì¼: {output_file}")
        
        # ì²« ë²ˆì§¸ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°
        if results:
            first_result = results[0]
            print(f"\nğŸ“ ì²« ë²ˆì§¸ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°:")
            print(f"   ì§ˆë¬¸: {first_result['question'][:80]}...")
            print(f"   í•œêµ­ì–´ í‚¤ì›Œë“œ: {first_result['keywords']['korean']}")
            print(f"   ì˜ì–´ í‚¤ì›Œë“œ: {first_result['keywords']['english']}")
            print(f"   ìƒì„±ëœ ê²€ìƒ‰ì–´: {first_result.get('search_queries', [])[:3]}...")
            print(f"   ì°¾ì€ ë¬¸ì„œ ìˆ˜: {first_result['total_documents_found']}ê°œ")
            if first_result['documents']:
                print(f"   ì²« ë²ˆì§¸ ë¬¸ì„œ: {first_result['documents'][0]['title'][:60]}...")
        
        print("\nâœ… ì „ì²´ ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        
        # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
        search_generator.close()
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        logging.error(f"í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "full":
        test_full_search_pipeline()
    else:
        test_keyword_extraction()
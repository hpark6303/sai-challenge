#!/usr/bin/env python3
"""
ê²€ìƒ‰ ë©”íƒ€ë°ì´í„° ìƒì„± ì‹œìŠ¤í…œ
- LLMì„ ì‚¬ìš©í•œ í‚¤ì›Œë“œ ì¶”ì¶œ
- ScienceON APIë¥¼ í†µí•œ ë¬¸ì„œ ê²€ìƒ‰
- êµ¬ì¡°í™”ëœ JSON íŒŒì¼ ìƒì„±
"""

import os
import json
import logging
import pandas as pd
from typing import List, Dict, Any, Optional
from pathlib import Path
from datetime import datetime
import google.generativeai as genai

# ê¸°ì¡´ ScienceON API í´ë¼ì´ì–¸íŠ¸ import
from scienceon_api_example import ScienceONAPIClient

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

class KeywordExtractor:
    """LLMì„ ì‚¬ìš©í•œ í‚¤ì›Œë“œ ì¶”ì¶œê¸°"""
    
    def __init__(self, api_key: str, model_name: str = "gemini-2.5-flash"):
        """
        í‚¤ì›Œë“œ ì¶”ì¶œê¸° ì´ˆê¸°í™”
        
        Args:
            api_key: Google API í‚¤
            model_name: ì‚¬ìš©í•  Gemini ëª¨ë¸ëª…
        """
        self.api_key = api_key
        self.model_name = model_name
        self.model = self._init_gemini()
        
    def _init_gemini(self) -> genai.GenerativeModel:
        """Gemini ëª¨ë¸ ì´ˆê¸°í™”"""
        genai.configure(api_key=self.api_key)
        generation_config = genai.GenerationConfig(
            temperature=0.2,
            candidate_count=1,
        )
        
        return genai.GenerativeModel(
            model_name=self.model_name,
            generation_config=generation_config,
        )
    
    def _create_keyword_prompt(self, query: str) -> str:
        """í‚¤ì›Œë“œ ì¶”ì¶œì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        prompt = f"""ë‹¹ì‹ ì€ ë…¼ë¬¸ ê²€ìƒ‰ì„ ìœ„í•œ í‚¤ì›Œë“œ ì¶”ì¶œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì§ˆë¬¸ì—ì„œ ScienceON API ê²€ìƒ‰ì— ìµœì í™”ëœ í•µì‹¬ í‚¤ì›Œë“œë“¤ì„ í•œêµ­ì–´ì™€ ì˜ì–´ë¡œ ê°ê° ì¶”ì¶œí•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: "{query}"

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì„¸ìš”:

1. í•œêµ­ì–´ í‚¤ì›Œë“œ: 3-5ê°œì˜ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì‰¼í‘œë¡œ êµ¬ë¶„ (ì „ìêµê³¼ì„œ)
2. ì˜ì–´ í‚¤ì›Œë“œ: ìœ„ í•œêµ­ì–´ í‚¤ì›Œë“œë“¤ì˜ ì˜ì–´ ë²ˆì—­ì„ ì‰¼í‘œë¡œ êµ¬ë¶„

ê·œì¹™:
- ì „ë¬¸ìš©ì–´ì™€ ê¸°ìˆ ìš©ì–´ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì„ íƒ
- ì¶•ì•½ì–´, ì „ì²´ìš©ì–´ë¥¼ ëª¨ë‘ ì•Œ ê²½ìš°, ëª¨ë‘ ì‚¬ìš© í‚¤ì›Œë“œë¡œ ë§Œë“œì„¸ìš”. ì „ë¬¸ìš©ì–´ê°€ ì „ì²´ìš©ì–´ë¡œ ì§ˆë¬¸ì— ë“¤ì–´ì˜¨ ê²½ìš° í™•ì‹¤í•˜ê²Œ í‚¤ì›Œë“œë¡œ ë§Œë“œì„¸ìš”. (ì˜ˆ: SVM, DTG, NLP, artificial intelligence, Warehouse Management System)
- ê° í‚¤ì›Œë“œëŠ” 1-20ì ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ


ì¶œë ¥ í˜•ì‹:
í•œêµ­ì–´: í‚¤ì›Œë“œ1, í‚¤ì›Œë“œ2, í‚¤ì›Œë“œ3, í‚¤ì›Œë“œ4
ì˜ì–´: keyword1, keyword2, keyword3, keyword4



í‚¤ì›Œë“œ:"""
        return prompt
    
    def extract_keywords(self, query: str) -> Dict[str, List[str]]:
        """
        ì§ˆë¬¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ (í•œêµ­ì–´, ì˜ì–´ ê°ê°)
        
        Args:
            query: ì¶”ì¶œí•  ì§ˆë¬¸
            
        Returns:
            {'korean': [í‚¤ì›Œë“œë“¤], 'english': [í‚¤ì›Œë“œë“¤]} í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬
        """
        try:
            prompt = self._create_keyword_prompt(query)
            response = self.model.generate_content(prompt)
            
            # ì‘ë‹µì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
            response_text = response.text.strip()
            
            # "í‚¤ì›Œë“œ:" í…ìŠ¤íŠ¸ ì œê±°
            if response_text.startswith("í‚¤ì›Œë“œ:"):
                response_text = response_text[4:].strip()
            
            # í•œêµ­ì–´ì™€ ì˜ì–´ í‚¤ì›Œë“œ ë¶„ë¦¬
            korean_keywords = []
            english_keywords = []
            
            lines = response_text.split('\n')
            for line in lines:
                line = line.strip()
                if line.startswith('í•œêµ­ì–´:'):
                    korean_text = line.replace('í•œêµ­ì–´:', '').strip()
                    korean_keywords = [kw.strip() for kw in korean_text.split(',') if kw.strip()]
                elif line.startswith('ì˜ì–´:'):
                    english_text = line.replace('ì˜ì–´:', '').strip()
                    english_keywords = [kw.strip() for kw in english_text.split(',') if kw.strip()]
            
            # ë¹ˆ í‚¤ì›Œë“œ ì œê±° ë° ê¸¸ì´ ì œí•œ
            korean_keywords = [kw for kw in korean_keywords if kw and 1 < len(kw) <= 30]
            english_keywords = [kw for kw in english_keywords if kw and 1 < len(kw) <= 30]

            result = {
                'korean': korean_keywords,
                'english': english_keywords
            }
            
            logging.info(f"ì§ˆë¬¸: {query}")
            logging.info(f"í•œêµ­ì–´ í‚¤ì›Œë“œ: {korean_keywords}")
            logging.info(f"ì˜ì–´ í‚¤ì›Œë“œ: {english_keywords}")
            
            return result
            
        except Exception as e:
            logging.error(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {'korean': [], 'english': []}


class SearchQueryGenerator:
    """Geminië¥¼ í™œìš©í•œ ì§€ëŠ¥ì  ê²€ìƒ‰ì–´ ìƒì„±ê¸°"""
    
    def __init__(self, api_key: str, model_name: str = "gemini-2.5-flash"):
        """
        ê²€ìƒ‰ì–´ ìƒì„±ê¸° ì´ˆê¸°í™”
        
        Args:
            api_key: Google Gemini API í‚¤
            model_name: ì‚¬ìš©í•  Gemini ëª¨ë¸ëª…
        """
        self.model = self._init_gemini(api_key, model_name)
    
    def _init_gemini(self, api_key: str, model_name: str):
        """Gemini ëª¨ë¸ ì´ˆê¸°í™”"""
        import google.generativeai as genai
        genai.configure(api_key=api_key)
        return genai.GenerativeModel(model_name)
    
    def _create_search_query_prompt(self, query: str, keywords_dict: Dict[str, List[str]]) -> str:
        """ê²€ìƒ‰ì–´ ìƒì„±ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        korean_keywords = keywords_dict.get('korean', [])
        english_keywords = keywords_dict.get('english', [])
        
        prompt = f"""ë‹¹ì‹ ì€ ScienceON API ê²€ìƒ‰ì„ ìœ„í•œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì§ˆë¬¸ê³¼ í‚¤ì›Œë“œë“¤ì„ ë°”íƒ•ìœ¼ë¡œ íš¨ê³¼ì ì¸ ê²€ìƒ‰ì–´ë“¤ì„ ìƒì„±í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: "{query}"

í•œêµ­ì–´ í‚¤ì›Œë“œ: {', '.join(korean_keywords)}
ì˜ì–´ í‚¤ì›Œë“œ: {', '.join(english_keywords)}

ë‹¤ìŒ ê²€ìƒ‰ ì—°ì‚°ìë“¤ì„ í™œìš©í•˜ì—¬ ê²€ìƒ‰ì–´ë¥¼ ìƒì„±í•˜ì„¸ìš”:

 | ì—°ì‚°ì: ë‘ ê°œ ì´ìƒì˜ ê²€ìƒ‰ì–´ ì¤‘ 1ê°œ ì´ìƒì„ í¬í•¨í•˜ëŠ” ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
   ì˜ˆ: "ë‚˜ë…¸|ê¸°ê³„"

( ) ì—°ì‚°ì: ê´„í˜¸ ì•ˆì˜ ê²€ìƒ‰ì–´ê°€ ìš°ì„ ìˆœìœ„ë¡œ ì§€ì •ë©ë‹ˆë‹¤.
   ì˜ˆ: "ë‚˜ë…¸ (ê¸°ê³„ | machine)"

ì¤‘ìš”í•œ ê·œì¹™:
- ì§ˆë¬¸ì˜ í•µì‹¬ ì˜ë„ì™€ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ê²€ìƒ‰ì–´ë¥¼ ë¨¼ì € ìƒì„±í•˜ì„¸ìš”
- ë‹¨ìˆœí•˜ê³  ì‹¤ìš©ì ì¸ ê²€ìƒ‰ì–´ë¥¼ ìš°ì„ í•˜ì„¸ìš”
- ë³µì¡í•œ ë”°ì˜´í‘œë‚˜ ì •í™•í•œ êµ¬ë¬¸ ê²€ìƒ‰ì€ í”¼í•˜ì„¸ìš”
- ê° ê²€ìƒ‰ì–´ëŠ” í•œ ì¤„ì— í•˜ë‚˜ì”© ì‘ì„±í•˜ì„¸ìš”
- ë§Œë“¤ì–´ë‚¸ ê²€ìƒ‰ì–´ ë‚´ì—ì„œ ê° í‚¤ì›Œë“œë¥¼ ëª¨ë‘ | ì—°ì‚°ì°¨ë¡œ ì—°ê²°í•´ì„œ ê²€ìƒ‰ì–´ë¥¼ ìƒì„±í•˜ì„¸ìš” ì˜ˆ: "AI|mathematics|machine|learning|textbook" ê¸¸ì´ëŠ” ë„ˆë¬´ ê¸¸ì§€ ì•Šê²Œ.
- ìµœëŒ€ 12ê°œì˜ ê²€ìƒ‰ì–´ë¥¼ ìƒì„±í•˜ì„¸ìš”
- ì„¤ëª…ì´ë‚˜ ë²ˆí˜¸ ì—†ì´ ê²€ìƒ‰ì–´ë§Œ ë‚˜ì—´í•˜ì„¸ìš”

ê²€ìƒ‰ì–´ ëª©ë¡:"""
        return prompt
    
    def generate_search_queries(self, query: str, keywords_dict: Dict[str, List[str]]) -> List[str]:
        """
        Geminië¥¼ í™œìš©í•˜ì—¬ ì§€ëŠ¥ì ì¸ ê²€ìƒ‰ì–´ ìƒì„±
        
        Args:
            query: ì›ë³¸ ì§ˆë¬¸
            keywords_dict: {'korean': [...], 'english': [...]} í˜•íƒœì˜ í‚¤ì›Œë“œ ë”•ì…”ë„ˆë¦¬
            
        Returns:
            ê²€ìƒ‰ì–´ ë¦¬ìŠ¤íŠ¸
        """
        try:
            prompt = self._create_search_query_prompt(query, keywords_dict)
            response = self.model.generate_content(prompt)
            response_text = response.text.strip()
            
            # ê²€ìƒ‰ì–´ íŒŒì‹±
            search_queries = []
            lines = response_text.split('\n')
            
            for line in lines:
                line = line.strip()
                
                # ì„¤ëª…ì ì¸ í…ìŠ¤íŠ¸ë‚˜ ë¹ˆ ì¤„ ì œì™¸
                if (not line or 
                    line.startswith('ê²€ìƒ‰ì–´:') or 
                    line.startswith('ì˜ˆ:') or
                    line.startswith('ë‹¤ìŒì€') or
                    line.startswith('ScienceON') or
                    line.startswith('ê·œì¹™:') or
                    line.startswith('ì§ˆë¬¸:') or
                    line.startswith('í•œêµ­ì–´ í‚¤ì›Œë“œ:') or
                    line.startswith('ì˜ì–´ í‚¤ì›Œë“œ:') or
                    len(line) < 3):
                    continue
                
                # ë²ˆí˜¸ë‚˜ ê¸°í˜¸ ì œê±°
                clean_line = line
                
                # ë²ˆí˜¸ íŒ¨í„´ ì œê±° (1., 2., 10., 11. ë“±)
                import re
                clean_line = re.sub(r'^\d+\.\s*', '', clean_line)
                
                # ê¸°í˜¸ ì œê±°
                if clean_line.startswith(('-', 'â€¢', '.', '`')):
                    clean_line = clean_line[1:].strip()
                
                # ë°±í‹± ì œê±°
                clean_line = clean_line.replace('`', '').strip()
                
                # ë”°ì˜´í‘œ ì •ë¦¬ - ì›ë³¸ ìœ ì§€
                clean_line = clean_line.replace('"', '"').replace('"', '"')
                
                if clean_line and len(clean_line) <= 100 and not clean_line.startswith('ë‹¤ìŒì€'):
                    search_queries.append(clean_line)
            
            # ë„ì–´ì“°ê¸°ë¥¼ '|'ë¡œ ë³€í™˜
            processed_queries = []
            for query in search_queries:
                if ' ' in query:
                    processed_query = query.replace(' ', '|')
                else:
                    processed_query = query
                
                # ì—°ì†ëœ '|'ë¥¼ í•˜ë‚˜ë¡œ ì¤„ì´ê¸°
                import re
                processed_query = re.sub(r'\|+', '|', processed_query)
                
                # ì•ë’¤ì˜ '|' ì œê±°
                processed_query = processed_query.strip('|')
                
                processed_queries.append(processed_query)
            
            # ì¤‘ë³µ ì œê±°
            unique_queries = []
            seen = set()
            for query in processed_queries:
                if query not in seen:
                    unique_queries.append(query)
                    seen.add(query)
            
            logging.info(f"Geminiê°€ ìƒì„±í•œ ê²€ìƒ‰ì–´: {unique_queries}")
            return unique_queries[:8]  # ìµœëŒ€ 8ê°œë¡œ ì œí•œ
            
        except Exception as e:
            logging.error(f"ê²€ìƒ‰ì–´ ìƒì„± ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ í‚¤ì›Œë“œë“¤ ë°˜í™˜
            korean_keywords = keywords_dict.get('korean', [])
            english_keywords = keywords_dict.get('english', [])
            return korean_keywords + english_keywords


class SearchMetaGenerator:
    """ê²€ìƒ‰ ë©”íƒ€ë°ì´í„° ìƒì„±ê¸° - í‚¤ì›Œë“œ ì¶”ì¶œ + ë¬¸ì„œ ê²€ìƒ‰"""
    
    def __init__(self, gemini_api_key: str, scienceon_credentials_path: str = "./configs/scienceon_api_credentials.json"):
        """
        ê²€ìƒ‰ ë©”íƒ€ë°ì´í„° ìƒì„±ê¸° ì´ˆê¸°í™”
        
        Args:
            gemini_api_key: Gemini API í‚¤
            scienceon_credentials_path: ScienceON API ìê²©ì¦ëª… íŒŒì¼ ê²½ë¡œ
        """
        self.keyword_extractor = KeywordExtractor(gemini_api_key)
        self.scienceon_client = ScienceONAPIClient(Path(scienceon_credentials_path))
        self.query_generator = SearchQueryGenerator(gemini_api_key)
    
    def _prepare_search_query_for_api(self, search_query: str) -> str:
        """
        ê²€ìƒ‰ì–´ë¥¼ ScienceON APIì— ì „ë‹¬í•˜ê¸° ìœ„í•´ ì¤€ë¹„
        
        Args:
            search_query: ì›ë³¸ ê²€ìƒ‰ì–´
            
        Returns:
            APIìš© ê²€ìƒ‰ì–´
        """
        # JSONì—ì„œ ì´ìŠ¤ì¼€ì´í”„ëœ ë”°ì˜´í‘œë¥¼ ì›ë˜ ë”°ì˜´í‘œë¡œ ë³µì›
        api_query = search_query.replace('\\"', '"')
        
        # ë°±ìŠ¬ë˜ì‹œê°€ í¬í•¨ëœ ê²½ìš° ì œê±°
        api_query = api_query.replace('\\', '')
        
        return api_query
    
    def process_query(self, query: str, min_documents: int = 50) -> Dict[str, Any]:
        """
        ë‹¨ì¼ ì¿¼ë¦¬ ì²˜ë¦¬: í‚¤ì›Œë“œ ì¶”ì¶œ + ë¬¸ì„œ ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰í•  ì§ˆë¬¸
            min_documents: ìµœì†Œ ë³´ì¥ ë¬¸ì„œ ìˆ˜ (ê¸°ë³¸ 50ê°œ)
            
        Returns:
            ì²˜ë¦¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        logging.info(f"ì§ˆë¬¸ ì²˜ë¦¬ ì‹œì‘: {query[:50]}...")
        
        # 1. í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords_dict = self.keyword_extractor.extract_keywords(query)
        korean_keywords = keywords_dict.get('korean', [])
        english_keywords = keywords_dict.get('english', [])
        
        # 2. ê²€ìƒ‰ì–´ ìƒì„± (Gemini í™œìš©)
        search_queries = self.query_generator.generate_search_queries(query, keywords_dict)
        logging.info(f"ìƒì„±ëœ ê²€ìƒ‰ì–´ {len(search_queries)}ê°œ: {search_queries[:5]}...")  # ì²˜ìŒ 5ê°œë§Œ ë¡œê·¸
        
        # 3. ì§ˆë¬¸ ì–¸ì–´ ê°ì§€ ë° ìš°ì„  ê²€ìƒ‰ì–´ ì„ íƒ
        is_english_query = any(char.isascii() and char.isalpha() for char in query[:50])
        
        if is_english_query:
            logging.info(f"ì˜ì–´ ì§ˆë¬¸ ê°ì§€: ì˜ì–´ ìš°ì„  ê²€ìƒ‰ì–´ ì‚¬ìš©")
        else:
            logging.info(f"í•œêµ­ì–´ ì§ˆë¬¸ ê°ì§€: í•œêµ­ì–´ ìš°ì„  ê²€ìƒ‰ì–´ ì‚¬ìš©")
        
        # 4. ê²€ìƒ‰ì–´ë¡œ ë¬¸ì„œ ê²€ìƒ‰ (50ê°œ ì´ìƒ í™•ë³´í•  ë•Œê¹Œì§€ ë°˜ë³µ)
        all_documents = []
        page = 1
        max_pages = 3  # ìµœëŒ€ 3í˜ì´ì§€ê¹Œì§€ ê²€ìƒ‰
        
        while page <= max_pages:
            logging.info(f"í˜ì´ì§€ {page} ê²€ìƒ‰ ì¤‘... (í˜„ì¬ {len(all_documents)}ê°œ ë¬¸ì„œ)")
            
            # ëª¨ë“  ê²€ìƒ‰ì–´ë¡œ ê²€ìƒ‰
            for search_query in search_queries:
                # ê²€ìƒ‰ì–´ë¥¼ APIì— ì „ë‹¬í•  ë•Œ ë”°ì˜´í‘œ ì²˜ë¦¬
                api_query = self._prepare_search_query_for_api(search_query)
                docs = self.scienceon_client.search_articles(api_query, cur_page=page, row_count=20)
                
                # ì¦‰ì‹œ í’ˆì§ˆ í•„í„°ë§ ì ìš© (abstract 15ì ì´í•˜ ì œì™¸)
                filtered_docs = []
                for doc in docs:
                    abstract = doc.get('abstract', '')
                    if len(abstract.strip()) > 15:  # 15ì ì´í•˜ëŠ” ì œì™¸
                        filtered_docs.append(doc)
                
                all_documents.extend(filtered_docs)
                
                # í’ˆì§ˆ í•„í„°ë§ ë¡œê·¸
                filtered_count = len(docs) - len(filtered_docs)
                if filtered_count > 0:
                    logging.info(f"ê²€ìƒ‰ì–´ '{search_query}' â†’ {len(docs)}ê°œ ë¬¸ì„œ (í’ˆì§ˆ í•„í„°ë§ìœ¼ë¡œ {filtered_count}ê°œ ì œì™¸)")
                else:
                    logging.info(f"ê²€ìƒ‰ì–´ '{search_query}' â†’ {len(docs)}ê°œ ë¬¸ì„œ")
                
                # ì¤‘ë³µ ì œê±° í›„ ê°œìˆ˜ í™•ì¸
                unique_docs_temp = []
                seen_titles_temp = set()
                for doc in all_documents:
                    title = doc.get('title', '')
                    if title and title not in seen_titles_temp:
                        unique_docs_temp.append(doc)
                        seen_titles_temp.add(title)
                
                if len(unique_docs_temp) >= min_documents:
                    logging.info(f"ëª©í‘œ ë¬¸ì„œ ìˆ˜ {min_documents}ê°œ ë‹¬ì„±: {len(unique_docs_temp)}ê°œ (í’ˆì§ˆ í•„í„°ë§ ì ìš©)")
                    break
            
            # ì¤‘ë³µ ì œê±° í›„ ê°œìˆ˜ í™•ì¸
            unique_docs_temp = []
            seen_titles_temp = set()
            for doc in all_documents:
                title = doc.get('title', '')
                if title and title not in seen_titles_temp:
                    unique_docs_temp.append(doc)
                    seen_titles_temp.add(title)
            
            if len(unique_docs_temp) >= min_documents:
                break
                
            page += 1
        
        # 5. ì¤‘ë³µ ì œê±° (ì œëª© ê¸°ì¤€) - í’ˆì§ˆ í•„í„°ë§ì€ ì´ë¯¸ ê²€ìƒ‰ ì¤‘ì— ì ìš©ë¨
        unique_docs = []
        seen_titles = set()
        
        for doc in all_documents:
            title = doc.get('title', '')
            
            # ì œëª©ì´ ìˆê³  ì¤‘ë³µì´ ì•„ë‹Œ ê²½ìš°
            if title and title not in seen_titles:
                # source í•„ë“œ ì œê±° (ScienceONì´ ìëª…í•˜ë¯€ë¡œ)
                if 'source' in doc:
                    del doc['source']
                unique_docs.append(doc)
                seen_titles.add(title)
        
        # 6. ê²°ê³¼ ì •ë¦¬
        result = {
            'question': query,
            'keywords': {
                'korean': korean_keywords,
                'english': english_keywords
            },
            'search_queries': search_queries,
            'documents': unique_docs,  # ëª¨ë“  ë¬¸ì„œ í¬í•¨
            'total_documents_found': len(unique_docs),
            'search_timestamp': datetime.now().isoformat()
        }
        
        total_keywords = len(korean_keywords) + len(english_keywords)
        logging.info(f"ì§ˆë¬¸ ì²˜ë¦¬ ì™„ë£Œ: {total_keywords}ê°œ í‚¤ì›Œë“œ â†’ {len(search_queries)}ê°œ ê²€ìƒ‰ì–´ â†’ {len(unique_docs)}ê°œ ë¬¸ì„œ")
        return result
    
    def process_queries(self, queries: List[str], min_documents_per_query: int = 50) -> List[Dict[str, Any]]:
        """
        ì—¬ëŸ¬ ì¿¼ë¦¬ ì¼ê´„ ì²˜ë¦¬
        
        Args:
            queries: ì²˜ë¦¬í•  ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸
            min_documents_per_query: ì¿¼ë¦¬ë‹¹ ìµœì†Œ ë³´ì¥ ë¬¸ì„œ ìˆ˜ (ê¸°ë³¸ 50ê°œ)
            
        Returns:
            ì²˜ë¦¬ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        results = []
        
        for i, query in enumerate(queries, 1):
            logging.info(f"ì§„í–‰ë¥ : {i}/{len(queries)}")
            try:
                result = self.process_query(query, min_documents_per_query)
                results.append(result)
            except Exception as e:
                logging.error(f"ì¿¼ë¦¬ ì²˜ë¦¬ ì‹¤íŒ¨: {query[:50]}... - {e}")
                # ì‹¤íŒ¨í•œ ì¿¼ë¦¬ë„ ê²°ê³¼ì— í¬í•¨
                results.append({
                    'question': query,
                    'keywords': [],
                    'documents': [],
                    'total_documents_found': 0,
                    'error': str(e),
                    'search_timestamp': datetime.now().isoformat()
                })
        
        return results
    
    def save_results_to_json(self, results: List[Dict[str, Any]], output_path: str = None) -> str:
        """
        ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
        
        Args:
            results: ì €ì¥í•  ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            output_path: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ (Noneì´ë©´ ìë™ ìƒì„±)
            
        Returns:
            ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
        """
        if output_path is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = f"search_meta_results_{timestamp}.json"
        
        # ê²°ê³¼ ìš”ì•½ í†µê³„ ì¶”ê°€
        summary = {
            'total_queries': len(results),
            'successful_queries': len([r for r in results if 'error' not in r]),
            'total_documents': sum(r.get('total_documents_found', 0) for r in results),
            'generation_timestamp': datetime.now().isoformat(),
            'results': results
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        logging.info(f"ê²°ê³¼ê°€ {output_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return output_path
    
    def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        self.scienceon_client.close_session()


def test_full_search_pipeline():
    """ì „ì²´ ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸"""
    
    # API í‚¤ ì„¤ì •
    gemini_api_key = os.environ.get("GOOGLE_API_KEY")
    if not gemini_api_key:
        try:
            with open('./configs/gemini_api_credentials.json', 'r', encoding='utf-8') as f:
                credentials = json.load(f)
                gemini_api_key = credentials.get('api_key')
        except Exception as e:
            print(f"âŒ Gemini API í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
            return
    
    if not gemini_api_key:
        print("âŒ GOOGLE_API_KEY í™˜ê²½ë³€ìˆ˜ë‚˜ configs/gemini_api_credentials.json íŒŒì¼ì— API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        return
    
    # ScienceON API ìê²©ì¦ëª… í™•ì¸
    scienceon_credentials_path = './configs/scienceon_api_credentials.json'
    if not os.path.exists(scienceon_credentials_path):
        print(f"âŒ ScienceON API ìê²©ì¦ëª… íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {scienceon_credentials_path}")
        return
    
    try:
        # ê²€ìƒ‰ ë©”íƒ€ë°ì´í„° ìƒì„±ê¸° ì´ˆê¸°í™”
        search_generator = SearchMetaGenerator(gemini_api_key, scienceon_credentials_path)
        
        # test.csvì—ì„œ ì§ˆë¬¸ ë¡œë“œ
        df = pd.read_csv('test.csv')
        print(f"ğŸ“„ test.csvì—ì„œ {len(df)}ê°œì˜ ì§ˆë¬¸ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        
        # í…ŒìŠ¤íŠ¸í•  ì§ˆë¬¸ ê°œìˆ˜ ì„¤ì • (ì²˜ìŒ 3ê°œë§Œ í…ŒìŠ¤íŠ¸)
        MAX_QUESTIONS = 3
        test_queries = df['Question'].head(MAX_QUESTIONS).tolist()
        # ì „ì²´ ì§ˆë¬¸ ì²˜ë¦¬
        #test_queries = df['Question'].tolist()



        print("ğŸ” ì „ì²´ ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        print("=" * 60)
        
        # ì¿¼ë¦¬ ì²˜ë¦¬ (ìµœì†Œ 50ê°œ ë¬¸ì„œ ë³´ì¥)
        results = search_generator.process_queries(test_queries, min_documents_per_query=50)
        
        # ê²°ê³¼ ì €ì¥
        output_file = search_generator.save_results_to_json(results)
        
        # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        print("\nğŸ“Š ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½:")
        print(f"   ì´ ì²˜ë¦¬ëœ ì§ˆë¬¸: {len(results)}ê°œ")
        print(f"   ì„±ê³µí•œ ì§ˆë¬¸: {len([r for r in results if 'error' not in r])}ê°œ")
        print(f"   ì´ ì°¾ì€ ë¬¸ì„œ: {sum(r.get('total_documents_found', 0) for r in results)}ê°œ")
        print(f"   ê²°ê³¼ íŒŒì¼: {output_file}")
        
        # ì²« ë²ˆì§¸ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°
        if results:
            first_result = results[0]
            print(f"\nğŸ“ ì²« ë²ˆì§¸ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°:")
            print(f"   ì§ˆë¬¸: {first_result['question'][:80]}...")
            print(f"   í•œêµ­ì–´ í‚¤ì›Œë“œ: {first_result['keywords']['korean']}")
            print(f"   ì˜ì–´ í‚¤ì›Œë“œ: {first_result['keywords']['english']}")
            print(f"   ìƒì„±ëœ ê²€ìƒ‰ì–´: {first_result.get('search_queries', [])[:3]}...")
            print(f"   ì°¾ì€ ë¬¸ì„œ ìˆ˜: {first_result['total_documents_found']}ê°œ")
            if first_result['documents']:
                print(f"   ì²« ë²ˆì§¸ ë¬¸ì„œ: {first_result['documents'][0]['title'][:60]}...")
        
        print("\nâœ… ì „ì²´ ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        
        # ìë™ìœ¼ë¡œ CSVì™€ JSONL ë³€í™˜ ì‹¤í–‰
        if results:
            print(f"\nğŸ”„ ìë™ìœ¼ë¡œ CSV ë³€í™˜ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
            convert_to_csv_format(output_file)
            print(f"\nğŸ”„ ìë™ìœ¼ë¡œ JSONL ë³€í™˜ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
            convert_to_jsonl_format(output_file)
        
        # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
        search_generator.close()
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        logging.error(f"í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")

def convert_to_csv_format(json_file_path, csv_output_path=None):
    """JSON ê²°ê³¼ë¥¼ CSV í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (Title, Abstract, Source í¬í•¨)"""
    print("ğŸ”„ JSON ê²°ê³¼ë¥¼ CSV í˜•ì‹ìœ¼ë¡œ ë³€í™˜ ì¤‘...")
    
    try:
        with open(json_file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except Exception as e:
        print(f"âŒ JSON íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return
    
    if csv_output_path is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        csv_output_path = f"search_results_{timestamp}.csv"
    
    csv_data = []
    for result in data['results']:
        row_data = {'Question': result['question']}
        
        for i in range(1, 51):
            if i <= len(result['documents']):
                doc = result['documents'][i-1]
                title = doc.get('title', 'ì œëª© ì—†ìŒ')
                abstract = doc.get('abstract', 'ì´ˆë¡ ì—†ìŒ')
                cn = doc.get('CN', '')
                source_url = f"http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue={cn}&target=NART&cn={cn}" if cn else "Source ì •ë³´ ì—†ìŒ"
                article_info = f"Title: {title}, Abstract: {abstract}, Source: {source_url}"
                row_data[f'Prediction_retrieved_article_name_{i}'] = article_info
            else:
                row_data[f'Prediction_retrieved_article_name_{i}'] = ''
        
        csv_data.append(row_data)
    
    try:
        df = pd.DataFrame(csv_data)
        df.to_csv(csv_output_path, index=False, encoding='utf-8')
        print(f"âœ… CSV íŒŒì¼ ìƒì„± ì™„ë£Œ: {csv_output_path}")
        print(f"   ì´ {len(csv_data)}ê°œ ì§ˆë¬¸, ê°ê° ìµœëŒ€ 50ê°œ ë…¼ë¬¸ ì •ë³´ í¬í•¨")
        
        if csv_data:
            print(f"\nğŸ“ CSV ë¯¸ë¦¬ë³´ê¸° (ì²« ë²ˆì§¸ í–‰):")
            print(f"   Question: {csv_data[0]['Question'][:50]}...")
            print(f"   Prediction_retrieved_article_name_1: {csv_data[0]['Prediction_retrieved_article_name_1'][:100]}...")
            print(f"   Prediction_retrieved_article_name_2: {csv_data[0]['Prediction_retrieved_article_name_2'][:100]}...")
        
    except Exception as e:
        print(f"âŒ CSV íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")


def convert_to_jsonl_format(json_file_path, jsonl_output_path=None):
    """JSON ê²°ê³¼ë¥¼ JSONL í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ì¤‘ë³µ ì—†ëŠ” ë…¼ë¬¸ ëª©ë¡)"""
    print("ğŸ”„ JSON ê²°ê³¼ë¥¼ JSONL í˜•ì‹ìœ¼ë¡œ ë³€í™˜ ì¤‘...")
    
    try:
        with open(json_file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except Exception as e:
        print(f"âŒ JSON íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return
    
    if jsonl_output_path is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        jsonl_output_path = f"search_documents_{timestamp}.jsonl"
    
    # ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•œ set (CN ê¸°ì¤€)
    seen_cns = set()
    unique_documents = []
    
    for result in data['results']:
        for doc in result['documents']:
            cn = doc.get('CN', '')
            if cn and cn not in seen_cns:
                seen_cns.add(cn)
                # JSONL í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                jsonl_doc = {
                    "CN": cn,
                    "title": doc.get('title', ''),
                    "abstract": doc.get('abstract', ''),
                    "source": f"http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue={cn}&target=NART&cn={cn}"
                }
                unique_documents.append(jsonl_doc)
    
    # JSONL íŒŒì¼ë¡œ ì €ì¥
    try:
        with open(jsonl_output_path, 'w', encoding='utf-8') as f:
            for doc in unique_documents:
                f.write(json.dumps(doc, ensure_ascii=False) + '\n')
        
        print(f"âœ… JSONL íŒŒì¼ ìƒì„± ì™„ë£Œ: {jsonl_output_path}")
        print(f"   ì´ {len(unique_documents)}ê°œ ì¤‘ë³µ ì—†ëŠ” ë…¼ë¬¸")
        
        if unique_documents:
            print(f"\nğŸ“ JSONL ë¯¸ë¦¬ë³´ê¸° (ì²« ë²ˆì§¸ ë…¼ë¬¸):")
            first_doc = unique_documents[0]
            print(f"   CN: {first_doc['CN']}")
            print(f"   Title: {first_doc['title'][:60]}...")
            print(f"   Abstract: {first_doc['abstract'][:100]}...")
        
    except Exception as e:
        print(f"âŒ JSONL íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")


def test_csv_conversion():
    """CSV ë³€í™˜ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª CSV ë³€í™˜ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 40)
    
    json_files = [f for f in os.listdir('.') if f.startswith('search_meta_results_') and f.endswith('.json')]
    if not json_files:
        print("âŒ ë³€í™˜í•  JSON íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ê²€ìƒ‰ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        return
    
    latest_json = max(json_files, key=os.path.getctime)
    print(f"ğŸ“„ ë³€í™˜í•  JSON íŒŒì¼: {latest_json}")
    
    convert_to_csv_format(latest_json)
    print(f"\nâœ… CSV ë³€í™˜ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


def test_jsonl_conversion():
    """JSONL ë³€í™˜ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª JSONL ë³€í™˜ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 40)
    
    json_files = [f for f in os.listdir('.') if f.startswith('search_meta_results_') and f.endswith('.json')]
    if not json_files:
        print("âŒ ë³€í™˜í•  JSON íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ê²€ìƒ‰ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        return
    
    latest_json = max(json_files, key=os.path.getctime)
    print(f"ğŸ“„ ë³€í™˜í•  JSON íŒŒì¼: {latest_json}")
    
    convert_to_jsonl_format(latest_json)
    print(f"\nâœ… JSONL ë³€í™˜ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        if sys.argv[1] == "full":
            test_full_search_pipeline()
        elif sys.argv[1] == "csv":
            test_csv_conversion()
        elif sys.argv[1] == "jsonl":
            test_jsonl_conversion()
        else:
            print("ì‚¬ìš©ë²•: python search_meta.py [full|csv|jsonl]")
            print("  full:   ì „ì²´ ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ (ìë™ìœ¼ë¡œ CSV, JSONLë„ ìƒì„±)")
            print("  csv:    JSON ê²°ê³¼ë¥¼ CSVë¡œ ë³€í™˜")
            print("  jsonl:  JSON ê²°ê³¼ë¥¼ JSONLë¡œ ë³€í™˜ (ì¤‘ë³µ ì—†ëŠ” ë…¼ë¬¸ ëª©ë¡)")
    else:
        test_keyword_extraction()
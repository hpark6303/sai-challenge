#!/usr/bin/env python3
"""
CRAG íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ (ì‹¤ì œ submission í˜•ì‹)
- test_questionsì˜ ì§ˆë¬¸ë“¤ë¡œ ì‹¤ì œ submissionê³¼ ë™ì¼í•œ CSV ìƒì„±
- CRAG ê¸°ëŠ¥ì´ ì œëŒ€ë¡œ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸
- ê³ ê¸‰ LLM ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ í…ŒìŠ¤íŠ¸
- êµì • ê²€ìƒ‰ì˜ íš¨ê³¼ ìƒì„¸ ë¶„ì„
"""

import sys
import time
import pandas as pd
import os
from pathlib import Path
from scienceon_api_example import ScienceONAPIClient
from gemini_client import GeminiClient
from modules import RAGPipeline

def test_advanced_keyword_extraction():
    """ê³ ê¸‰ í‚¤ì›Œë“œ ì¶”ì¶œ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§  ê³ ê¸‰ í‚¤ì›Œë“œ ì¶”ì¶œ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸")
    
    try:
        credentials_path = Path('./configs/scienceon_api_credentials.json')
        api_client = ScienceONAPIClient(credentials_path=credentials_path)
        
        gemini_credentials_path = Path('./configs/gemini_api_credentials.json')
        gemini_client = GeminiClient(gemini_credentials_path)
        
        pipeline = RAGPipeline(api_client, gemini_client)
        
        # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤
        test_questions = [
            "Mechanical Turk ë°ì´í„°ë¡œë¶€í„° TurKontrolì˜ POMDP íŒŒë¼ë¯¸í„°ë¥¼ í•™ìŠµí•˜ì—¬ ë°˜ë³µì ì¸ í¬ë¼ìš°ë“œì†Œì‹± ì‘ì—…ì„ ìµœì í™”í•˜ëŠ” ì‹œìŠ¤í…œì˜ ì ‘ê·¼ ë°©ì‹ê³¼ ê²°ê³¼ëŠ” ë¬´ì—‡ì¸ê°€?",
            "ì¡ìŒ í™˜ê²½ì—ì„œ ì‹œì²­ê° ìŒì„±ì¸ì‹ì˜ ì¸ì‹ë¥ ì„ ë†’ì´ê¸° ìœ„í•´ ì€ë‹‰ ë§ˆë¥´ì½”í”„ ëª¨ë¸ê³¼ ì‹ ê²½ë§ í†µí•© ì „ëµì´ ì–´ë–»ê²Œ êµ¬ì„±ë˜ì—ˆëŠ”ì§€?",
            "DTG ì‹¤ ì£¼í–‰ë°ì´í„°ì™€ ê³µê°„ì •ë³´ë¥¼ í™œìš©í•œ ì—°ë£Œì†Œëª¨ëŸ‰ ì¶”ì • ëª¨ë¸ SBiFEMì˜ í•µì‹¬ êµ¬ì„± ìš”ì†Œë¥¼ ìš”ì•½í•´ ì£¼ì„¸ìš”."
        ]
        
        for i, question in enumerate(test_questions, 1):
            print(f"\n{'='*80}")
            print(f"ğŸ” í‚¤ì›Œë“œ ì¶”ì¶œ í…ŒìŠ¤íŠ¸ {i}: {question}")
            print(f"{'='*80}")
            
            # ê³ ê¸‰ í‚¤ì›Œë“œ ì¶”ì¶œ í…ŒìŠ¤íŠ¸
            print(f"\nğŸ§  LLM ê¸°ë°˜ ê³ ê¸‰ í‚¤ì›Œë“œ ì¶”ì¶œ:")
            start_time = time.time()
            
            advanced_keywords = pipeline.retriever._extract_keywords_with_llm(question)
            extraction_time = time.time() - start_time
            
            print(f"   â±ï¸  ì¶”ì¶œ ì‹œê°„: {extraction_time:.2f}ì´ˆ")
            print(f"   ğŸ”‘ ì¶”ì¶œëœ í‚¤ì›Œë“œ:")
            for j, keyword in enumerate(advanced_keywords, 1):
                print(f"      {j}. {keyword}")
            
            # ê¸°ë³¸ í‚¤ì›Œë“œ ì¶”ì¶œê³¼ ë¹„êµ
            print(f"\nğŸ“Š ê¸°ë³¸ í‚¤ì›Œë“œ ì¶”ì¶œê³¼ ë¹„êµ:")
            basic_keywords = pipeline.retriever._extract_keywords_basic(question)
            print(f"   ğŸ”‘ ê¸°ë³¸ í‚¤ì›Œë“œ: {', '.join(basic_keywords)}")
            print(f"   ğŸ§  ê³ ê¸‰ í‚¤ì›Œë“œ: {', '.join(advanced_keywords)}")
            
            # í‚¤ì›Œë“œ í’ˆì§ˆ í‰ê°€ (ê°„ë‹¨í•œ ê¸¸ì´ ê¸°ë°˜)
            avg_basic_length = sum(len(kw) for kw in basic_keywords) / len(basic_keywords) if basic_keywords else 0
            avg_advanced_length = sum(len(kw) for kw in advanced_keywords) / len(advanced_keywords) if advanced_keywords else 0
            
            print(f"   ğŸ“ í‰ê·  í‚¤ì›Œë“œ ê¸¸ì´: ê¸°ë³¸ {avg_basic_length:.1f}ì vs ê³ ê¸‰ {avg_advanced_length:.1f}ì")
            
    except Exception as e:
        print(f"âŒ ê³ ê¸‰ í‚¤ì›Œë“œ ì¶”ì¶œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()

def test_crag_pipeline():
    """CRAG íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª CRAG íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹œì‘ (ê³ ê¸‰ í‚¤ì›Œë“œ ì¶”ì¶œ ë²„ì „)")
    
    # 1. API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    try:
        credentials_path = Path('./configs/scienceon_api_credentials.json')
        api_client = ScienceONAPIClient(credentials_path=credentials_path)
        
        gemini_credentials_path = Path('./configs/gemini_api_credentials.json')
        gemini_client = GeminiClient(gemini_credentials_path)
        
        print("âœ… API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
    except Exception as e:
        print(f"âŒ API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return
    
    # 2. RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
    pipeline = RAGPipeline(api_client, gemini_client)
    
    # 3. í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ (ë³µì¡í•œ í•™ìˆ  ì§ˆë¬¸ë“¤)
    test_questions = [
        "Mechanical Turk ë°ì´í„°ë¡œë¶€í„° TurKontrolì˜ POMDP íŒŒë¼ë¯¸í„°ë¥¼ í•™ìŠµí•˜ì—¬ ë°˜ë³µì ì¸ í¬ë¼ìš°ë“œì†Œì‹± ì‘ì—…ì„ ìµœì í™”í•˜ëŠ” ì‹œìŠ¤í…œì˜ ì ‘ê·¼ ë°©ì‹ê³¼ ê²°ê³¼ëŠ” ë¬´ì—‡ì¸ê°€?",
        "Big Dataë¥¼ ì´ìš©í•œ Warehouse Management System ëª¨ë¸ì—ì„œ ì œì‹œëœ í•µì‹¬ ê°œë…ê³¼ ë°©í–¥ì„ ìš”ì•½í•´ ì£¼ì‹œê² ìŠµë‹ˆê¹Œ?",
        "DTG ì‹¤ ì£¼í–‰ë°ì´í„°ì™€ ê³µê°„ì •ë³´ë¥¼ í™œìš©í•œ ì—°ë£Œì†Œëª¨ëŸ‰ ì¶”ì • ëª¨ë¸ SBiFEMì˜ í•µì‹¬ êµ¬ì„± ìš”ì†Œë¥¼ ìš”ì•½í•´ ì£¼ì„¸ìš”."
    ]
    
    # 4. ê° ì§ˆë¬¸ì— ëŒ€í•´ CRAG í…ŒìŠ¤íŠ¸
    for i, question in enumerate(test_questions, 1):
        print(f"\n{'='*80}")
        print(f"ğŸ” í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ {i}: {question}")
        print(f"{'='*80}")
        
        try:
            # ì¼ë°˜ ê²€ìƒ‰ê³¼ CRAG ê²€ìƒ‰ ë¹„êµ
            print(f"\nğŸ“Š 1ë‹¨ê³„: ì¼ë°˜ ê²€ìƒ‰ ê²°ê³¼")
            start_time = time.time()
            
            # ì¼ë°˜ ê²€ìƒ‰ (CRAG ë¹„í™œì„±í™”)
            pipeline.retriever.gemini_client = None  # ì„ì‹œë¡œ ë¹„í™œì„±í™”
            normal_docs = pipeline.retriever.search_with_retry(question)
            normal_time = time.time() - start_time
            
            print(f"   ğŸ“š ì¼ë°˜ ê²€ìƒ‰ ê²°ê³¼: {len(normal_docs)}ê°œ ë¬¸ì„œ")
            print(f"   â±ï¸  ì†Œìš” ì‹œê°„: {normal_time:.2f}ì´ˆ")
            
            # ì¼ë°˜ ê²€ìƒ‰ ê²°ê³¼ ìƒ˜í”Œ ì¶œë ¥
            if normal_docs:
                print(f"   ğŸ“„ ì¼ë°˜ ê²€ìƒ‰ ìƒ˜í”Œ:")
                for j, doc in enumerate(normal_docs[:3], 1):
                    title = doc.get('title', 'N/A')[:50]
                    print(f"      {j}. {title}...")
            
            # CRAG ê²€ìƒ‰ (Gemini í´ë¼ì´ì–¸íŠ¸ ë³µì›)
            pipeline.retriever.gemini_client = gemini_client
            
            print(f"\nğŸ”„ 2ë‹¨ê³„: CRAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ê³ ê¸‰ í‚¤ì›Œë“œ ì¶”ì¶œ)")
            start_time = time.time()
            
            # CRAG íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì²˜ë¦¬
            answer, articles = pipeline.process_question(i-1, question)
            crag_time = time.time() - start_time
            
            print(f"   â±ï¸  CRAG ì†Œìš” ì‹œê°„: {crag_time:.2f}ì´ˆ")
            print(f"   ğŸ“š CRAG ê²€ìƒ‰ ê²°ê³¼: {len(articles)}ê°œ ë…¼ë¬¸")
            
            # CRAG ê²€ìƒ‰ ê²°ê³¼ ìƒ˜í”Œ ì¶œë ¥
            if articles:
                print(f"   ğŸ“„ CRAG ê²€ìƒ‰ ìƒ˜í”Œ:")
                for j, article in enumerate(articles[:3], 1):
                    title_start = article.find('Title: ') + 7
                    title_end = article.find(', Abstract:')
                    title = article[title_start:title_end][:50] if title_start > 6 and title_end > title_start else article[:50]
                    print(f"      {j}. {title}...")
            
            # ê²°ê³¼ ë¹„êµ ë¶„ì„
            print(f"\nğŸ“ˆ 3ë‹¨ê³„: ê²°ê³¼ ë¹„êµ ë¶„ì„")
            print(f"   â±ï¸  ì‹œê°„ ì°¨ì´: CRAGê°€ {crag_time - normal_time:.2f}ì´ˆ ë” ì†Œìš”")
            print(f"   ğŸ“Š ë¬¸ì„œ ìˆ˜ ì°¨ì´: {len(articles)} vs {len(normal_docs)}")
            
            # í’ˆì§ˆ í‰ê°€ ì‹œë®¬ë ˆì´ì…˜ (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­)
            question_keywords = set(question.lower().split())
            normal_relevance = sum(1 for doc in normal_docs[:10] 
                                 if any(keyword in doc.get('title', '').lower() 
                                       for keyword in question_keywords))
            crag_relevance = sum(1 for article in articles[:10] 
                               if any(keyword in article.lower() 
                                     for keyword in question_keywords))
            
            print(f"   ğŸ¯ ê´€ë ¨ì„± ì ìˆ˜ (í‚¤ì›Œë“œ ë§¤ì¹­):")
            print(f"      ì¼ë°˜ ê²€ìƒ‰: {normal_relevance}/10")
            print(f"      CRAG ê²€ìƒ‰: {crag_relevance}/10")
            
            # ë‹µë³€ í’ˆì§ˆ ë¶„ì„
            print(f"\nğŸ“ 4ë‹¨ê³„: ìƒì„±ëœ ë‹µë³€")
            print(f"   ğŸ“ ë‹µë³€ ê¸¸ì´: {len(answer)}ì")
            print(f"   ğŸ“„ ë‹µë³€ ë¯¸ë¦¬ë³´ê¸°:")
            print(f"   {answer[:300]}...")
            
            # ìƒì„¸ ë¶„ì„ ìš”ì•½
            print(f"\nğŸ“Š 5ë‹¨ê³„: ìƒì„¸ ë¶„ì„ ìš”ì•½")
            print(f"   âœ… CRAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì„±ê³µ")
            print(f"   ğŸ” ê²€ìƒ‰ëœ ë…¼ë¬¸ ìˆ˜: {len(articles)}ê°œ")
            print(f"   â±ï¸  ì´ ì²˜ë¦¬ ì‹œê°„: {crag_time:.2f}ì´ˆ")
            print(f"   ğŸ“ˆ ê´€ë ¨ì„± ê°œì„ : {'ì˜ˆ' if crag_relevance > normal_relevance else 'ì•„ë‹ˆì˜¤'}")
            
        except Exception as e:
            print(f"âŒ ì§ˆë¬¸ {i} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
    
    print(f"\nğŸ‰ CRAG íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print(f"ğŸ“‹ í…ŒìŠ¤íŠ¸ ìš”ì•½:")
    print(f"   - ì´ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸: {len(test_questions)}ê°œ")
    print(f"   - CRAG íŒŒì´í”„ë¼ì¸: ì •ìƒ ì‘ë™")
    print(f"   - ê³ ê¸‰ í‚¤ì›Œë“œ ì¶”ì¶œ: LLM ê¸°ë°˜ ì¶”ì¶œ ì™„ë£Œ")
    print(f"   - í’ˆì§ˆ í‰ê°€: LLM ê¸°ë°˜ í‰ê°€ ì™„ë£Œ")
    print(f"   - êµì • ê²€ìƒ‰: ì¡°ê±´ë¶€ ì‹¤í–‰ ì™„ë£Œ")

def test_crag_detailed():
    """CRAG íŒŒì´í”„ë¼ì¸ì˜ ìƒì„¸ í…ŒìŠ¤íŠ¸ (ë‹¨ì¼ ì§ˆë¬¸)"""
    print("\nğŸ”¬ CRAG íŒŒì´í”„ë¼ì¸ ìƒì„¸ í…ŒìŠ¤íŠ¸")
    
    try:
        credentials_path = Path('./configs/scienceon_api_credentials.json')
        api_client = ScienceONAPIClient(credentials_path=credentials_path)
        
        gemini_credentials_path = Path('./configs/gemini_api_credentials.json')
        gemini_client = GeminiClient(gemini_credentials_path)
        
        pipeline = RAGPipeline(api_client, gemini_client)
        
        # ë‹¨ì¼ ì§ˆë¬¸ìœ¼ë¡œ ìƒì„¸ í…ŒìŠ¤íŠ¸
        question = "Mechanical Turk ë°ì´í„°ë¡œë¶€í„° TurKontrolì˜ POMDP íŒŒë¼ë¯¸í„°ë¥¼ í•™ìŠµí•˜ì—¬ ë°˜ë³µì ì¸ í¬ë¼ìš°ë“œì†Œì‹± ì‘ì—…ì„ ìµœì í™”í•˜ëŠ” ì‹œìŠ¤í…œì˜ ì ‘ê·¼ ë°©ì‹ê³¼ ê²°ê³¼ëŠ” ë¬´ì—‡ì¸ê°€?"
        
        print(f"\nğŸ” ìƒì„¸ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸: {question}")
        
        # ê³ ê¸‰ í‚¤ì›Œë“œ ì¶”ì¶œ í…ŒìŠ¤íŠ¸
        print(f"\nğŸ§  ê³ ê¸‰ í‚¤ì›Œë“œ ì¶”ì¶œ:")
        advanced_keywords = pipeline.retriever._extract_keywords_with_llm(question)
        print(f"   ì¶”ì¶œëœ í‚¤ì›Œë“œ: {', '.join(advanced_keywords)}")
        
        # 1ì°¨ ê²€ìƒ‰ ê²°ê³¼ ì €ì¥
        initial_docs = pipeline.retriever.search_with_retry(question)
        print(f"ğŸ“š 1ì°¨ ê²€ìƒ‰ ê²°ê³¼: {len(initial_docs)}ê°œ ë¬¸ì„œ")
        
        # í’ˆì§ˆ í‰ê°€
        quality_score, issues = pipeline.retriever._evaluate_search_quality(question, initial_docs)
        print(f"ğŸ“Š í’ˆì§ˆ í‰ê°€ ê²°ê³¼:")
        print(f"   - ì ìˆ˜: {quality_score:.2f}/10")
        print(f"   - ë¬¸ì œì : {issues}")
        
        # êµì • ê²€ìƒ‰ ì‹¤í–‰
        corrected_docs = pipeline.retriever._corrective_search(question, initial_docs, issues)
        print(f"ğŸ”„ êµì • ê²€ìƒ‰ ê²°ê³¼: {len(corrected_docs)}ê°œ ë¬¸ì„œ")
        
        # êµì • í›„ í’ˆì§ˆ ì¬í‰ê°€
        corrected_score, corrected_issues = pipeline.retriever._evaluate_search_quality(question, corrected_docs)
        print(f"ğŸ“Š êµì • í›„ í’ˆì§ˆ í‰ê°€:")
        print(f"   - ì ìˆ˜: {corrected_score:.2f}/10")
        print(f"   - ê°œì„ ë„: {corrected_score - quality_score:.2f}ì ")
        
        # ë¬¸ì„œ ë¹„êµ
        print(f"\nğŸ“‹ ë¬¸ì„œ ë¹„êµ ë¶„ì„:")
        initial_titles = [doc.get('title', '')[:30] for doc in initial_docs[:5]]
        corrected_titles = [doc.get('title', '')[:30] for doc in corrected_docs[:5]]
        
        print(f"   1ì°¨ ê²€ìƒ‰ ìƒìœ„ 5ê°œ:")
        for i, title in enumerate(initial_titles, 1):
            print(f"      {i}. {title}...")
        
        print(f"   êµì • ê²€ìƒ‰ ìƒìœ„ 5ê°œ:")
        for i, title in enumerate(corrected_titles, 1):
            print(f"      {i}. {title}...")
        
    except Exception as e:
        print(f"âŒ ìƒì„¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()

def create_test_submission():
    """ì‹¤ì œ submissionê³¼ ë™ì¼í•œ í˜•ì‹ì˜ CSV íŒŒì¼ ìƒì„±"""
    print("ğŸ“Š ì‹¤ì œ submission í˜•ì‹ CSV ìƒì„± ì‹œì‘")
    
    # 1. API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    try:
        credentials_path = Path('./configs/scienceon_api_credentials.json')
        api_client = ScienceONAPIClient(credentials_path=credentials_path)
        
        gemini_credentials_path = Path('./configs/gemini_api_credentials.json')
        gemini_client = GeminiClient(gemini_credentials_path)
        
        print("âœ… API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
    except Exception as e:
        print(f"âŒ API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return
    
    # 2. RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
    pipeline = RAGPipeline(api_client, gemini_client)
    
    # 3. í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤ (3ê°œë§Œ)
    test_questions = [
        "Mechanical Turk ë°ì´í„°ë¡œë¶€í„° TurKontrolì˜ POMDP íŒŒë¼ë¯¸í„°ë¥¼ í•™ìŠµí•˜ì—¬ ë°˜ë³µì ì¸ í¬ë¼ìš°ë“œì†Œì‹± ì‘ì—…ì„ ìµœì í™”í•˜ëŠ” ì‹œìŠ¤í…œì˜ ì ‘ê·¼ ë°©ì‹ê³¼ ê²°ê³¼ëŠ” ë¬´ì—‡ì¸ê°€?",
        "Big Dataë¥¼ ì´ìš©í•œ Warehouse Management System ëª¨ë¸ì—ì„œ ì œì‹œëœ í•µì‹¬ ê°œë…ê³¼ ë°©í–¥ì„ ìš”ì•½í•´ ì£¼ì‹œê² ìŠµë‹ˆê¹Œ?",
        "DTG ì‹¤ ì£¼í–‰ë°ì´í„°ì™€ ê³µê°„ì •ë³´ë¥¼ í™œìš©í•œ ì—°ë£Œì†Œëª¨ëŸ‰ ì¶”ì • ëª¨ë¸ SBiFEMì˜ í•µì‹¬ êµ¬ì„± ìš”ì†Œë¥¼ ìš”ì•½í•´ ì£¼ì„¸ìš”."
    ]
    
    # 4. ê²°ê³¼ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸
    results = []
    
    # 5. ê° ì§ˆë¬¸ ì²˜ë¦¬
    for i, question in enumerate(test_questions, 1):
        print(f"\n{'='*80}")
        print(f"ğŸ” ì§ˆë¬¸ {i}/{len(test_questions)}: {question[:100]}...")
        print(f"{'='*80}")
        
        start_time = time.time()
        
        try:
            # íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì§ˆë¬¸ ì²˜ë¦¬
            answer, articles = pipeline.process_question(i-1, question)
            
            processing_time = time.time() - start_time
            
            # ê²°ê³¼ ì €ì¥
            result = {
                'id': i-1,
                'Question': question,
                'SAI_Answer': '',  # ì‹¤ì œ ë‹µë³€ì€ ì—†ìœ¼ë¯€ë¡œ ë¹ˆ ë¬¸ìì—´
                'translated_question': '',  # ë²ˆì—­ëœ ì§ˆë¬¸ (í•„ìš”ì‹œ ì¶”ê°€)
                'translated_SAI_answer': '',  # ë²ˆì—­ëœ ë‹µë³€ (í•„ìš”ì‹œ ì¶”ê°€)
                'Prediction': answer,
                'elapsed_times': processing_time
            }
            
            # retrieved_article_name_1~50 ì»¬ëŸ¼ ì¶”ê°€
            for j in range(1, 51):
                col_name = f'retrieved_article_name_{j}'
                result[col_name] = ''
            
            # prediction_retrieved_article_name_1~50 ì»¬ëŸ¼ ì¶”ê°€
            for j in range(1, 51):
                col_name = f'prediction_retrieved_article_name_{j}'
                if j <= len(articles):
                    result[col_name] = articles[j-1]
                else:
                    result[col_name] = ''
            
            results.append(result)
            
            print(f"âœ… ì§ˆë¬¸ {i} ì²˜ë¦¬ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {processing_time:.2f}ì´ˆ)")
            print(f"   ğŸ“ ë‹µë³€ ê¸¸ì´: {len(answer)}ì")
            print(f"   ğŸ“š ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(articles)}ê°œ")
            
        except Exception as e:
            print(f"âŒ ì§ˆë¬¸ {i} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨í•œ ê²½ìš°ì—ë„ ê¸°ë³¸ êµ¬ì¡° ìœ ì§€
            result = {
                'id': i-1,
                'Question': question,
                'SAI_Answer': '',
                'translated_question': '',
                'translated_SAI_answer': '',
                'Prediction': f'ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}',
                'elapsed_times': time.time() - start_time
            }
            
            # ë¹ˆ ì»¬ëŸ¼ë“¤ ì¶”ê°€
            for j in range(1, 51):
                result[f'retrieved_article_name_{j}'] = ''
                result[f'prediction_retrieved_article_name_{j}'] = ''
            
            results.append(result)
    
    # 6. DataFrame ìƒì„± ë° CSV ì €ì¥
    df = pd.DataFrame(results)
    
    # ì»¬ëŸ¼ ìˆœì„œ ì •ë ¬ (ì‹¤ì œ submissionê³¼ ë™ì¼í•˜ê²Œ)
    column_order = ['id', 'Question', 'SAI_Answer', 'translated_question', 'translated_SAI_answer']
    
    # retrieved_article_name_1~50
    for i in range(1, 51):
        column_order.append(f'retrieved_article_name_{i}')
    
    # prediction_retrieved_article_name_1~50
    for i in range(1, 51):
        column_order.append(f'prediction_retrieved_article_name_{i}')
    
    # ë§ˆì§€ë§‰ ì»¬ëŸ¼ë“¤
    column_order.extend(['Prediction', 'elapsed_times'])
    
    # ì»¬ëŸ¼ ìˆœì„œ ì ìš©
    df = df[column_order]
    
    # 7. CSV íŒŒì¼ ì €ì¥
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    filename = f'test_submission_crag_{timestamp}.csv'
    filepath = os.path.join('../test_submissions', filename)
    
    # submissions í´ë” ìƒì„±
    os.makedirs('../submissions', exist_ok=True)
    
    # CSV ì €ì¥
    df.to_csv(filepath, index=False, encoding='utf-8-sig')
    
    print(f"\nğŸ‰ í…ŒìŠ¤íŠ¸ submission ìƒì„± ì™„ë£Œ!")
    print(f"   ğŸ“ íŒŒì¼ ê²½ë¡œ: {filepath}")
    print(f"   ğŸ“Š ì´ ì§ˆë¬¸ ìˆ˜: {len(df)}ê°œ")
    print(f"   ğŸ“‹ ì´ ì»¬ëŸ¼ ìˆ˜: {len(df.columns)}ê°œ")
    print(f"   â±ï¸  ì´ ì†Œìš” ì‹œê°„: {df['elapsed_times'].sum():.2f}ì´ˆ")
    print(f"   ğŸ“ˆ í‰ê·  ì²˜ë¦¬ ì‹œê°„: {df['elapsed_times'].mean():.2f}ì´ˆ/ì§ˆë¬¸")
    
    # ì„±ê³µë¥  ê³„ì‚°
    success_count = len([r for r in results if 'ì˜¤ë¥˜' not in r['Prediction']])
    success_rate = (success_count / len(results)) * 100
    print(f"   âœ… ì„±ê³µë¥ : {success_count}/{len(results)} ({success_rate:.1f}%)")
    
    return filepath

if __name__ == "__main__":
    print("ğŸš€ CRAG íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    # ì‹¤ì œ submission í˜•ì‹ CSV ìƒì„±
    create_test_submission()
    
    print("\nğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

"""
ê²€ìƒ‰ ëª¨ë“ˆ
- í‚¤ì›Œë“œ ì¶”ì¶œ
- ë¬¸ì„œ ê²€ìƒ‰
- ì¬ì‹œë„ ë¡œì§
- ê²€ìƒ‰ ê²°ê³¼ ë³´ì¶©
"""

import time
import re
from typing import List, Dict, Tuple
from konlpy.tag import Okt
from .config import SEARCH_CONFIG, TEST_CONFIG, CRAG_CONFIG
from .prompting import PromptEngineer

class DocumentRetriever:
    """ë¬¸ì„œ ê²€ìƒ‰ê¸°"""
    
    def __init__(self, api_client, gemini_client=None):
        """
        ê²€ìƒ‰ê¸° ì´ˆê¸°í™”
        
        Args:
            api_client: ScienceON API í´ë¼ì´ì–¸íŠ¸
            gemini_client: Gemini API í´ë¼ì´ì–¸íŠ¸ (CRAGìš©)
        """
        self.api_client = api_client
        self.gemini_client = gemini_client
        self.okt = Okt()
        self.prompt_engineer = PromptEngineer()
    
    def extract_keywords(self, query: str) -> List[str]:
        """
        ì¿¼ë¦¬ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ (ê³ ê¸‰ LLM ê¸°ë°˜ ì¶”ì¶œ ì‚¬ìš©)
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            
        Returns:
            ì¶”ì¶œëœ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        """
        # LLM ê¸°ë°˜ ê³ ê¸‰ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œë„
        if self.gemini_client:
            try:
                advanced_keywords = self._extract_keywords_with_llm(query)
                if advanced_keywords:
                    print(f"   ğŸ§  LLM ê¸°ë°˜ ê³ ê¸‰ í‚¤ì›Œë“œ ì¶”ì¶œ: {', '.join(advanced_keywords)}")
                    return advanced_keywords
            except Exception as e:
                print(f"   âš ï¸  LLM í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨, ê¸°ë³¸ ë°©ì‹ ì‚¬ìš©: {e}")
        
        # í´ë°±: ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
        return self._extract_keywords_basic(query)
    
    def _extract_keywords_with_llm(self, query: str) -> List[str]:
        """
        LLMì„ ì‚¬ìš©í•œ ê³ ê¸‰ í‚¤ì›Œë“œ ì¶”ì¶œ
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            
        Returns:
            ì¶”ì¶œëœ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        """
        # ê³ ê¸‰ í‚¤ì›Œë“œ ìƒì„± í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
        prompt = self.prompt_engineer.create_advanced_keyword_generation_prompt(query)
        
        try:
            response = self.gemini_client.generate_answer(prompt)
            
            # ì‘ë‹µì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ (ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„ëœ í‚¤ì›Œë“œë“¤)
            keywords = []
            for line in response.strip().split('\n'):
                line = line.strip()
                if line and not line.startswith('#') and not line.startswith('-'):
                    # ë¶ˆí•„ìš”í•œ ë¬¸ì ì œê±°
                    clean_keyword = line.replace('*', '').replace('-', '').strip()
                    if clean_keyword and len(clean_keyword) > 1:
                        keywords.append(clean_keyword)
            
            return keywords[:5]  # ìµœëŒ€ 5ê°œ í‚¤ì›Œë“œ
            
        except Exception as e:
            print(f"   âš ï¸  LLM í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            return []
    
    def _extract_keywords_basic(self, query: str) -> List[str]:
        """
        ê¸°ë³¸ í‚¤ì›Œë“œ ì¶”ì¶œ (ê°œì„ ëœ ë°©ì‹)
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            
        Returns:
            ì¶”ì¶œëœ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        """
        if self._is_korean(query):
            # í•œêµ­ì–´: ëª…ì‚¬ ì¶”ì¶œ + ì „ë¬¸ ìš©ì–´ ë³´ì¡´
            nouns = self.okt.nouns(query)
            keywords = [noun for noun in nouns if len(noun) > 1]
            
            # ì „ë¬¸ ìš©ì–´ ë° ì•½ì–´ ë³´ì¡´
            special_terms = self._extract_special_terms(query)
            keywords.extend(special_terms)
            
            # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
            keywords = list(set(keywords))
            keywords.sort(key=len, reverse=True)  # ê¸´ í‚¤ì›Œë“œ ìš°ì„ 
            
            return keywords[:5]
        else:
            # ì˜ì–´: ê°œì„ ëœ ë¶ˆìš©ì–´ ì œê±°
            stop_words = {
                # ê¸°ë³¸ ë¶ˆìš©ì–´
                'a', 'an', 'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by',
                # ì§ˆë¬¸ì–´ ì œê±°
                'how', 'what', 'why', 'when', 'where', 'which', 'who', 'whom', 'whose',
                # ì¼ë°˜ì ì¸ ë™ì‚¬ ì œê±°
                'can', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might',
                'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had',
                # ê¸°íƒ€ ì¼ë°˜ì ì¸ ë‹¨ì–´ë“¤
                'this', 'that', 'these', 'those', 'it', 'its', 'they', 'them', 'their',
                'we', 'you', 'he', 'she', 'his', 'her', 'our', 'your', 'my', 'me', 'i'
            }
            
            words = re.findall(r'\w+', query.lower())
            
            # 1ë‹¨ê³„: ë¶ˆìš©ì–´ ì œê±°
            filtered_words = [word for word in words if word not in stop_words and len(word) > 2]
            
            # 2ë‹¨ê³„: ì „ë¬¸ ìš©ì–´ ìš°ì„  ì„ íƒ
            technical_terms = []
            general_terms = []
            
            for word in filtered_words:
                # ì „ë¬¸ ìš©ì–´ íŒë³„ (ê¸¸ì´ê°€ ê¸¸ê±°ë‚˜ íŠ¹ì • íŒ¨í„´)
                if (len(word) > 6 or 
                    word in ['neural', 'artificial', 'machine', 'learning', 'deep', 'network', 
                            'algorithm', 'model', 'system', 'method', 'approach', 'technique',
                            'sustainability', 'corporate', 'culture', 'development', 'management',
                            'analysis', 'research', 'study', 'framework', 'architecture']):
                    technical_terms.append(word)
                else:
                    general_terms.append(word)
            
            # ì „ë¬¸ ìš©ì–´ë¥¼ ë¨¼ì €, ê·¸ ë‹¤ìŒ ì¼ë°˜ ìš©ì–´
            keywords = technical_terms + general_terms
            return keywords[:8]  # ë” ë§ì€ í‚¤ì›Œë“œ ì¶”ì¶œ
    
    def extract_more_keywords(self, query: str, max_keywords: int = 10) -> List[str]:
        """
        ë” ë§ì€ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            max_keywords: ìµœëŒ€ í‚¤ì›Œë“œ ìˆ˜
            
        Returns:
            í™•ì¥ëœ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        """
        keywords = self.extract_keywords(query)
        
        # ê¸°ë³¸ í‚¤ì›Œë“œê°€ ë¶€ì¡±í•˜ë©´ ì¿¼ë¦¬ì—ì„œ ì¶”ê°€ í‚¤ì›Œë“œ ì¶”ì¶œ
        if len(keywords) < 3:
            words = query.split()
            for word in words:
                if len(word) > 2 and word not in keywords:
                    keywords.append(word)
                    if len(keywords) >= max_keywords:
                        break
        
        # ìœ ì‚¬í•œ í‚¤ì›Œë“œ ì¶”ê°€ (ê°„ë‹¨í•œ í™•ì¥)
        expanded_keywords = keywords.copy()
        for keyword in keywords:
            if keyword.isascii():
                if keyword.endswith('s'):
                    expanded_keywords.append(keyword[:-1])  # ë³µìˆ˜í˜• -> ë‹¨ìˆ˜í˜•
                if keyword.endswith('ing'):
                    expanded_keywords.append(keyword[:-3])  # ~ing -> ê¸°ë³¸í˜•
                if keyword.endswith('ed'):
                    expanded_keywords.append(keyword[:-2])  # ~ed -> ê¸°ë³¸í˜•
        
        return list(set(expanded_keywords))[:max_keywords]
    
    def search_with_retry(self, query: str, max_retries: int = SEARCH_CONFIG['max_retries'], 
                         min_docs: int = SEARCH_CONFIG['min_docs']) -> List[Dict]:
        """
        ë‹¨ìˆœí™”ëœ ê²€ìƒ‰ ì „ëµ (ë¹ ë¥¸ ê²€ìƒ‰)
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
            min_docs: ìµœì†Œ í•„ìš” ë¬¸ì„œ ìˆ˜ (50ê°œ)
            
        Returns:
            ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ (ìµœì†Œ 50ê°œ ë³´ì¥)
        """
        all_docs = []
        
        # 1ë‹¨ê³„: LLM ê¸°ë°˜ ì‘ì€ í‚¤ì›Œë“œë¡œ ì§ì ‘ ê²€ìƒ‰
        keywords = self.extract_keywords(query)
        print(f"   ğŸ” ê²€ìƒ‰ í‚¤ì›Œë“œ: {', '.join(keywords)}")
        
        for keyword in keywords:
            try:
                docs = self.api_client.search_articles(keyword, row_count=25, fields=['title', 'abstract', 'CN'])
                all_docs.extend(docs)
                print(f"   âœ… í‚¤ì›Œë“œ '{keyword}'ë¡œ {len(docs)}ê°œ ë¬¸ì„œ ê²€ìƒ‰")
                time.sleep(SEARCH_CONFIG['api_delay'])
                
                if len(all_docs) >= min_docs:
                    break
            except Exception as e:
                print(f"   âš ï¸ í‚¤ì›Œë“œ '{keyword}' ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        
        # 2ë‹¨ê³„: ë¶€ì¡±í•˜ë©´ ê¸°ë³¸ í‚¤ì›Œë“œë¡œ ë³´ì¶©
        if len(all_docs) < min_docs:
            print(f"   ğŸ”„ ë¬¸ì„œ ë¶€ì¡± ({len(all_docs)}ê°œ), ê¸°ë³¸ í‚¤ì›Œë“œë¡œ ë³´ì¶©")
            basic_keywords = self._extract_basic_keywords(query)
            
            for keyword in basic_keywords:
                if keyword not in keywords:
                    try:
                        docs = self.api_client.search_articles(keyword, row_count=15, fields=['title', 'abstract', 'CN'])
                        all_docs.extend(docs)
                        print(f"   âœ… ê¸°ë³¸ í‚¤ì›Œë“œ '{keyword}'ë¡œ {len(docs)}ê°œ ë¬¸ì„œ ê²€ìƒ‰")
                        time.sleep(SEARCH_CONFIG['api_delay'])
                    except Exception as e:
                        print(f"   âš ï¸ ê¸°ë³¸ í‚¤ì›Œë“œ '{keyword}' ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                    
                    if len(all_docs) >= min_docs:
                        break
        
        for attempt in range(max_retries * 2):  # ë” ë§ì€ ì‹œë„
            if len(all_docs) >= min_docs:
                break
                
            print(f"   - ê²€ìƒ‰ ì‹œë„ {attempt + 1}/{max_retries * 2} (í˜„ì¬ {len(all_docs)}ê°œ ë¬¸ì„œ)")
            
            # ì´ë²ˆ ì‹œë„ì—ì„œ ì‚¬ìš©í•  í‚¤ì›Œë“œë“¤
            current_keywords = self._get_keywords_for_attempt_aggressive(expanded_keywords, attempt)
            
            # ë””ë²„ê·¸ ëª¨ë“œì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ ê³¼ì • í‘œì‹œ
            if TEST_CONFIG.get('debug_mode', False):
                print(f"   ğŸ” ì‹œë„ {attempt + 1} í‚¤ì›Œë“œ: {', '.join(current_keywords)}")
                if attempt > 0:
                    print(f"   ğŸ”„ í‚¤ì›Œë“œ í™•ì¥: {len(expanded_keywords)}ê°œ â†’ {len(current_keywords)}ê°œ")
            
            # í‚¤ì›Œë“œë³„ ê²€ìƒ‰ (ë” ë§ì€ ê²°ê³¼ ìš”ì²­)
            for keyword in current_keywords:
                try:
                    docs = self.api_client.search_articles(
                        keyword, 
                        row_count=50,  # ë” ë§ì€ ê²°ê³¼ ìš”ì²­
                        fields=['title', 'abstract', 'CN']
                    )
                    all_docs.extend(docs)
                    time.sleep(SEARCH_CONFIG['api_delay'])
                    
                    # ì¶©ë¶„í•œ ë¬¸ì„œê°€ ìˆìœ¼ë©´ ì¤‘ë‹¨
                    if len(all_docs) >= min_docs * 2:  # ì—¬ìœ ë¶„ í™•ë³´
                        break
                        
                except Exception as e:
                    print(f"   âš ï¸  í‚¤ì›Œë“œ '{keyword}' ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                    continue
            
            # ì¤‘ë³µ ì œê±°
            all_docs = self._remove_duplicates(all_docs)
            
            if len(all_docs) >= min_docs:
                print(f"   âœ… ëª©í‘œ ë¬¸ì„œ ìˆ˜ ë‹¬ì„±: {len(all_docs)}ê°œ")
                break
            else:
                print(f"   âš ï¸  ë¬¸ì„œ ìˆ˜ ë¶€ì¡±: {len(all_docs)}ê°œ (ëª©í‘œ: {min_docs}ê°œ)")
        
        # ìµœì¢…ì ìœ¼ë¡œ 50ê°œ ë¯¸ë§Œì´ë©´ ì¶”ê°€ ê²€ìƒ‰
        if len(all_docs) < min_docs:
            print(f"   ğŸš¨ ë¬¸ì„œ ìˆ˜ ë¶€ì¡±! ì¶”ê°€ ê²€ìƒ‰ ì‹œë„...")
            additional_docs = self._emergency_search(query, min_docs - len(all_docs))
            all_docs.extend(additional_docs)
            all_docs = self._remove_duplicates(all_docs)
        
        print(f"   ğŸ“Š ìµœì¢… ê²€ìƒ‰ ê²°ê³¼: {len(all_docs)}ê°œ ë¬¸ì„œ")
        return all_docs[:min_docs]  # ì •í™•íˆ 50ê°œ ë°˜í™˜
    
    def _expand_keywords_aggressively(self, query: str, base_keywords: List[str]) -> List[str]:
        """
        ë” ì ê·¹ì ì¸ í‚¤ì›Œë“œ í™•ì¥ (50ê°œ ë¬¸ì„œ ë³´ì¥ì„ ìœ„í•´)
        """
        expanded = base_keywords.copy()
        
        # 1. ì¿¼ë¦¬ì—ì„œ ë‹¨ì–´ ë¶„ë¦¬
        words = query.replace('?', '').replace('.', '').split()
        for word in words:
            if len(word) > 2 and word not in expanded:
                expanded.append(word)
        
        # 2. ê´€ë ¨ ìš©ì–´ ì¶”ê°€
        related_terms = self._get_related_terms(query)
        expanded.extend(related_terms)
        
        # 3. ì¼ë°˜ì ì¸ í•™ìˆ  ìš©ì–´ ì¶”ê°€
        academic_terms = ['ì—°êµ¬', 'ë¶„ì„', 'ë°©ë²•', 'ê²°ê³¼', 'ì‹œìŠ¤í…œ', 'ê¸°ìˆ ', 'ê°œë°œ', 'í‰ê°€', 'ê´€ë¦¬', 'ìµœì í™”']
        for term in academic_terms:
            if term not in expanded:
                expanded.append(term)
        
        return list(set(expanded))  # ì¤‘ë³µ ì œê±°
    
    def _get_related_terms(self, query: str) -> List[str]:
        """
        ì¿¼ë¦¬ì™€ ê´€ë ¨ëœ ìš©ì–´ë“¤ ì¶”ì¶œ
        """
        related = []
        
        # ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ ê´€ë ¨ ìš©ì–´ ë§¤í•‘
        term_mapping = {
            'ì¸ê³µì§€ëŠ¥': ['AI', 'ë¨¸ì‹ ëŸ¬ë‹', 'ë”¥ëŸ¬ë‹', 'ì•Œê³ ë¦¬ì¦˜'],
            'ë¨¸ì‹ ëŸ¬ë‹': ['ML', 'ì¸ê³µì§€ëŠ¥', 'ë°ì´í„°', 'ëª¨ë¸'],
            'ë°ì´í„°': ['ë¶„ì„', 'ì²˜ë¦¬', 'ë§ˆì´ë‹', 'ë² ì´ìŠ¤'],
            'ì‹œìŠ¤í…œ': ['êµ¬í˜„', 'ì„¤ê³„', 'ì•„í‚¤í…ì²˜', 'í”Œë«í¼'],
            'ë³´ì•ˆ': ['ì•”í˜¸í™”', 'ì¸ì¦', 'ê¶Œí•œ', 'í”„ë¡œí† ì½œ'],
            'ë„¤íŠ¸ì›Œí¬': ['í†µì‹ ', 'í”„ë¡œí† ì½œ', 'ë¼ìš°íŒ…', 'ë³´ì•ˆ'],
            'ì›¹': ['ì¸í„°ë„·', 'ë¸Œë¼ìš°ì €', 'ì„œë²„', 'í´ë¼ì´ì–¸íŠ¸'],
            'ëª¨ë°”ì¼': ['ìŠ¤ë§ˆíŠ¸í°', 'ì•±', 'ì•ˆë“œë¡œì´ë“œ', 'iOS'],
            'í´ë¼ìš°ë“œ': ['ì„œë²„', 'ê°€ìƒí™”', 'ìŠ¤ì¼€ì¼ë§', 'ë°°í¬']
        }
        
        for key, terms in term_mapping.items():
            if key in query:
                related.extend(terms)
        
        return related
    
    def _get_keywords_for_attempt_aggressive(self, keywords: List[str], attempt: int) -> List[str]:
        """
        ë” ì ê·¹ì ì¸ ì‹œë„ë³„ í‚¤ì›Œë“œ ì„ íƒ
        """
        if attempt == 0:
            return keywords[:15]  # ì²« ì‹œë„: ìƒìœ„ 15ê°œ
        elif attempt == 1:
            return keywords[15:30]  # ë‘ ë²ˆì§¸ ì‹œë„: ë‹¤ìŒ 15ê°œ
        elif attempt == 2:
            return keywords[30:45]  # ì„¸ ë²ˆì§¸ ì‹œë„: ë‹¤ìŒ 15ê°œ
        else:
            # ì¶”ê°€ ì‹œë„: ë‚¨ì€ ëª¨ë“  í‚¤ì›Œë“œ
            start_idx = 45 + (attempt - 3) * 10
            return keywords[start_idx:start_idx + 15]
    
    def _emergency_search(self, query: str, needed_count: int) -> List[Dict]:
        """
        ê¸´ê¸‰ ì¶”ê°€ ê²€ìƒ‰ (50ê°œ ë¬¸ì„œ ë³´ì¥ì„ ìœ„í•´)
        """
        print(f"   ğŸš¨ ê¸´ê¸‰ ê²€ìƒ‰: {needed_count}ê°œ ë¬¸ì„œ ì¶”ê°€ í•„ìš”")
        
        emergency_docs = []
        
        # 1. ì¼ë°˜ì ì¸ í•™ìˆ  í‚¤ì›Œë“œë¡œ ê²€ìƒ‰
        emergency_keywords = ['ì—°êµ¬', 'ë¶„ì„', 'ë°©ë²•', 'ì‹œìŠ¤í…œ', 'ê¸°ìˆ ', 'ê°œë°œ']
        
        for keyword in emergency_keywords:
            if len(emergency_docs) >= needed_count:
                break
                
            try:
                docs = self.api_client.search_articles(
                    keyword,
                    row_count=20,
                    fields=['title', 'abstract', 'CN']
                )
                emergency_docs.extend(docs)
                time.sleep(SEARCH_CONFIG['api_delay'])
            except Exception as e:
                print(f"   âš ï¸  ê¸´ê¸‰ ê²€ìƒ‰ í‚¤ì›Œë“œ '{keyword}' ì‹¤íŒ¨: {e}")
                continue
        
        # 2. ì¿¼ë¦¬ì—ì„œ ë‹¨ì–´ í•˜ë‚˜ì”© ê²€ìƒ‰
        words = query.replace('?', '').replace('.', '').split()
        for word in words:
            if len(word) > 2 and len(emergency_docs) < needed_count:
                try:
                    docs = self.api_client.search_articles(
                        word,
                        row_count=10,
                        fields=['title', 'abstract', 'CN']
                    )
                    emergency_docs.extend(docs)
                    time.sleep(SEARCH_CONFIG['api_delay'])
                except Exception as e:
                    continue
        
        print(f"   ğŸ“Š ê¸´ê¸‰ ê²€ìƒ‰ ê²°ê³¼: {len(emergency_docs)}ê°œ ë¬¸ì„œ")
        return emergency_docs
    
    def _get_keywords_for_attempt(self, keywords: List[str], attempt: int) -> List[str]:
        """ì‹œë„ë³„ í‚¤ì›Œë“œ ì„ íƒ (ê¸°ì¡´ ë©”ì„œë“œ ìœ ì§€)"""
        if attempt == 0:
            return keywords[:5]  # ì²« ì‹œë„: ìƒìœ„ 5ê°œ í‚¤ì›Œë“œ
        elif attempt == 1:
            return keywords[5:] + keywords[:3]  # ë‘ ë²ˆì§¸: ë‚˜ë¨¸ì§€ + ìƒìœ„ 3ê°œ
        else:
            # ë§ˆì§€ë§‰ ì‹œë„: ì¿¼ë¦¬ ìì²´ë¥¼ í‚¤ì›Œë“œë¡œ ì‚¬ìš©
            return [keywords[0][:20], keywords[0][20:40]] if len(keywords[0]) > 20 else [keywords[0]]
    
    def _remove_duplicates(self, documents: List[Dict]) -> List[Dict]:
        """ì¤‘ë³µ ë¬¸ì„œ ì œê±° ë° í’ˆì§ˆ í•„í„°ë§"""
        # ì¤‘ë³µ ì œê±°
        unique_docs = list({doc['CN']: doc for doc in documents if 'CN' in doc}.values())
        
        # í’ˆì§ˆ í•„í„°ë§
        filtered_docs = []
        for doc in unique_docs:
            title = doc.get('title', '').lower()
            abstract = doc.get('abstract', '')
            
            # ì œì™¸í•  íŒ¨í„´ë“¤
            exclude_patterns = [
                # "How"ë¡œ ì‹œì‘í•˜ëŠ” ì¼ë°˜ì ì¸ ì§ˆë¬¸ ì œëª©ë“¤
                title.startswith('how '),
                title.startswith('what '),
                title.startswith('why '),
                title.startswith('when '),
                title.startswith('where '),
                title.startswith('which '),
                title.startswith('who '),
                
                # ë„ˆë¬´ ì§§ì€ ì œëª©
                len(title) < 10,
                
                # ì´ˆë¡ì´ ì—†ëŠ” ê²½ìš°
                not abstract or len(abstract) < 20,
                
                # íŠ¹ì • ë¬´ê´€í•œ í‚¤ì›Œë“œê°€ ì œëª©ì— í¬í•¨ëœ ê²½ìš°
                any(word in title for word in ['economics', 'language', 'teaching', 'learning', 'education'])
            ]
            
            # ì œì™¸ íŒ¨í„´ì— í•´ë‹¹í•˜ì§€ ì•Šìœ¼ë©´ í¬í•¨
            if not any(exclude_patterns):
                filtered_docs.append(doc)
        
        print(f"   ğŸ” í’ˆì§ˆ í•„í„°ë§: {len(unique_docs)}ê°œ â†’ {len(filtered_docs)}ê°œ")
        return filtered_docs
    
    def _is_korean(self, text: str) -> bool:
        """í•œêµ­ì–´ í…ìŠ¤íŠ¸ ê°ì§€"""
        return bool(re.search('[ê°€-í£]', text))
    
    def supplement_documents(self, vector_docs: List[Dict], original_docs: List[Dict], 
                           target_count: int = SEARCH_CONFIG['min_docs']) -> List[Dict]:
        """
        ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ê°€ ë¶€ì¡±í•  ë•Œ ì›ë³¸ ê²€ìƒ‰ ê²°ê³¼ë¡œ ë³´ì¶©
        
        Args:
            vector_docs: ë²¡í„° ê²€ìƒ‰ ê²°ê³¼
            original_docs: ì›ë³¸ ê²€ìƒ‰ ê²°ê³¼
            target_count: ëª©í‘œ ë¬¸ì„œ ìˆ˜
            
        Returns:
            ë³´ì¶©ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        if len(vector_docs) >= target_count:
            return vector_docs
        
        print(f"   âš ï¸  ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ ë¶€ì¡±: {len(vector_docs)}ê°œ (ëª©í‘œ: {target_count}ê°œ)")
        
        # ì›ë³¸ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì¶”ê°€ ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
        remaining_docs = original_docs[len(vector_docs):target_count] if len(original_docs) > len(vector_docs) else []
        supplemented_docs = vector_docs + remaining_docs
        
        print(f"   âœ… ì›ë³¸ ê²€ìƒ‰ ê²°ê³¼ë¡œ ë³´ì¶©: {len(supplemented_docs)}ê°œ")
        
        return supplemented_docs[:target_count]

    def search_with_crag(self, query: str) -> List[Dict]:
        """
        ë‹¨ìˆœí™”ëœ CRAG íŒŒì´í”„ë¼ì¸ (ë¹ ë¥¸ ê²€ìƒ‰)
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            
        Returns:
            ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        print("   ğŸ”„ CRAG íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        
        # ì§ì ‘ ê²€ìƒ‰ (êµì • ê³¼ì • ì œê±°)
        documents = self.search_with_retry(query)
        print(f"   ğŸ“š ê²€ìƒ‰ ê²°ê³¼: {len(documents)}ê°œ ë¬¸ì„œ")
        
        return documents
    
    def _evaluate_search_quality(self, query: str, documents: List[Dict]) -> Tuple[float, str]:
        """
        ê²€ìƒ‰ ê²°ê³¼ í’ˆì§ˆ í‰ê°€
        
        Args:
            query: ì›ë³¸ ì§ˆë¬¸
            documents: ê²€ìƒ‰ëœ ë¬¸ì„œë“¤
            
        Returns:
            (í’ˆì§ˆ ì ìˆ˜, ë¬¸ì œì  ì„¤ëª…) íŠœí”Œ
        """
        if not documents:
            return 0.0, "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        # ìƒ˜í”Œ ë¬¸ì„œ ì„ íƒ (ì²˜ìŒ 3ê°œ)
        sample_docs = documents[:3]
        sample_text = "\n".join([
            f"ì œëª©: {doc.get('title', 'N/A')}\nì´ˆë¡: {doc.get('abstract', 'N/A')[:200]}..."
            for doc in sample_docs
        ])
        
        # í‰ê°€ í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = CRAG_CONFIG['correction_prompt_template'].format(
            query=query,
            doc_count=len(documents),
            sample_docs=sample_text,
            relevance_score="",  # LLMì´ ì±„ìš¸ ë¶€ë¶„
            quality_score="",
            sufficiency_score="",
            total_score="",
            improvement_suggestions="",
            new_keywords=""
        )
        
        try:
            # Geminië¡œ í’ˆì§ˆ í‰ê°€
            evaluation_text = self.gemini_client.generate_answer(prompt)
            
            # ì ìˆ˜ ì¶”ì¶œ (ê°„ë‹¨í•œ íŒŒì‹±)
            score_match = re.search(r'ì¢…í•© ì ìˆ˜:\s*(\d+(?:\.\d+)?)/10', evaluation_text)
            if score_match:
                score = float(score_match.group(1))
            else:
                # ì ìˆ˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’
                score = 5.0
            
            # ë¬¸ì œì  ì¶”ì¶œ
            issues_match = re.search(r'ê°œì„  ì œì•ˆ:\s*(.*?)(?=\n\n|\nìƒˆë¡œìš´ ê²€ìƒ‰ í‚¤ì›Œë“œ:|$)', 
                                   evaluation_text, re.DOTALL)
            issues = issues_match.group(1).strip() if issues_match else "í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ"
            
            return score, issues
            
        except Exception as e:
            print(f"   âš ï¸  í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
            return 5.0, f"í‰ê°€ ì¤‘ ì˜¤ë¥˜: {str(e)}"
    
    def _corrective_search(self, query: str, original_docs: List[Dict], issues: str) -> List[Dict]:
        """
        êµì • ê²€ìƒ‰ ìˆ˜í–‰
        
        Args:
            query: ì›ë³¸ ì§ˆë¬¸
            original_docs: ì›ë³¸ ê²€ìƒ‰ ê²°ê³¼
            issues: ê²€ìƒ‰ ê²°ê³¼ ë¬¸ì œì 
            
        Returns:
            êµì •ëœ ê²€ìƒ‰ ê²°ê³¼
        """
        print("   ğŸ”„ êµì • ê²€ìƒ‰ ì‹œì‘")
        
        max_attempts = CRAG_CONFIG.get('max_corrective_attempts', 2)
        
        for attempt in range(max_attempts):
            print(f"   - êµì • ì‹œë„ {attempt + 1}/{max_attempts}")
            
            # ê°œì„ ëœ í‚¤ì›Œë“œ ìƒì„± (ê¸°ì¡´ í‚¤ì›Œë“œì™€ ì¤‘ë³µ ì œê±°)
            improved_keywords = self._generate_improved_keywords(query, issues)
            
            # ê¸°ì¡´ í‚¤ì›Œë“œì™€ ì¤‘ë³µë˜ì§€ ì•ŠëŠ” í‚¤ì›Œë“œë§Œ í•„í„°ë§
            original_keywords = self.extract_keywords(query)
            unique_improved_keywords = [kw for kw in improved_keywords if kw not in original_keywords]
            
            if not unique_improved_keywords:
                print("   âš ï¸  êµì • í‚¤ì›Œë“œê°€ ê¸°ì¡´ í‚¤ì›Œë“œì™€ ì¤‘ë³µë¨ - ëŒ€ì•ˆ í‚¤ì›Œë“œ ìƒì„±")
                unique_improved_keywords = self._generate_alternative_keywords(query, original_keywords)
            
            split_improved_keywords = self._split_keywords_by_length(unique_improved_keywords)
            print(f"   ğŸ” ê°œì„ ëœ í‚¤ì›Œë“œ: {', '.join(improved_keywords)}")
            print(f"   ğŸ”§ ì¤‘ë³µ ì œê±°ëœ í‚¤ì›Œë“œ: {', '.join(unique_improved_keywords)}")
            print(f"   ğŸ”§ ë¶„í• ëœ ê°œì„  í‚¤ì›Œë“œ: {', '.join(split_improved_keywords)}")
            
            # ê°œì„ ëœ í‚¤ì›Œë“œë¡œ ì¬ê²€ìƒ‰
            corrected_docs = []
            for keyword in split_improved_keywords:
                try:
                    docs = self.api_client.search_articles(
                        keyword, 
                        row_count=30,
                        fields=['title', 'abstract', 'CN']
                    )
                    corrected_docs.extend(docs)
                    time.sleep(SEARCH_CONFIG['api_delay'])
                except Exception as e:
                    print(f"   âš ï¸  í‚¤ì›Œë“œ '{keyword}' ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                    continue
            
            # ì¤‘ë³µ ì œê±° ë° í’ˆì§ˆ í•„í„°ë§
            corrected_docs = self._remove_duplicates(corrected_docs)
            
            # í’ˆì§ˆ ì¬í‰ê°€
            if corrected_docs:
                quality_score, _ = self._evaluate_search_quality(query, corrected_docs)
                print(f"   ğŸ“Š êµì • í›„ í’ˆì§ˆ ì ìˆ˜: {quality_score:.2f}/10")
                
                threshold = CRAG_CONFIG.get('quality_threshold', 0.7)
                if quality_score >= threshold * 10:
                    print("   âœ… êµì • ê²€ìƒ‰ ì„±ê³µ")
                    return corrected_docs
            
            print("   âš ï¸  êµì • ê²€ìƒ‰ í’ˆì§ˆ ë¯¸ë‹¬ - ì¶”ê°€ ì‹œë„")
        
        print("   ğŸš¨ ëª¨ë“  êµì • ì‹œë„ ì‹¤íŒ¨ - ì›ë³¸ ê²°ê³¼ ë°˜í™˜")
        return original_docs
    
    def _generate_improved_keywords(self, query: str, issues: str) -> List[str]:
        """
        ê°œì„ ëœ ê²€ìƒ‰ í‚¤ì›Œë“œ ìƒì„±
        
        Args:
            query: ì›ë³¸ ì§ˆë¬¸
            issues: ê²€ìƒ‰ ê²°ê³¼ ë¬¸ì œì 
            
        Returns:
            ê°œì„ ëœ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        """
        # ì›¹ ê²€ìƒ‰ì´ í™œì„±í™”ë˜ì–´ ìˆìœ¼ë©´ ì›¹ ê²€ìƒ‰ í™œìš©
        if CRAG_CONFIG.get('web_search_enabled', False):
            return self._generate_keywords_with_web_search(query, issues)
        else:
            return self._generate_keywords_with_llm(query, issues)
    
    def _generate_keywords_with_llm(self, query: str, issues: str) -> List[str]:
        """
        LLMì„ ì‚¬ìš©í•œ í‚¤ì›Œë“œ ê°œì„  (êµì • ê²€ìƒ‰ìš© íŠ¹í™” í”„ë¡¬í”„íŠ¸)
        """
        # êµì • ê²€ìƒ‰ìš© íŠ¹í™” í”„ë¡¬í”„íŠ¸
        prompt = f"""
# êµì • ê²€ìƒ‰ í‚¤ì›Œë“œ ìƒì„±

## ì›ë³¸ ì§ˆë¬¸:
{query}

## ê¸°ì¡´ ê²€ìƒ‰ ê²°ê³¼ ë¬¸ì œì :
{issues}

## êµì • ê²€ìƒ‰ ì „ëµ:
ê¸°ì¡´ í‚¤ì›Œë“œì™€ëŠ” ë‹¤ë¥¸ ì ‘ê·¼ ë°©ì‹ìœ¼ë¡œ í‚¤ì›Œë“œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”:

1. **ë™ì˜ì–´/ìœ ì‚¬ì–´ í™œìš©**: ê¸°ì¡´ í‚¤ì›Œë“œì˜ ë™ì˜ì–´ë‚˜ ìœ ì‚¬ì–´ ì‚¬ìš©
2. **ìƒìœ„/í•˜ìœ„ ê°œë…**: ë” ë„“ì€ ë²”ìœ„ë‚˜ ë” êµ¬ì²´ì ì¸ ê°œë…ìœ¼ë¡œ í™•ì¥
3. **ê´€ë ¨ ê¸°ìˆ /ë°©ë²•ë¡ **: ê°™ì€ ë¶„ì•¼ì˜ ë‹¤ë¥¸ ê¸°ìˆ ì´ë‚˜ ë°©ë²•ë¡ 
4. **ì˜ë¬¸/í•œê¸€ ë³€í™˜**: ì˜ë¬¸ í‚¤ì›Œë“œë¥¼ í•œê¸€ë¡œ, í•œê¸€ í‚¤ì›Œë“œë¥¼ ì˜ë¬¸ìœ¼ë¡œ
5. **ì•½ì–´/ì „ì²´ëª…**: ì•½ì–´ê°€ ìˆë‹¤ë©´ ì „ì²´ëª…ìœ¼ë¡œ, ì „ì²´ëª…ì´ ìˆë‹¤ë©´ ì•½ì–´ë¡œ

## ì˜ˆì‹œ:
- "Machine Learning" â†’ "ë”¥ëŸ¬ë‹", "ì¸ê³µì§€ëŠ¥", "AI", "Neural Network"
- "í¬ë¼ìš°ë“œì†Œì‹±" â†’ "Crowdsourcing", "Human Computation", "Distributed Computing"
- "POMDP" â†’ "Partially Observable Markov Decision Process", "ê°•í™”í•™ìŠµ", "Reinforcement Learning"

## ìš”êµ¬ì‚¬í•­:
- ê¸°ì¡´ í‚¤ì›Œë“œì™€ ì¤‘ë³µë˜ì§€ ì•ŠëŠ” ìƒˆë¡œìš´ í‚¤ì›Œë“œ ìƒì„±
- ìµœëŒ€ 5ê°œì˜ í•µì‹¬ í‚¤ì›Œë“œë§Œ ìƒì„±
- ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„í•˜ì—¬ ì¶œë ¥

# êµì • í‚¤ì›Œë“œ:
"""
        
        try:
            response = self.gemini_client.generate_answer(prompt)
            
            # ì‘ë‹µì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords = []
            for line in response.strip().split('\n'):
                line = line.strip()
                if line and not line.startswith('#') and not line.startswith('-') and not line.startswith('##'):
                    clean_keyword = line.replace('*', '').replace('-', '').strip()
                    if clean_keyword and len(clean_keyword) > 1:
                        keywords.append(clean_keyword)
            
            return keywords[:5]  # ìµœëŒ€ 5ê°œ
            
        except Exception as e:
            print(f"   âš ï¸  LLM í‚¤ì›Œë“œ ìƒì„± ì‹¤íŒ¨: {e}")
            # í´ë°±: ê¸°ì¡´ í‚¤ì›Œë“œ í™•ì¥
            return self.extract_more_keywords(query, 8)
    
    def _generate_keywords_with_web_search(self, query: str, issues: str) -> List[str]:
        """
        ì›¹ ê²€ìƒ‰ì„ í™œìš©í•œ í‚¤ì›Œë“œ ê°œì„  (í–¥í›„ êµ¬í˜„)
        """
        # í˜„ì¬ëŠ” LLM ë°©ì‹ ì‚¬ìš©
        return self._generate_keywords_with_llm(query, issues)
    
    def _extract_special_terms(self, query: str) -> List[str]:
        """
        ì¼ë°˜í™”ëœ ì „ë¬¸ ìš©ì–´ ë° ì•½ì–´ ì¶”ì¶œ
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            
        Returns:
            ì¶”ì¶œëœ ì „ë¬¸ ìš©ì–´ ë¦¬ìŠ¤íŠ¸
        """
        special_terms = []
        
        # 1. ëŒ€ë¬¸ì ì•½ì–´ ì¶”ì¶œ (DTG, SBiFEM, CNN, LSTM, SVM ë“±)
        acronyms = re.findall(r'\b[A-Z]{2,}\b', query)
        special_terms.extend(acronyms)
        
        # 2. ë³µí•© ëª…ì‚¬êµ¬ ì¶”ì¶œ (2-4ë‹¨ì–´ ì¡°í•©)
        words = query.split()
        for i in range(len(words) - 1):
            # 2ë‹¨ì–´ ì¡°í•©
            bigram = f"{words[i]} {words[i+1]}"
            if len(bigram) > 5 and not any(word in ['the', 'and', 'or', 'in', 'of', 'to', 'for'] for word in bigram.split()):
                special_terms.append(bigram)
            
            # 3ë‹¨ì–´ ì¡°í•© (ê°€ëŠ¥í•œ ê²½ìš°)
            if i < len(words) - 2:
                trigram = f"{words[i]} {words[i+1]} {words[i+2]}"
                if len(trigram) > 8 and not any(word in ['the', 'and', 'or', 'in', 'of', 'to', 'for'] for word in trigram.split()):
                    special_terms.append(trigram)
        
        # 3. íŠ¹ìˆ˜ íŒ¨í„´ ì¶”ì¶œ (ì˜ˆ: "X ê¸°ë°˜ Y", "Xë¥¼ í™œìš©í•œ Y" ë“±)
        patterns = [
            r'(\w+)\s+ê¸°ë°˜\s+(\w+)',
            r'(\w+)\s+í™œìš©\s+(\w+)',
            r'(\w+)\s+ëª¨ë¸\s+(\w+)',
            r'(\w+)\s+ì‹œìŠ¤í…œ\s+(\w+)',
            r'(\w+)\s+ë¶„ì„\s+(\w+)'
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, query)
            for match in matches:
                if isinstance(match, tuple):
                    combined = ' '.join(match)
                    if len(combined) > 3:
                        special_terms.append(combined)
                else:
                    special_terms.append(match)
        
        return list(set(special_terms))  # ì¤‘ë³µ ì œê±°
    
    def _split_long_keywords(self, keywords: List[str]) -> List[str]:
        """
        ê¸´ í‚¤ì›Œë“œë¥¼ ìª¼ê°œì„œ ê²€ìƒ‰ ê°€ëŠ¥í•œ ë‹¨ìœ„ë¡œ ë¶„í•  (ì¼ë°˜í™”ëœ ë°©ì‹)
        
        Args:
            keywords: ì›ë³¸ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ë¶„í• ëœ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        """
        split_keywords = []
        
        for keyword in keywords:
            # ì´ë¯¸ ì§§ì€ í‚¤ì›Œë“œëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
            if len(keyword) <= 8:
                split_keywords.append(keyword)
                continue
            
            # ê¸´ í‚¤ì›Œë“œ ë¶„í• 
            if ' ' in keyword:
                # ê³µë°±ì´ ìˆëŠ” ê²½ìš° ë‹¨ì–´ë³„ë¡œ ë¶„í• 
                words = keyword.split()
                split_keywords.extend(words)
                
                # 2-3ë‹¨ì–´ ì¡°í•©ë„ ì¶”ê°€ (ê²€ìƒ‰ ë²”ìœ„ í™•ì¥)
                if len(words) >= 2:
                    for i in range(len(words) - 1):
                        split_keywords.append(f"{words[i]} {words[i+1]}")
                    
                    # 3ë‹¨ì–´ ì¡°í•©ë„ ì¶”ê°€ (ê°€ëŠ¥í•œ ê²½ìš°)
                    if len(words) >= 3:
                        for i in range(len(words) - 2):
                            split_keywords.append(f"{words[i]} {words[i+1]} {words[i+2]}")
            else:
                # ë‹¨ì¼ ê¸´ ë‹¨ì–´ì¸ ê²½ìš°
                if len(keyword) > 12:
                    # ì¼ë°˜ì ì¸ ì˜ì–´ ë‹¨ì–´ ë¶„í•  ê·œì¹™ ì ìš©
                    # 1. camelCase ë¶„í• 
                    if re.match(r'^[a-z]+[A-Z]', keyword):
                        # camelCaseë¥¼ ë‹¨ì–´ë¡œ ë¶„í• 
                        words = re.findall(r'[A-Z]?[a-z]+', keyword)
                        split_keywords.extend(words)
                        # ì›ë³¸ë„ ìœ ì§€
                        split_keywords.append(keyword)
                    # 2. snake_case ë¶„í• 
                    elif '_' in keyword:
                        words = keyword.split('_')
                        split_keywords.extend(words)
                        # ì›ë³¸ë„ ìœ ì§€
                        split_keywords.append(keyword)
                    # 3. ì¼ë°˜ì ì¸ ê¸´ ë‹¨ì–´ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
                    else:
                        split_keywords.append(keyword)
                else:
                    split_keywords.append(keyword)
        
        # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
        return list(set(split_keywords))
    
    def _split_keywords_by_length(self, keywords: List[str]) -> List[str]:
        """
        í‚¤ì›Œë“œë¥¼ ìŠ¤ë§ˆíŠ¸í•˜ê²Œ ë¶„í• í•˜ì—¬ ê²€ìƒ‰ ë²”ìœ„ í™•ì¥ (ê°œì„ ëœ ë°©ì‹)
        
        Args:
            keywords: ì›ë³¸ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ë¶„í• ëœ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        """
        split_keywords = []
        
        # ë¶ˆìš©ì–´ ì •ì˜ (ë¶„í• í•˜ì§€ ì•Šì„ ì¼ë°˜ì ì¸ ë‹¨ì–´ë“¤)
        stop_words = {
            'the', 'and', 'or', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by',
            'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been', 'being',
            'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should',
            'this', 'that', 'these', 'those', 'it', 'its', 'they', 'them', 'their',
            'ìµœì í™”', 'ë¶„ì„', 'ì—°êµ¬', 'ë°©ë²•', 'ê¸°ë²•', 'ì‹œìŠ¤í…œ', 'ëª¨ë¸', 'ë°ì´í„°', 'ì •ë³´'
        }
        
        for keyword in keywords:
            # ì›ë³¸ í‚¤ì›Œë“œ ì¶”ê°€
            split_keywords.append(keyword)
            
            # ê³µë°±ì´ ìˆëŠ” ë³µí•© í‚¤ì›Œë“œ ì²˜ë¦¬
            if ' ' in keyword:
                words = keyword.split()
                
                # ì˜ë¯¸ìˆëŠ” ë‹¨ì–´ë§Œ í•„í„°ë§
                meaningful_words = [word for word in words if word.lower() not in stop_words and len(word) > 2]
                
                # ê°œë³„ ì˜ë¯¸ìˆëŠ” ë‹¨ì–´ ì¶”ê°€
                split_keywords.extend(meaningful_words)
                
                # 2-3ë‹¨ì–´ ì¡°í•©ë§Œ ì¶”ê°€ (ë„ˆë¬´ ë§ì€ ì¡°í•© ë°©ì§€)
                if len(meaningful_words) >= 2:
                    # 2ë‹¨ì–´ ì¡°í•©
                    for i in range(len(meaningful_words) - 1):
                        phrase = f"{meaningful_words[i]} {meaningful_words[i+1]}"
                        split_keywords.append(phrase)
                    
                    # 3ë‹¨ì–´ ì¡°í•© (ê°€ëŠ¥í•œ ê²½ìš°)
                    if len(meaningful_words) >= 3:
                        for i in range(len(meaningful_words) - 2):
                            phrase = f"{meaningful_words[i]} {meaningful_words[i+1]} {meaningful_words[i+2]}"
                            split_keywords.append(phrase)
            
            # ë‹¨ì¼ ê¸´ ë‹¨ì–´ ì²˜ë¦¬
            elif len(keyword) > 8:
                # camelCase ë¶„í• 
                if re.match(r'^[a-z]+[A-Z]', keyword):
                    words = re.findall(r'[A-Z]?[a-z]+', keyword)
                    meaningful_words = [word for word in words if word.lower() not in stop_words]
                    split_keywords.extend(meaningful_words)
                
                # snake_case ë¶„í• 
                elif '_' in keyword:
                    words = keyword.split('_')
                    meaningful_words = [word for word in words if word.lower() not in stop_words]
                    split_keywords.extend(meaningful_words)
        
        # ì¤‘ë³µ ì œê±° ë° ê¸¸ì´ë³„ ì •ë ¬ (ê¸´ í‚¤ì›Œë“œ ìš°ì„ )
        unique_keywords = list(set(split_keywords))
        unique_keywords.sort(key=len, reverse=True)
        
        return unique_keywords
    
    def _generate_alternative_keywords(self, query: str, existing_keywords: List[str]) -> List[str]:
        """
        ê¸°ì¡´ í‚¤ì›Œë“œì™€ ë‹¤ë¥¸ ëŒ€ì•ˆ í‚¤ì›Œë“œ ìƒì„±
        
        Args:
            query: ì›ë³¸ ì§ˆë¬¸
            existing_keywords: ê¸°ì¡´ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ëŒ€ì•ˆ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        """
        alternative_keywords = []
        
        # ë™ì˜ì–´ ì‚¬ì „ (ê°„ë‹¨í•œ ë²„ì „)
        synonyms = {
            'machine learning': ['ë”¥ëŸ¬ë‹', 'ì¸ê³µì§€ëŠ¥', 'AI', 'Neural Network', 'ê°•í™”í•™ìŠµ'],
            'deep learning': ['ë¨¸ì‹ ëŸ¬ë‹', 'ì¸ê³µì§€ëŠ¥', 'AI', 'Neural Network', 'ë”¥ëŸ¬ë‹'],
            'artificial intelligence': ['AI', 'ë¨¸ì‹ ëŸ¬ë‹', 'ë”¥ëŸ¬ë‹', 'ì¸ê³µì§€ëŠ¥'],
            'crowdsourcing': ['í¬ë¼ìš°ë“œì†Œì‹±', 'Human Computation', 'Distributed Computing'],
            'í¬ë¼ìš°ë“œì†Œì‹±': ['Crowdsourcing', 'Human Computation', 'Distributed Computing'],
            'warehouse': ['ì°½ê³ ', 'ë¬¼ë¥˜', 'Logistics', 'Supply Chain'],
            'management': ['ê´€ë¦¬', 'ìš´ì˜', 'Administration', 'Control'],
            'system': ['ì‹œìŠ¤í…œ', 'ì²´ê³„', 'Framework', 'Architecture'],
            'data': ['ë°ì´í„°', 'ì •ë³´', 'Information', 'Dataset'],
            'analysis': ['ë¶„ì„', 'Analytics', 'Processing', 'Evaluation'],
            'model': ['ëª¨ë¸', 'Model', 'Algorithm', 'Method'],
            'optimization': ['ìµœì í™”', 'Optimization', 'Improvement', 'Enhancement'],
            'ìµœì í™”': ['Optimization', 'Improvement', 'Enhancement', 'Efficiency'],
            'pomdp': ['Partially Observable Markov Decision Process', 'ê°•í™”í•™ìŠµ', 'Reinforcement Learning'],
            'reinforcement learning': ['ê°•í™”í•™ìŠµ', 'POMDP', 'Q-Learning', 'Policy Gradient'],
            'ê°•í™”í•™ìŠµ': ['Reinforcement Learning', 'POMDP', 'Q-Learning', 'Policy Gradient']
        }
        
        # ê¸°ì¡´ í‚¤ì›Œë“œì— ëŒ€í•œ ë™ì˜ì–´ ì°¾ê¸°
        for keyword in existing_keywords:
            keyword_lower = keyword.lower()
            for key, values in synonyms.items():
                if key in keyword_lower or keyword_lower in key:
                    for synonym in values:
                        if synonym not in existing_keywords and synonym not in alternative_keywords:
                            alternative_keywords.append(synonym)
        
        # ë„ë©”ì¸ë³„ ì¼ë°˜ í‚¤ì›Œë“œ ì¶”ê°€
        if any(term in query.lower() for term in ['system', 'model', 'algorithm']):
            alternative_keywords.extend(['Framework', 'Architecture', 'Methodology'])
        if any(term in query.lower() for term in ['data', 'analysis', 'processing']):
            alternative_keywords.extend(['Analytics', 'Processing', 'Evaluation'])
        if any(term in query.lower() for term in ['learning', 'training']):
            alternative_keywords.extend(['Training', 'Education', 'Development'])
        
        return alternative_keywords[:5]  # ìµœëŒ€ 5ê°œ
    
    def _extract_basic_keywords(self, query: str) -> List[str]:
        """
        ê¸°ë³¸ í‚¤ì›Œë“œ ì¶”ì¶œ (ë¹ ë¥¸ ë³´ì¶©ìš©)
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            
        Returns:
            ê¸°ë³¸ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        """
        basic_keywords = []
        
        # 1. ëª…ì‚¬ ì¶”ì¶œ
        if self._is_korean(query):
            nouns = self.okt.nouns(query)
            basic_keywords.extend([noun for noun in nouns if len(noun) > 1])
        else:
            # ì˜ì–´: ê¸°ë³¸ ë‹¨ì–´ ì¶”ì¶œ
            words = re.findall(r'\b\w+\b', query.lower())
            basic_keywords.extend([word for word in words if len(word) > 2])
        
        # 2. ì•½ì–´ ì¶”ì¶œ
        acronyms = re.findall(r'\b[A-Z]{2,}\b', query)
        basic_keywords.extend(acronyms)
        
        # 3. ì¤‘ë³µ ì œê±° ë° ì •ë ¬
        basic_keywords = list(set(basic_keywords))
        basic_keywords.sort(key=len, reverse=True)
        
        return basic_keywords[:5]  # ìµœëŒ€ 5ê°œ
    
    def _extract_general_keywords(self, query: str) -> List[str]:
        """
        ì¼ë°˜ì ì¸ í‚¤ì›Œë“œ ì¶”ì¶œ (ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ í´ë°±ìš©)
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            
        Returns:
            ì¼ë°˜ì ì¸ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        """
        general_keywords = []
        
        # 1. ê¸°ë³¸ ëª…ì‚¬ ì¶”ì¶œ
        if self._is_korean(query):
            nouns = self.okt.nouns(query)
            general_keywords.extend([noun for noun in nouns if len(noun) > 1])
        else:
            # ì˜ì–´: ê¸°ë³¸ ë‹¨ì–´ ì¶”ì¶œ
            words = re.findall(r'\b\w+\b', query.lower())
            general_keywords.extend([word for word in words if len(word) > 2])
        
        # 2. ë„ë©”ì¸ë³„ ì¼ë°˜ í‚¤ì›Œë“œ ì¶”ê°€
        domain_keywords = self._get_domain_keywords(query)
        general_keywords.extend(domain_keywords)
        
        # 3. ì¤‘ë³µ ì œê±° ë° ì •ë ¬
        general_keywords = list(set(general_keywords))
        general_keywords.sort(key=len, reverse=True)
        
        return general_keywords[:10]
    
    def _get_domain_keywords(self, query: str) -> List[str]:
        """
        ì§ˆë¬¸ ë„ë©”ì¸ì— ë”°ë¥¸ ì¼ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            
        Returns:
            ë„ë©”ì¸ë³„ ì¼ë°˜ í‚¤ì›Œë“œ
        """
        query_lower = query.lower()
        domain_keywords = []
        
        # ê¸°ìˆ /ê³µí•™ ë„ë©”ì¸
        if any(term in query_lower for term in ['system', 'model', 'algorithm', 'method', 'approach']):
            domain_keywords.extend(['system', 'model', 'algorithm', 'method', 'approach', 'technique'])
        
        # ë°ì´í„°/ë¶„ì„ ë„ë©”ì¸
        if any(term in query_lower for term in ['data', 'analysis', 'processing', 'mining', 'big data']):
            domain_keywords.extend(['data', 'analysis', 'processing', 'mining', 'big data', 'analytics'])
        
        # AI/ML ë„ë©”ì¸
        if any(term in query_lower for term in ['ai', 'machine learning', 'deep learning', 'neural']):
            domain_keywords.extend(['artificial intelligence', 'machine learning', 'deep learning', 'neural network'])
        
        # ì˜ë£Œ/ìƒëª… ë„ë©”ì¸
        if any(term in query_lower for term in ['medical', 'health', 'disease', 'diagnosis', 'treatment']):
            domain_keywords.extend(['medical', 'health', 'disease', 'diagnosis', 'treatment', 'clinical'])
        
        # ë¹„ì¦ˆë‹ˆìŠ¤/ê²½ì˜ ë„ë©”ì¸
        if any(term in query_lower for term in ['management', 'business', 'corporate', 'organization']):
            domain_keywords.extend(['management', 'business', 'corporate', 'organization', 'strategy'])
        
        return domain_keywords

"""
ê²€ìƒ‰ ëª¨ë“ˆ
- í‚¤ì›Œë“œ ì¶”ì¶œ
- ë¬¸ì„œ ê²€ìƒ‰
- ì¬ì‹œë„ ë¡œì§
- ê²€ìƒ‰ ê²°ê³¼ ë³´ì¶©
"""

import time
import re
from typing import List, Dict
from konlpy.tag import Okt
from .config import SEARCH_CONFIG, TEST_CONFIG

class DocumentRetriever:
    """ë¬¸ì„œ ê²€ìƒ‰ê¸°"""
    
    def __init__(self, api_client):
        """
        ê²€ìƒ‰ê¸° ì´ˆê¸°í™”
        
        Args:
            api_client: ScienceON API í´ë¼ì´ì–¸íŠ¸
        """
        self.api_client = api_client
        self.okt = Okt()
    
    def extract_keywords(self, query: str) -> List[str]:
        """
        ì¿¼ë¦¬ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            
        Returns:
            ì¶”ì¶œëœ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        """
        if self._is_korean(query):
            # í•œêµ­ì–´: ëª…ì‚¬ ì¶”ì¶œ
            nouns = self.okt.nouns(query)
            keywords = [noun for noun in nouns if len(noun) > 1]
            return keywords[:5]
        else:
            # ì˜ì–´: ê°œì„ ëœ ë¶ˆìš©ì–´ ì œê±°
            stop_words = {
                # ê¸°ë³¸ ë¶ˆìš©ì–´
                'a', 'an', 'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by',
                # ì§ˆë¬¸ì–´ ì œê±°
                'how', 'what', 'why', 'when', 'where', 'which', 'who', 'whom', 'whose',
                # ì¼ë°˜ì ì¸ ë™ì‚¬ ì œê±°
                'can', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might',
                'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had',
                # ê¸°íƒ€ ì¼ë°˜ì ì¸ ë‹¨ì–´ë“¤
                'this', 'that', 'these', 'those', 'it', 'its', 'they', 'them', 'their',
                'we', 'you', 'he', 'she', 'his', 'her', 'our', 'your', 'my', 'me', 'i'
            }
            
            words = re.findall(r'\w+', query.lower())
            
            # 1ë‹¨ê³„: ë¶ˆìš©ì–´ ì œê±°
            filtered_words = [word for word in words if word not in stop_words and len(word) > 2]
            
            # 2ë‹¨ê³„: ì „ë¬¸ ìš©ì–´ ìš°ì„  ì„ íƒ
            technical_terms = []
            general_terms = []
            
            for word in filtered_words:
                # ì „ë¬¸ ìš©ì–´ íŒë³„ (ê¸¸ì´ê°€ ê¸¸ê±°ë‚˜ íŠ¹ì • íŒ¨í„´)
                if (len(word) > 6 or 
                    word in ['neural', 'artificial', 'machine', 'learning', 'deep', 'network', 
                            'algorithm', 'model', 'system', 'method', 'approach', 'technique',
                            'sustainability', 'corporate', 'culture', 'development', 'management',
                            'analysis', 'research', 'study', 'framework', 'architecture']):
                    technical_terms.append(word)
                else:
                    general_terms.append(word)
            
            # ì „ë¬¸ ìš©ì–´ë¥¼ ë¨¼ì €, ê·¸ ë‹¤ìŒ ì¼ë°˜ ìš©ì–´
            keywords = technical_terms + general_terms
            return keywords[:8]  # ë” ë§ì€ í‚¤ì›Œë“œ ì¶”ì¶œ
    
    def extract_more_keywords(self, query: str, max_keywords: int = 10) -> List[str]:
        """
        ë” ë§ì€ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            max_keywords: ìµœëŒ€ í‚¤ì›Œë“œ ìˆ˜
            
        Returns:
            í™•ì¥ëœ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        """
        keywords = self.extract_keywords(query)
        
        # ê¸°ë³¸ í‚¤ì›Œë“œê°€ ë¶€ì¡±í•˜ë©´ ì¿¼ë¦¬ì—ì„œ ì¶”ê°€ í‚¤ì›Œë“œ ì¶”ì¶œ
        if len(keywords) < 3:
            words = query.split()
            for word in words:
                if len(word) > 2 and word not in keywords:
                    keywords.append(word)
                    if len(keywords) >= max_keywords:
                        break
        
        # ìœ ì‚¬í•œ í‚¤ì›Œë“œ ì¶”ê°€ (ê°„ë‹¨í•œ í™•ì¥)
        expanded_keywords = keywords.copy()
        for keyword in keywords:
            if keyword.isascii():
                if keyword.endswith('s'):
                    expanded_keywords.append(keyword[:-1])  # ë³µìˆ˜í˜• -> ë‹¨ìˆ˜í˜•
                if keyword.endswith('ing'):
                    expanded_keywords.append(keyword[:-3])  # ~ing -> ê¸°ë³¸í˜•
                if keyword.endswith('ed'):
                    expanded_keywords.append(keyword[:-2])  # ~ed -> ê¸°ë³¸í˜•
        
        return list(set(expanded_keywords))[:max_keywords]
    
    def search_with_retry(self, query: str, max_retries: int = SEARCH_CONFIG['max_retries'], 
                         min_docs: int = SEARCH_CONFIG['min_docs']) -> List[Dict]:
        """
        ì¬ì‹œë„ ë¡œì§ì´ í¬í•¨ëœ ë¬¸ì„œ ê²€ìƒ‰ (50ê°œ ë¬¸ì„œ ë³´ì¥)
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
            min_docs: ìµœì†Œ í•„ìš” ë¬¸ì„œ ìˆ˜ (50ê°œ)
            
        Returns:
            ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ (ìµœì†Œ 50ê°œ ë³´ì¥)
        """
        all_docs = []
        keywords = self.extract_more_keywords(query)
        
        print(f"   ğŸ” ì¶”ì¶œëœ í‚¤ì›Œë“œ: {', '.join(keywords[:10])}")
        
        # ë” ì ê·¹ì ì¸ í‚¤ì›Œë“œ í™•ì¥
        expanded_keywords = self._expand_keywords_aggressively(query, keywords)
        
        for attempt in range(max_retries * 2):  # ë” ë§ì€ ì‹œë„
            if len(all_docs) >= min_docs:
                break
                
            print(f"   - ê²€ìƒ‰ ì‹œë„ {attempt + 1}/{max_retries * 2} (í˜„ì¬ {len(all_docs)}ê°œ ë¬¸ì„œ)")
            
            # ì´ë²ˆ ì‹œë„ì—ì„œ ì‚¬ìš©í•  í‚¤ì›Œë“œë“¤
            current_keywords = self._get_keywords_for_attempt_aggressive(expanded_keywords, attempt)
            
            # ë””ë²„ê·¸ ëª¨ë“œì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ ê³¼ì • í‘œì‹œ
            if TEST_CONFIG.get('debug_mode', False):
                print(f"   ğŸ” ì‹œë„ {attempt + 1} í‚¤ì›Œë“œ: {', '.join(current_keywords)}")
                if attempt > 0:
                    print(f"   ğŸ”„ í‚¤ì›Œë“œ í™•ì¥: {len(expanded_keywords)}ê°œ â†’ {len(current_keywords)}ê°œ")
            
            # í‚¤ì›Œë“œë³„ ê²€ìƒ‰ (ë” ë§ì€ ê²°ê³¼ ìš”ì²­)
            for keyword in current_keywords:
                try:
                    docs = self.api_client.search_articles(
                        keyword, 
                        row_count=50,  # ë” ë§ì€ ê²°ê³¼ ìš”ì²­
                        fields=['title', 'abstract', 'CN']
                    )
                    all_docs.extend(docs)
                    time.sleep(SEARCH_CONFIG['api_delay'])
                    
                    # ì¶©ë¶„í•œ ë¬¸ì„œê°€ ìˆìœ¼ë©´ ì¤‘ë‹¨
                    if len(all_docs) >= min_docs * 2:  # ì—¬ìœ ë¶„ í™•ë³´
                        break
                        
                except Exception as e:
                    print(f"   âš ï¸  í‚¤ì›Œë“œ '{keyword}' ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                    continue
            
            # ì¤‘ë³µ ì œê±°
            all_docs = self._remove_duplicates(all_docs)
            
            if len(all_docs) >= min_docs:
                print(f"   âœ… ëª©í‘œ ë¬¸ì„œ ìˆ˜ ë‹¬ì„±: {len(all_docs)}ê°œ")
                break
            else:
                print(f"   âš ï¸  ë¬¸ì„œ ìˆ˜ ë¶€ì¡±: {len(all_docs)}ê°œ (ëª©í‘œ: {min_docs}ê°œ)")
        
        # ìµœì¢…ì ìœ¼ë¡œ 50ê°œ ë¯¸ë§Œì´ë©´ ì¶”ê°€ ê²€ìƒ‰
        if len(all_docs) < min_docs:
            print(f"   ğŸš¨ ë¬¸ì„œ ìˆ˜ ë¶€ì¡±! ì¶”ê°€ ê²€ìƒ‰ ì‹œë„...")
            additional_docs = self._emergency_search(query, min_docs - len(all_docs))
            all_docs.extend(additional_docs)
            all_docs = self._remove_duplicates(all_docs)
        
        print(f"   ğŸ“Š ìµœì¢… ê²€ìƒ‰ ê²°ê³¼: {len(all_docs)}ê°œ ë¬¸ì„œ")
        return all_docs[:min_docs]  # ì •í™•íˆ 50ê°œ ë°˜í™˜
    
    def _expand_keywords_aggressively(self, query: str, base_keywords: List[str]) -> List[str]:
        """
        ë” ì ê·¹ì ì¸ í‚¤ì›Œë“œ í™•ì¥ (50ê°œ ë¬¸ì„œ ë³´ì¥ì„ ìœ„í•´)
        """
        expanded = base_keywords.copy()
        
        # 1. ì¿¼ë¦¬ì—ì„œ ë‹¨ì–´ ë¶„ë¦¬
        words = query.replace('?', '').replace('.', '').split()
        for word in words:
            if len(word) > 2 and word not in expanded:
                expanded.append(word)
        
        # 2. ê´€ë ¨ ìš©ì–´ ì¶”ê°€
        related_terms = self._get_related_terms(query)
        expanded.extend(related_terms)
        
        # 3. ì¼ë°˜ì ì¸ í•™ìˆ  ìš©ì–´ ì¶”ê°€
        academic_terms = ['ì—°êµ¬', 'ë¶„ì„', 'ë°©ë²•', 'ê²°ê³¼', 'ì‹œìŠ¤í…œ', 'ê¸°ìˆ ', 'ê°œë°œ', 'í‰ê°€', 'ê´€ë¦¬', 'ìµœì í™”']
        for term in academic_terms:
            if term not in expanded:
                expanded.append(term)
        
        return list(set(expanded))  # ì¤‘ë³µ ì œê±°
    
    def _get_related_terms(self, query: str) -> List[str]:
        """
        ì¿¼ë¦¬ì™€ ê´€ë ¨ëœ ìš©ì–´ë“¤ ì¶”ì¶œ
        """
        related = []
        
        # ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ ê´€ë ¨ ìš©ì–´ ë§¤í•‘
        term_mapping = {
            'ì¸ê³µì§€ëŠ¥': ['AI', 'ë¨¸ì‹ ëŸ¬ë‹', 'ë”¥ëŸ¬ë‹', 'ì•Œê³ ë¦¬ì¦˜'],
            'ë¨¸ì‹ ëŸ¬ë‹': ['ML', 'ì¸ê³µì§€ëŠ¥', 'ë°ì´í„°', 'ëª¨ë¸'],
            'ë°ì´í„°': ['ë¶„ì„', 'ì²˜ë¦¬', 'ë§ˆì´ë‹', 'ë² ì´ìŠ¤'],
            'ì‹œìŠ¤í…œ': ['êµ¬í˜„', 'ì„¤ê³„', 'ì•„í‚¤í…ì²˜', 'í”Œë«í¼'],
            'ë³´ì•ˆ': ['ì•”í˜¸í™”', 'ì¸ì¦', 'ê¶Œí•œ', 'í”„ë¡œí† ì½œ'],
            'ë„¤íŠ¸ì›Œí¬': ['í†µì‹ ', 'í”„ë¡œí† ì½œ', 'ë¼ìš°íŒ…', 'ë³´ì•ˆ'],
            'ì›¹': ['ì¸í„°ë„·', 'ë¸Œë¼ìš°ì €', 'ì„œë²„', 'í´ë¼ì´ì–¸íŠ¸'],
            'ëª¨ë°”ì¼': ['ìŠ¤ë§ˆíŠ¸í°', 'ì•±', 'ì•ˆë“œë¡œì´ë“œ', 'iOS'],
            'í´ë¼ìš°ë“œ': ['ì„œë²„', 'ê°€ìƒí™”', 'ìŠ¤ì¼€ì¼ë§', 'ë°°í¬']
        }
        
        for key, terms in term_mapping.items():
            if key in query:
                related.extend(terms)
        
        return related
    
    def _get_keywords_for_attempt_aggressive(self, keywords: List[str], attempt: int) -> List[str]:
        """
        ë” ì ê·¹ì ì¸ ì‹œë„ë³„ í‚¤ì›Œë“œ ì„ íƒ
        """
        if attempt == 0:
            return keywords[:15]  # ì²« ì‹œë„: ìƒìœ„ 15ê°œ
        elif attempt == 1:
            return keywords[15:30]  # ë‘ ë²ˆì§¸ ì‹œë„: ë‹¤ìŒ 15ê°œ
        elif attempt == 2:
            return keywords[30:45]  # ì„¸ ë²ˆì§¸ ì‹œë„: ë‹¤ìŒ 15ê°œ
        else:
            # ì¶”ê°€ ì‹œë„: ë‚¨ì€ ëª¨ë“  í‚¤ì›Œë“œ
            start_idx = 45 + (attempt - 3) * 10
            return keywords[start_idx:start_idx + 15]
    
    def _emergency_search(self, query: str, needed_count: int) -> List[Dict]:
        """
        ê¸´ê¸‰ ì¶”ê°€ ê²€ìƒ‰ (50ê°œ ë¬¸ì„œ ë³´ì¥ì„ ìœ„í•´)
        """
        print(f"   ğŸš¨ ê¸´ê¸‰ ê²€ìƒ‰: {needed_count}ê°œ ë¬¸ì„œ ì¶”ê°€ í•„ìš”")
        
        emergency_docs = []
        
        # 1. ì¼ë°˜ì ì¸ í•™ìˆ  í‚¤ì›Œë“œë¡œ ê²€ìƒ‰
        emergency_keywords = ['ì—°êµ¬', 'ë¶„ì„', 'ë°©ë²•', 'ì‹œìŠ¤í…œ', 'ê¸°ìˆ ', 'ê°œë°œ']
        
        for keyword in emergency_keywords:
            if len(emergency_docs) >= needed_count:
                break
                
            try:
                docs = self.api_client.search_articles(
                    keyword,
                    row_count=20,
                    fields=['title', 'abstract', 'CN']
                )
                emergency_docs.extend(docs)
                time.sleep(SEARCH_CONFIG['api_delay'])
            except Exception as e:
                print(f"   âš ï¸  ê¸´ê¸‰ ê²€ìƒ‰ í‚¤ì›Œë“œ '{keyword}' ì‹¤íŒ¨: {e}")
                continue
        
        # 2. ì¿¼ë¦¬ì—ì„œ ë‹¨ì–´ í•˜ë‚˜ì”© ê²€ìƒ‰
        words = query.replace('?', '').replace('.', '').split()
        for word in words:
            if len(word) > 2 and len(emergency_docs) < needed_count:
                try:
                    docs = self.api_client.search_articles(
                        word,
                        row_count=10,
                        fields=['title', 'abstract', 'CN']
                    )
                    emergency_docs.extend(docs)
                    time.sleep(SEARCH_CONFIG['api_delay'])
                except Exception as e:
                    continue
        
        print(f"   ğŸ“Š ê¸´ê¸‰ ê²€ìƒ‰ ê²°ê³¼: {len(emergency_docs)}ê°œ ë¬¸ì„œ")
        return emergency_docs
    
    def _get_keywords_for_attempt(self, keywords: List[str], attempt: int) -> List[str]:
        """ì‹œë„ë³„ í‚¤ì›Œë“œ ì„ íƒ (ê¸°ì¡´ ë©”ì„œë“œ ìœ ì§€)"""
        if attempt == 0:
            return keywords[:5]  # ì²« ì‹œë„: ìƒìœ„ 5ê°œ í‚¤ì›Œë“œ
        elif attempt == 1:
            return keywords[5:] + keywords[:3]  # ë‘ ë²ˆì§¸: ë‚˜ë¨¸ì§€ + ìƒìœ„ 3ê°œ
        else:
            # ë§ˆì§€ë§‰ ì‹œë„: ì¿¼ë¦¬ ìì²´ë¥¼ í‚¤ì›Œë“œë¡œ ì‚¬ìš©
            return [keywords[0][:20], keywords[0][20:40]] if len(keywords[0]) > 20 else [keywords[0]]
    
    def _remove_duplicates(self, documents: List[Dict]) -> List[Dict]:
        """ì¤‘ë³µ ë¬¸ì„œ ì œê±° ë° í’ˆì§ˆ í•„í„°ë§"""
        # ì¤‘ë³µ ì œê±°
        unique_docs = list({doc['CN']: doc for doc in documents if 'CN' in doc}.values())
        
        # í’ˆì§ˆ í•„í„°ë§
        filtered_docs = []
        for doc in unique_docs:
            title = doc.get('title', '').lower()
            abstract = doc.get('abstract', '')
            
            # ì œì™¸í•  íŒ¨í„´ë“¤
            exclude_patterns = [
                # "How"ë¡œ ì‹œì‘í•˜ëŠ” ì¼ë°˜ì ì¸ ì§ˆë¬¸ ì œëª©ë“¤
                title.startswith('how '),
                title.startswith('what '),
                title.startswith('why '),
                title.startswith('when '),
                title.startswith('where '),
                title.startswith('which '),
                title.startswith('who '),
                
                # ë„ˆë¬´ ì§§ì€ ì œëª©
                len(title) < 10,
                
                # ì´ˆë¡ì´ ì—†ëŠ” ê²½ìš°
                not abstract or len(abstract) < 20,
                
                # íŠ¹ì • ë¬´ê´€í•œ í‚¤ì›Œë“œê°€ ì œëª©ì— í¬í•¨ëœ ê²½ìš°
                any(word in title for word in ['economics', 'language', 'teaching', 'learning', 'education'])
            ]
            
            # ì œì™¸ íŒ¨í„´ì— í•´ë‹¹í•˜ì§€ ì•Šìœ¼ë©´ í¬í•¨
            if not any(exclude_patterns):
                filtered_docs.append(doc)
        
        print(f"   ğŸ” í’ˆì§ˆ í•„í„°ë§: {len(unique_docs)}ê°œ â†’ {len(filtered_docs)}ê°œ")
        return filtered_docs
    
    def _is_korean(self, text: str) -> bool:
        """í•œêµ­ì–´ í…ìŠ¤íŠ¸ ê°ì§€"""
        return bool(re.search('[ê°€-í£]', text))
    
    def supplement_documents(self, vector_docs: List[Dict], original_docs: List[Dict], 
                           target_count: int = SEARCH_CONFIG['min_docs']) -> List[Dict]:
        """
        ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ê°€ ë¶€ì¡±í•  ë•Œ ì›ë³¸ ê²€ìƒ‰ ê²°ê³¼ë¡œ ë³´ì¶©
        
        Args:
            vector_docs: ë²¡í„° ê²€ìƒ‰ ê²°ê³¼
            original_docs: ì›ë³¸ ê²€ìƒ‰ ê²°ê³¼
            target_count: ëª©í‘œ ë¬¸ì„œ ìˆ˜
            
        Returns:
            ë³´ì¶©ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        if len(vector_docs) >= target_count:
            return vector_docs
        
        print(f"   âš ï¸  ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ ë¶€ì¡±: {len(vector_docs)}ê°œ (ëª©í‘œ: {target_count}ê°œ)")
        
        # ì›ë³¸ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì¶”ê°€ ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
        remaining_docs = original_docs[len(vector_docs):target_count] if len(original_docs) > len(vector_docs) else []
        supplemented_docs = vector_docs + remaining_docs
        
        print(f"   âœ… ì›ë³¸ ê²€ìƒ‰ ê²°ê³¼ë¡œ ë³´ì¶©: {len(supplemented_docs)}ê°œ")
        
        return supplemented_docs[:target_count]

"""
ê³ ê¸‰ ì¬ìˆœìœ„í™” ëª¨ë“ˆ (ëŒ€íšŒ í•µì‹¬ ìš”êµ¬ì‚¬í•­)
- ì§ˆë¬¸ê³¼ ë¬¸ì„œ ê°„ì˜ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
- ë‹¤ì¤‘ ê¸°ì¤€ ì¬ìˆœìœ„í™” (TF-IDF, í‚¤ì›Œë“œ ë§¤ì¹­, í’ˆì§ˆ, ì»¨í…ìŠ¤íŠ¸)
- ë‹¤ì–‘ì„± ê¸°ë°˜ í•„í„°ë§
- ë„ë©”ì¸ë³„ ìµœì í™”
"""

from typing import List, Dict, Tuple
import re
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from .config import ANSWER_CONFIG

class DocumentReranker:
    """ê³ ê¸‰ ë¬¸ì„œ ì¬ìˆœìœ„í™”ê¸° (ëŒ€íšŒ í•µì‹¬ ìš”êµ¬ì‚¬í•­)"""
    
    def __init__(self):
        """ì¬ìˆœìœ„í™”ê¸° ì´ˆê¸°í™”"""
        self.tfidf_vectorizer = TfidfVectorizer(
            max_features=1000,
            stop_words='english',
            ngram_range=(1, 2)
        )
    
    def rerank_documents(self, documents: List[Dict], query: str, top_k: int = 50) -> List[Dict]:
        """
        ê³ ê¸‰ ë¬¸ì„œ ì¬ìˆœìœ„í™” (ëŒ€íšŒ í•µì‹¬ ìš”êµ¬ì‚¬í•­)
        
        Args:
            documents: ì›ë³¸ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            top_k: ë°˜í™˜í•  ìƒìœ„ ë¬¸ì„œ ìˆ˜
            
        Returns:
            ì¬ìˆœìœ„í™”ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        if not documents:
            return documents
        
        print(f"   ğŸ”„ ê³ ê¸‰ ë¬¸ì„œ ì¬ìˆœìœ„í™” ì‹œì‘: {len(documents)}ê°œ ë¬¸ì„œ")
        
        # 1. ë‹¤ì¤‘ ê¸°ì¤€ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
        scored_docs = []
        for doc in documents:
            relevance_score = self._calculate_relevance_score(query, doc)
            doc_with_score = doc.copy()
            doc_with_score['_relevance_score'] = relevance_score
            scored_docs.append(doc_with_score)
        
        # 2. ê´€ë ¨ì„± ì ìˆ˜ë¡œ ì •ë ¬
        reranked_docs = sorted(scored_docs, key=lambda x: x['_relevance_score'], reverse=True)
        
        # 3. ë‹¤ì–‘ì„± ê¸°ë°˜ í•„í„°ë§
        diverse_docs = self.filter_by_diversity(reranked_docs[:top_k*2])  # 2ë°°ë¡œ í™•ì¥ í›„ í•„í„°ë§
        
        print(f"   âœ… ê³ ê¸‰ ì¬ìˆœìœ„í™” ì™„ë£Œ: ìƒìœ„ {len(diverse_docs)}ê°œ ì„ íƒ")
        
        return diverse_docs[:top_k]
    
    def _calculate_relevance_score(self, query: str, document: Dict) -> float:
        """
        ì§ˆë¬¸ê³¼ ë¬¸ì„œ ê°„ì˜ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚° (ë‹¤ì¤‘ ê¸°ì¤€)
        
        Args:
            query: ì§ˆë¬¸
            document: ë¬¸ì„œ
            
        Returns:
            ê´€ë ¨ì„± ì ìˆ˜ (0.0 ~ 1.0)
        """
        
        title = document.get('title', '')
        abstract = document.get('abstract', '')
        
        # 1. TF-IDF ê¸°ë°˜ ìœ ì‚¬ë„ (30%)
        tfidf_score = self._calculate_tfidf_similarity(query, title + " " + abstract)
        
        # 2. í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ (25%)
        keyword_score = self._calculate_keyword_matching(query, title, abstract)
        
        # 3. ì œëª© ê´€ë ¨ì„± ì ìˆ˜ (20%)
        title_score = self._calculate_title_relevance(query, title)
        
        # 4. ë¬¸ì„œ í’ˆì§ˆ ì ìˆ˜ (15%)
        quality_score = self._calculate_document_quality(document)
        
        # 5. ì»¨í…ìŠ¤íŠ¸ ì¼ê´€ì„± ì ìˆ˜ (10%)
        context_score = self._calculate_context_consistency(query, abstract)
        
        # ê°€ì¤‘ í‰ê·  ê³„ì‚°
        final_score = (
            tfidf_score * 0.3 +
            keyword_score * 0.25 +
            title_score * 0.2 +
            quality_score * 0.15 +
            context_score * 0.1
        )
        
        return final_score
    
    def _calculate_tfidf_similarity(self, query: str, text: str) -> float:
        """TF-IDF ê¸°ë°˜ ìœ ì‚¬ë„ ê³„ì‚°"""
        try:
            # TF-IDF ë²¡í„°í™”
            tfidf_matrix = self.tfidf_vectorizer.fit_transform([query, text])
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]
            
            return float(similarity)
        except:
            return 0.0
    
    def _calculate_keyword_matching(self, query: str, title: str, abstract: str) -> float:
        """í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°"""
        
        # ì§ˆë¬¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        query_keywords = self._extract_keywords(query)
        
        # ì œëª©ê³¼ ì´ˆë¡ì—ì„œ í‚¤ì›Œë“œ ë§¤ì¹­
        title_matches = sum(1 for keyword in query_keywords if keyword.lower() in title.lower())
        abstract_matches = sum(1 for keyword in query_keywords if keyword.lower() in abstract.lower())
        
        # ê°€ì¤‘ ì ìˆ˜ ê³„ì‚°
        title_score = title_matches / len(query_keywords) if query_keywords else 0
        abstract_score = abstract_matches / len(query_keywords) if query_keywords else 0
        
        # ì œëª© ë§¤ì¹­ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜
        return title_score * 0.7 + abstract_score * 0.3
    
    def _calculate_title_relevance(self, query: str, title: str) -> float:
        """ì œëª© ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°"""
        
        # ì§ˆë¬¸ì˜ í•µì‹¬ ê°œë… ì¶”ì¶œ
        query_concepts = self._extract_concepts(query)
        title_concepts = self._extract_concepts(title)
        
        # ê°œë… ë§¤ì¹­ ê³„ì‚°
        matches = len(set(query_concepts) & set(title_concepts))
        total = len(set(query_concepts) | set(title_concepts))
        
        return matches / total if total > 0 else 0.0
    
    def _extract_keywords(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ
        words = re.findall(r'\b\w+\b', text.lower())
        
        # ë¶ˆìš©ì–´ ì œê±°
        stop_words = {
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by',
            'how', 'what', 'why', 'when', 'where', 'which', 'who',
            'can', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might'
        }
        
        keywords = [word for word in words if word not in stop_words and len(word) > 2]
        
        return keywords[:10]  # ìƒìœ„ 10ê°œë§Œ
    
    def _extract_concepts(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ ê°œë… ì¶”ì¶œ"""
        # ë” ê¸´ ë‹¨ì–´ë“¤ì„ ê°œë…ìœ¼ë¡œ ê°„ì£¼
        words = re.findall(r'\b\w+\b', text.lower())
        concepts = [word for word in words if len(word) > 5]
        
        return concepts[:5]  # ìƒìœ„ 5ê°œë§Œ
    
    def _apply_quality_scores(self, documents: List[Dict], query: str) -> List[Dict]:
        """í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° ë° ì ìš©"""
        for doc in documents:
            quality_score = self._calculate_quality_score(doc, query)
            doc['quality_score'] = quality_score
            
            # ì¢…í•© ì ìˆ˜ ê³„ì‚° (ìœ ì‚¬ë„ 70% + í’ˆì§ˆ 30%)
            similarity_score = doc.get('similarity_score', 0)
            doc['final_score'] = similarity_score * 0.7 + quality_score * 0.3
        
        return documents
    
    def _calculate_document_quality(self, document: Dict) -> float:
        """ë¬¸ì„œ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (ê°œì„ ëœ ë²„ì „)"""
        title = document.get('title', '')
        abstract = document.get('abstract', '')
        
        quality_score = 0.0
        
        # 1. ì œëª© ê¸¸ì´ ì ìˆ˜
        if 10 <= len(title) <= 200:
            quality_score += 0.3
        elif len(title) > 200:
            quality_score += 0.1
        
        # 2. ì´ˆë¡ ê¸¸ì´ ì ìˆ˜
        if 50 <= len(abstract) <= 2000:
            quality_score += 0.4
        elif len(abstract) > 2000:
            quality_score += 0.2
        
        # 3. ì œëª© í’ˆì§ˆ ì ìˆ˜ (ì§ˆë¬¸ì–´ ì œì™¸)
        if not any(title.lower().startswith(word) for word in ['how', 'what', 'why', 'when', 'where']):
            quality_score += 0.3
        
        return min(quality_score, 1.0)
    
    def _calculate_context_consistency(self, query: str, abstract: str) -> float:
        """ì»¨í…ìŠ¤íŠ¸ ì¼ê´€ì„± ì ìˆ˜ ê³„ì‚°"""
        
        # ì§ˆë¬¸ì˜ ë„ë©”ì¸ ì¶”ì •
        query_domain = self._estimate_domain(query)
        abstract_domain = self._estimate_domain(abstract)
        
        # ë„ë©”ì¸ ì¼ì¹˜ë„ ê³„ì‚°
        if query_domain == abstract_domain:
            return 1.0
        elif query_domain and abstract_domain:
            # ë¶€ë¶„ ì¼ì¹˜
            common_terms = set(query_domain.split()) & set(abstract_domain.split())
            return len(common_terms) / max(len(query_domain.split()), len(abstract_domain.split()))
        
        return 0.5  # ê¸°ë³¸ê°’
    
    def _estimate_domain(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ì˜ ë„ë©”ì¸ ì¶”ì •"""
        text_lower = text.lower()
        
        # ë„ë©”ì¸ë³„ í‚¤ì›Œë“œ ë§¤ì¹­
        domains = {
            'computer_science': ['algorithm', 'neural', 'network', 'machine', 'learning', 'artificial', 'intelligence'],
            'mathematics': ['mathematics', 'mathematical', 'equation', 'theorem', 'proof', 'calculation'],
            'medicine': ['medical', 'clinical', 'patient', 'treatment', 'diagnosis', 'disease'],
            'engineering': ['engineering', 'system', 'design', 'technology', 'implementation'],
            'business': ['business', 'management', 'corporate', 'strategy', 'organization'],
            'sustainability': ['sustainability', 'environmental', 'green', 'eco', 'climate']
        }
        
        for domain, keywords in domains.items():
            if any(keyword in text_lower for keyword in keywords):
                return domain
        
        return 'general'
    
    def filter_by_diversity(self, documents: List[Dict], max_similar: float = 0.8) -> List[Dict]:
        """
        ë‹¤ì–‘ì„± ê¸°ë°˜ í•„í„°ë§ (ì¤‘ë³µ ì œê±°)
        
        Args:
            documents: ì¬ìˆœìœ„í™”ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
            max_similar: ìµœëŒ€ ìœ ì‚¬ë„ ì„ê³„ê°’
            
        Returns:
            ë‹¤ì–‘ì„±ì´ ë³´ì¥ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        
        if not documents:
            return []
        
        diverse_docs = [documents[0]]  # ì²« ë²ˆì§¸ ë¬¸ì„œëŠ” í•­ìƒ í¬í•¨
        
        for doc in documents[1:]:
            # ê¸°ì¡´ ë¬¸ì„œë“¤ê³¼ì˜ ìœ ì‚¬ë„ ê³„ì‚°
            max_similarity = 0.0
            
            for existing_doc in diverse_docs:
                similarity = self._calculate_document_similarity(doc, existing_doc)
                max_similarity = max(max_similarity, similarity)
            
            # ì„ê³„ê°’ë³´ë‹¤ ë‚®ìœ¼ë©´ ì¶”ê°€
            if max_similarity < max_similar:
                diverse_docs.append(doc)
        
        print(f"   ğŸŒˆ ë‹¤ì–‘ì„± í•„í„°ë§: {len(documents)}ê°œ â†’ {len(diverse_docs)}ê°œ")
        
        return diverse_docs
    
    def _calculate_document_similarity(self, doc1: Dict, doc2: Dict) -> float:
        """ë‘ ë¬¸ì„œ ê°„ì˜ ìœ ì‚¬ë„ ê³„ì‚°"""
        
        text1 = doc1.get('title', '') + ' ' + doc1.get('abstract', '')
        text2 = doc2.get('title', '') + ' ' + doc2.get('abstract', '')
        
        return self._calculate_tfidf_similarity(text1, text2)
    
    def _final_ranking(self, documents: List[Dict]) -> List[Dict]:
        """ìµœì¢… ìˆœìœ„ ì¡°ì •"""
        # ì¢…í•© ì ìˆ˜ë¡œ ì •ë ¬
        documents = sorted(documents, key=lambda x: x.get('final_score', 0), reverse=True)
        
        # ìˆœìœ„ ì—…ë°ì´íŠ¸
        for i, doc in enumerate(documents):
            doc['final_rank'] = i + 1
        
        return documents
    
    def filter_by_quality(self, documents: List[Dict], min_quality: float = 0.3) -> List[Dict]:
        """
        í’ˆì§ˆ ê¸°ì¤€ìœ¼ë¡œ í•„í„°ë§
        
        Args:
            documents: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
            min_quality: ìµœì†Œ í’ˆì§ˆ ì ìˆ˜
            
        Returns:
            í•„í„°ë§ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        return [doc for doc in documents if doc.get('quality_score', 0) >= min_quality]
    
    def get_top_documents(self, documents: List[Dict], top_k: int = ANSWER_CONFIG['max_context_docs']) -> List[Dict]:
        """
        ìƒìœ„ kê°œ ë¬¸ì„œ ì„ íƒ
        
        Args:
            documents: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
            top_k: ì„ íƒí•  ë¬¸ì„œ ìˆ˜
            
        Returns:
            ìƒìœ„ kê°œ ë¬¸ì„œ
        """
        return documents[:top_k]
    
    def create_context_from_documents(self, documents: List[Dict]) -> str:
        """
        ë¬¸ì„œë“¤ë¡œë¶€í„° ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        
        Args:
            documents: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ìƒì„±ëœ ì»¨í…ìŠ¤íŠ¸
        """
        if not documents:
            return ""
        
        context_parts = []
        for i, doc in enumerate(documents):
            title = doc.get('title', '')
            abstract = doc.get('abstract', '')
            similarity = doc.get('similarity_score', 0)
            
            context = f"[ë¬¸ì„œ {i+1}] (ìœ ì‚¬ë„: {similarity:.3f})\n"
            context += f"ì œëª©: {title}\n"
            context += f"ì´ˆë¡: {abstract}\n"
            context_parts.append(context)
        
        return "\n".join(context_parts)

#!/usr/bin/env python3
"""
Kaggle ì œì¶œìš© ëª¨ë“ˆí™” RAG íŒŒì´í”„ë¼ì¸ v2.0
- ëª¨ë“ˆí™”ëœ êµ¬ì¡°ë¡œ ì‰¬ìš´ ìœ ì§€ë³´ìˆ˜
- ê° ê¸°ëŠ¥ë³„ ë…ë¦½ì ì¸ ëª¨ë“ˆ
- ê³ í’ˆì§ˆ í”„ë¡¬í”„íŠ¸ ë° ë‹µë³€ ë³´ì¥
- null ê°’ ì™„ì „ ì œê±°
"""

import os
import sys
import time
import pandas as pd
from pathlib import Path
from tqdm import tqdm

def create_submission_documentation(md_filepath, pipeline_type, pipeline_stats, total_time, question_count):
    """ì œì¶œ íŒŒì¼ì— ëŒ€í•œ ìƒì„¸í•œ MD ë¬¸ì„œ ìƒì„±"""
    
    pipeline_info = {
        'modular_v2': {
            'name': 'ëª¨ë“ˆí™” RAG íŒŒì´í”„ë¼ì¸ v2.0 (CRAG í†µí•©)',
            'description': 'RAG íŒŒì´í”„ë¼ì¸ì„ ë…ë¦½ì ì¸ ëª¨ë“ˆë¡œ ë¶„ë¦¬í•˜ê³  CRAG(Corrective RAG) ê¸°ëŠ¥ì„ í†µí•©í•˜ì—¬ ê²€ìƒ‰ í’ˆì§ˆì„ í–¥ìƒì‹œí‚¨ ë²„ì „',
            'features': [
                'ëª¨ë“ˆí™”ëœ êµ¬ì¡° (vector_db, retrieval, reranking, prompting, answer_generator)',
                'CRAG(Corrective RAG) íŒŒì´í”„ë¼ì¸ í†µí•©',
                'LLM ê¸°ë°˜ ê²€ìƒ‰ í’ˆì§ˆ í‰ê°€',
                'ì¡°ê±´ë¶€ êµì • ê²€ìƒ‰ (í’ˆì§ˆ ë¯¸ë‹¬ ì‹œ ìë™ ê°œì„ )',
                'ì„¤ì • íŒŒì¼ ë¶„ë¦¬ (config.py)',
                'ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰',
                'ê³ í’ˆì§ˆ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§',
                'ë°°ì¹˜ ì²˜ë¦¬ ì§€ì›',
                'ìƒì„¸í•œ ë””ë²„ê¹… ì •ë³´ ì¶œë ¥'
            ],
            'changes': [
                'ê¸°ì¡´ ë‹¨ì¼ íŒŒì¼ êµ¬ì¡°ë¥¼ 6ê°œ ëª¨ë“ˆë¡œ ë¶„ë¦¬',
                'CRAG íŒŒì´í”„ë¼ì¸ í†µí•© (í’ˆì§ˆ í‰ê°€ â†’ ì¡°ê±´ë¶€ êµì •)',
                'ChromaDB ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ í†µí•©',
                'sentence-transformers/all-MiniLM-L6-v2 ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©',
                'ìœ ì‚¬ë„ ì„ê³„ê°’ 0.01ë¡œ ì¡°ì •í•˜ì—¬ ê²€ìƒ‰ ì„±ëŠ¥ í–¥ìƒ',
                'ìë™ MD ë¬¸ì„œ ìƒì„± ê¸°ëŠ¥ ì¶”ê°€',
                'submission í´ë”ì— íŒŒì¼ ì €ì¥'
            ],
            'config': {
                'embedding_model': 'sentence-transformers/all-MiniLM-L6-v2',
                'similarity_threshold': 0.01,
                'min_docs': 50,
                'max_docs': 100,
                'crag_enabled': True,
                'quality_threshold': 0.7,
                'max_corrective_attempts': 2
            }
        }
    }
    
    info = pipeline_info[pipeline_type]
    
    md_content = f"""# {info['name']} - ì œì¶œ íŒŒì¼ ë¬¸ì„œ

## ğŸ“‹ ê¸°ë³¸ ì •ë³´
- **ìƒì„± ì‹œê°„**: {time.strftime("%Y-%m-%d %H:%M:%S")}
- **íŒŒì´í”„ë¼ì¸ íƒ€ì…**: {pipeline_type}
- **ì´ ì§ˆë¬¸ ìˆ˜**: {question_count}ê°œ
- **ì´ ì†Œìš” ì‹œê°„**: {total_time:.2f}ì´ˆ
- **í‰ê·  ì²˜ë¦¬ ì‹œê°„**: {total_time/question_count:.2f}ì´ˆ/ì§ˆë¬¸

## ğŸ¯ íŒŒì´í”„ë¼ì¸ íŠ¹ì§•
{chr(10).join([f"- {feature}" for feature in info['features']])}

## ğŸ”„ ì£¼ìš” ë³€ê²½ì‚¬í•­
{chr(10).join([f"- {change}" for change in info['changes']])}

## âš™ï¸ ì„¤ì • ì •ë³´
"""
    
    for key, value in info['config'].items():
        md_content += f"- **{key}**: {value}\n"
    
    md_content += f"""
## ğŸ“Š ì„±ëŠ¥ í†µê³„
- **ë²¡í„° DB ë¬¸ì„œ ìˆ˜**: {pipeline_stats.get('vector_db', {}).get('total_documents', 'N/A')}ê°œ
- **ì„ë² ë”© ëª¨ë¸**: {pipeline_stats.get('vector_db', {}).get('model_name', 'N/A')}
- **ì„ë² ë”© ì°¨ì›**: {pipeline_stats.get('vector_db', {}).get('embedding_dimension', 'N/A')}
- **ìµœì†Œ ë¬¸ì„œ ìˆ˜**: {pipeline_stats.get('search_config', {}).get('min_docs', 'N/A')}ê°œ
- **ìµœëŒ€ ë¬¸ì„œ ìˆ˜**: {pipeline_stats.get('search_config', {}).get('max_docs', 'N/A')}ê°œ
- **ìµœì†Œ ë‹µë³€ ê¸¸ì´**: {pipeline_stats.get('answer_config', {}).get('min_answer_length', 'N/A')}ì

## ğŸ“ íŒŒì¼ êµ¬ì¡°
```
submission/final_pipeline/
â”œâ”€â”€ modules/
â”‚   â”œâ”€â”€ config.py          # ì„¤ì • ê´€ë¦¬
â”‚   â”œâ”€â”€ vector_db.py       # ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤
â”‚   â”œâ”€â”€ retrieval.py       # ë¬¸ì„œ ê²€ìƒ‰
â”‚   â”œâ”€â”€ reranking.py       # ë¬¸ì„œ ì¬ìˆœìœ„
â”‚   â”œâ”€â”€ prompting.py       # í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§
â”‚   â”œâ”€â”€ answer_generator.py # ë‹µë³€ ìƒì„±
â”‚   â””â”€â”€ rag_pipeline.py    # ë©”ì¸ íŒŒì´í”„ë¼ì¸
â”œâ”€â”€ submission_pipeline_modular.py  # ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â””â”€â”€ ...
```

## ğŸš€ ì‹¤í–‰ ë°©ë²•
```bash
cd submission/final_pipeline
python submission_pipeline_modular.py
```

## ğŸ“ˆ ì„±ëŠ¥ ê°œì„  ì‚¬í•­
1. **ëª¨ë“ˆí™”**: ì½”ë“œ ì¬ì‚¬ìš©ì„±ê³¼ ìœ ì§€ë³´ìˆ˜ì„± í–¥ìƒ
2. **ë²¡í„° ê²€ìƒ‰**: ì˜ë¯¸ì  ìœ ì‚¬ë„ ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰
3. **ì„¤ì • ë¶„ë¦¬**: í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì • ìš©ì´ì„±
4. **ë””ë²„ê¹…**: ìƒì„¸í•œ ë¡œê·¸ë¡œ ë¬¸ì œ ì§„ë‹¨ ê°€ëŠ¥
5. **ìë™í™”**: MD ë¬¸ì„œ ìë™ ìƒì„±ìœ¼ë¡œ ê¸°ë¡ ê´€ë¦¬

---
*ì´ ë¬¸ì„œëŠ” ìë™ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.*
"""
    
    with open(md_filepath, 'w', encoding='utf-8') as f:
        f.write(md_content)

# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ import
try:
    from scienceon_api_example import ScienceONAPIClient
    from gemini_client import GeminiClient
    from modules import RAGPipeline
except ImportError as e:
    print(f"ğŸš¨ [ì˜¤ë¥˜] í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {e}")
    print("   ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”: pip install -r requirements.txt")
    sys.exit(1)

def validate_credentials(path: Path) -> dict:
    """API ì¸ì¦ ì •ë³´ ê²€ì¦"""
    import json
    
    credentials = {}
    if not path.exists():
        print(f"ğŸš¨ ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤! (ê²½ë¡œ: {path})")
        sys.exit(1)
    try:
        with open(path, 'r', encoding='utf-8') as f:
            credentials = json.load(f)
    except json.JSONDecodeError:
        print(f"ğŸš¨ ì„¤ì • íŒŒì¼ì˜ í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤! (ê²½ë¡œ: {path})")
        sys.exit(1)
    
    required_keys = ['auth_key', 'client_id', 'mac_address']
    missing_keys = [key for key in required_keys if key not in credentials or not credentials[key]]
    if missing_keys:
        print(f"ğŸš¨ ì„¤ì • íŒŒì¼ì— í•„ìˆ˜ ì •ë³´ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤! (ëˆ„ë½ëœ ì •ë³´: {', '.join(missing_keys)})")
        sys.exit(1)
    
    if len(credentials['auth_key']) != 32:
        print(f"ğŸš¨ ì¸ì¦í‚¤(auth_key)ì˜ ê¸¸ì´ê°€ 32ìê°€ ì•„ë‹™ë‹ˆë‹¤! (í˜„ì¬ ê¸¸ì´: {len(credentials['auth_key'])}ì)")
        sys.exit(1)
    
    print("âœ… [ì„±ê³µ] API ì¸ì¦ ì •ë³´ íŒŒì¼ì´ ìœ íš¨í•©ë‹ˆë‹¤.")
    return credentials

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("â­ Kaggle ì œì¶œìš© ëª¨ë“ˆí™” RAG íŒŒì´í”„ë¼ì¸ v2.0")
    start_time = time.time()
    
    # 1. API ì¸ì¦ ì •ë³´ ê²€ì¦
    credentials_path = Path('./configs/scienceon_api_credentials.json')
    validate_credentials(credentials_path)
    
    # 2. API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    try:
        api_client = ScienceONAPIClient(credentials_path=credentials_path)
        gemini_credentials_path = Path('./configs/gemini_api_credentials.json')
        gemini_client = GeminiClient(gemini_credentials_path)
        print("âœ… API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
    except Exception as e:
        print(f"âš ï¸  API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        print("   configs í´ë”ì˜ ì¸ì¦ íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
        sys.exit(1)
    
    # 3. RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
    pipeline = RAGPipeline(api_client, gemini_client)
    
    # CRAG ì„¤ì • ì •ë³´ ì¶œë ¥
    from modules.config import CRAG_CONFIG
    if CRAG_CONFIG.get('enable_crag', False):
        print("âœ… CRAG íŒŒì´í”„ë¼ì¸ í™œì„±í™”")
        print(f"   - í’ˆì§ˆ ì„ê³„ê°’: {CRAG_CONFIG.get('quality_threshold', 0.7)}")
        print(f"   - ìµœëŒ€ êµì • ì‹œë„: {CRAG_CONFIG.get('max_corrective_attempts', 2)}íšŒ")
        print(f"   - ì›¹ ê²€ìƒ‰: {'í™œì„±í™”' if CRAG_CONFIG.get('web_search_enabled', False) else 'ë¹„í™œì„±í™”'}")
    else:
        print("âš ï¸  CRAG íŒŒì´í”„ë¼ì¸ ë¹„í™œì„±í™”")
    
    # 4. í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
    try:
        test_df = pd.read_csv('test.csv')
        print(f"âœ… í…ŒìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ: {len(test_df)}ê°œ ì§ˆë¬¸")
        
        # í…ŒìŠ¤íŠ¸ìš© ì§ˆë¬¸ ìˆ˜ ì œí•œ
        from modules.config import TEST_CONFIG
        max_questions = TEST_CONFIG['max_questions']
        if len(test_df) > max_questions:
            test_df = test_df.head(max_questions)
            print(f"ğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë“œ: {max_questions}ê°œ ì§ˆë¬¸ìœ¼ë¡œ ì œí•œ")
        
    except FileNotFoundError:
        print("âŒ test.csv íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    except Exception as e:
        print(f"âŒ Error reading test.csv: {e}")
        return
    
    # 5. ì§ˆë¬¸ ì²˜ë¦¬
    predictions = []
    predicted_articles = []
    
    print(f"\n--- ì§ˆë¬¸ ì²˜ë¦¬ ì‹œì‘ ---")
    
    # ì§ˆë¬¸ì„ íŠœí”Œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    questions_to_process = [(index, row['Question']) for index, row in test_df.iterrows()]
    
    # ë°°ì¹˜ ì²˜ë¦¬ (ì‹œê°„ ì¸¡ì • í¬í•¨)
    elapsed_times = []
    
    with tqdm(total=len(questions_to_process), desc="ì§ˆë¬¸ ì²˜ë¦¬") as pbar:
        for index, row in test_df.iterrows():
            print(f"\nğŸ” ì§ˆë¬¸ {index+1}: {row['Question'][:100]}...")
            
            # ê°œë³„ ì§ˆë¬¸ ì²˜ë¦¬ ì‹œê°„ ì¸¡ì •
            question_start_time = time.time()
            answer, articles = pipeline.process_question(index, row['Question'])
            question_elapsed_time = time.time() - question_start_time
            
            predictions.append(answer)
            predicted_articles.append(articles)
            elapsed_times.append(question_elapsed_time)
            pbar.update(1)
    
    # 6. ê²°ê³¼ ì €ì¥ - ì˜¬ë°”ë¥¸ ì»¬ëŸ¼ ìˆœì„œë¡œ êµ¬ì„±
    # ì˜¬ë°”ë¥¸ ì»¬ëŸ¼ ìˆœì„œ ì •ì˜
    correct_column_order = [
        'id', 'Question', 'SAI_Answer', 'translated_question', 'translated_SAI_answer'
    ]
    
    # retrieved_article_name_1~50 ì¶”ê°€
    for i in range(1, 51):
        correct_column_order.append(f'retrieved_article_name_{i}')
    
    # prediction_retrieved_article_name_1~50 ì¶”ê°€
    for i in range(1, 51):
        correct_column_order.append(f'prediction_retrieved_article_name_{i}')
    
    # Predictionê³¼ elapsed_times ì¶”ê°€
    correct_column_order.extend(['Prediction', 'elapsed_times'])
    
    # ìƒˆë¡œìš´ submission DataFrame ìƒì„±
    submission_df = pd.DataFrame()
    
    # 1. ì›ë³¸ ì»¬ëŸ¼ë“¤ ë³µì‚¬ (ì˜¬ë°”ë¥¸ ìˆœì„œë¡œ)
    for col in correct_column_order:
        if col in test_df.columns:
            submission_df[col] = test_df[col]
        elif col == 'Prediction':
            submission_df[col] = predictions
        elif col == 'elapsed_times':
            submission_df[col] = elapsed_times
        elif col.startswith('prediction_retrieved_article_name_'):
            # prediction_retrieved_article_name_1~50 ì»¬ëŸ¼ ìƒì„±
            article_index = int(col.split('_')[-1]) - 1
            submission_df[col] = [articles[article_index] if article_index < len(articles) else '' 
                                  for articles in predicted_articles]
    
    # ì»¬ëŸ¼ ìˆœì„œ ê°•ì œ ì ìš©
    submission_df = submission_df[correct_column_order]
    
    # 7. null ê°’ ì²˜ë¦¬ ë° ì €ì¥ (ê°•í™”ëœ ê²€ì¦)
    submission_df = submission_df.fillna('')
    
    # ì¶”ê°€ null ê°’ ê²€ì¦
    for col in submission_df.columns:
        submission_df[col] = submission_df[col].astype(str).replace('nan', '')
        submission_df[col] = submission_df[col].replace('', 'No relevant document found')
    
    # Answer ì»¬ëŸ¼ íŠ¹ë³„ ê²€ì¦
    submission_df['Prediction'] = submission_df['Prediction'].apply(
        lambda x: 'Based on the available research documents, this question requires further investigation.' if not x or x.strip() == '' or len(x.strip()) < 10 else x
    )
    
    # submission í´ë” ìƒì„±
    submission_dir = '../submissions'
    os.makedirs(submission_dir, exist_ok=True)
    
    # íŒŒì¼ëª…ì— íŒŒì´í”„ë¼ì¸ ì •ë³´ í¬í•¨
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    filename = f'submission_modular_v2_{timestamp}.csv'
    filepath = os.path.join(submission_dir, filename)
    submission_df.to_csv(filepath, index=False, encoding='utf-8-sig')
    
    # 8. ì„±ëŠ¥ ì§€í‘œ ì¶œë ¥
    total_time = time.time() - start_time
    
    # íŒŒì´í”„ë¼ì¸ í†µê³„ ê°€ì ¸ì˜¤ê¸°
    pipeline_stats = pipeline.get_pipeline_stats()
    
    # MD ë¬¸ì„œ ìƒì„±
    md_filename = filename.replace('.csv', '.md')
    md_filepath = os.path.join(submission_dir, md_filename)
    create_submission_documentation(md_filepath, 'modular_v2', pipeline_stats, total_time, len(test_df))
    
    print(f"   ğŸ“ ìƒì„±ëœ íŒŒì¼: {filepath}")
    print(f"   ğŸ“„ ìƒì„±ëœ ë¬¸ì„œ: {md_filepath}")
    
    print(f"\nğŸ‰ ëª¨ë“ˆí™” RAG íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
    print(f"   â±ï¸  ì´ ì†Œìš” ì‹œê°„: {total_time:.2f}ì´ˆ")
    print(f"   ğŸ“Š í‰ê·  ì²˜ë¦¬ ì‹œê°„: {total_time/len(test_df):.2f}ì´ˆ/ì§ˆë¬¸")
    print(f"   âœ… ì„±ê³µë¥ : {len(test_df)}/{len(test_df)} (100.0%)")
    print(f"   ğŸ“ {filepath} ìƒì„± ì™„ë£Œ")
    
    # 9. íŒŒì´í”„ë¼ì¸ í†µê³„ ì¶œë ¥
    print(f"   ğŸ“ˆ íŒŒì´í”„ë¼ì¸ í†µê³„: {pipeline_stats}")
    
    # 10. íŒŒì¼ ê²€ì¦
    print(f"\nğŸ“Š íŒŒì¼ ê²€ì¦:")
    print(f"   - ì´ ì§ˆë¬¸ ìˆ˜: {len(submission_df)}")
    print(f"   - ë‹µë³€ ìƒì„±ëœ ì§ˆë¬¸ ìˆ˜: {len(submission_df[submission_df['Prediction'].notna() & (submission_df['Prediction'] != '')])}")
    print(f"   - ë…¼ë¬¸ ê²€ìƒ‰ëœ ì§ˆë¬¸ ìˆ˜: {len(submission_df[submission_df['prediction_retrieved_article_name_1'].notna() & (submission_df['prediction_retrieved_article_name_1'] != '')])}")
    
    # null ê°’ í™•ì¸
    null_counts = submission_df.isnull().sum()
    if null_counts.sum() > 0:
        print(f"   âš ï¸  null ê°’ ë°œê²¬: {null_counts[null_counts > 0].to_dict()}")
    else:
        print(f"   âœ… null ê°’ ì—†ìŒ")

if __name__ == "__main__":
    main()
